{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrakritiNayak/Genzel-Lab-Internship-OS-Tracking/blob/main/Pre_labelled_Training_and_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK255E7YoEIt"
      },
      "source": [
        "# DeepLabCut - Colab GPU for training and analysis in (single animal) projects\n",
        "###Post creation of project folder with labeled data with GUI\n",
        "###Test1_8bp 19/5/22 \n",
        "- create a training set\n",
        "- train a network\n",
        "- evaluate a network\n",
        "- create simple quality check plots\n",
        "- analyze novel videos\n",
        "\n",
        "####Change the path in the config file when returning to GUI usage through PC.\n",
        "\n",
        "Protocol paper: https://www.nature.com/articles/s41596-019-0176-0\n",
        "\n",
        "Pre-print: https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txoddlM8hLKm"
      },
      "source": [
        "## \"Runtime\" ->\"change runtime type\"->select \"Python3\", then \"GPU\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gkX3Y5jBZG_2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q23BzhA6CXxu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b78cbd1b-0554-481a-eab8-504ab8b1ebef"
      },
      "source": [
        "#(this will take a few minutes to install all the dependences!)\n",
        "!pip install deeplabcut"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deeplabcut\n",
            "  Downloading deeplabcut-2.2.1.1-py3-none-any.whl (591 kB)\n",
            "\u001b[K     |████████████████████████████████| 591 kB 19.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.21.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.13)\n",
            "Collecting tf-slim>=1.1.0\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 51.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.0 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (4.64.0)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.3.5)\n",
            "Collecting statsmodels!=0.13.2,>=0.11\n",
            "  Downloading statsmodels-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 55.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tables>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.7.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.0.2)\n",
            "Collecting matplotlib>=3.3\n",
            "  Downloading matplotlib-3.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 64.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.7.3)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.56.0)\n",
            "Requirement already satisfied: networkx>=2.6 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (2.6.3)\n",
            "Collecting tensorpack>=0.11\n",
            "  Downloading tensorpack-0.11-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[K     |████████████████████████████████| 296 kB 71.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (7.1.2)\n",
            "Collecting ruamel.yaml>=0.15.0\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/9e/cb/938214ac358fbef7058343b3765c79a1b7ed0c366f7f992ce7ff38335652/ruamel.yaml-0.17.21-py3-none-any.whl\u001b[0m\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 10.8 MB/s \n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/filterpy/\u001b[0m\n",
            "\u001b[?25hCollecting filterpy>=1.4.4\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 46.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.4.0)\n",
            "Requirement already satisfied: scikit-image<=1.0.0,>=0.17 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.18.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->deeplabcut) (4.6.0.66)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->deeplabcut) (1.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->deeplabcut) (1.15.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->deeplabcut) (2.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->deeplabcut) (21.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->deeplabcut) (0.11.0)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.34.4-py3-none-any.whl (944 kB)\n",
            "\u001b[K     |████████████████████████████████| 944 kB 61.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->deeplabcut) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->deeplabcut) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->deeplabcut) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.3->deeplabcut) (4.1.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.54->deeplabcut) (0.39.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.54->deeplabcut) (4.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.54->deeplabcut) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->deeplabcut) (2022.1)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 27.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image<=1.0.0,>=0.17->deeplabcut) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image<=1.0.0,>=0.17->deeplabcut) (1.3.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0->deeplabcut) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0->deeplabcut) (1.1.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels!=0.13.2,>=0.11->deeplabcut) (0.5.2)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.7/dist-packages (from tables>=3.7.0->deeplabcut) (2.8.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (3.3.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (2.8.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (3.17.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (14.0.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.14.1)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (2.8.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (0.26.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.47.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (0.5.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0->deeplabcut) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.0->deeplabcut) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.54->deeplabcut) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from tensorpack>=0.11->deeplabcut) (0.8.10)\n",
            "Collecting msgpack-numpy>=0.4.4.2\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.7/dist-packages (from tensorpack>=0.11->deeplabcut) (23.2.0)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.7/dist-packages (from tensorpack>=0.11->deeplabcut) (5.4.8)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from tensorpack>=0.11->deeplabcut) (1.0.4)\n",
            "Building wheels for collected packages: filterpy\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110474 sha256=a7efb4251baaaa956c41fa2866af3863e5cb50947f5dea9cb57b676a4fa427fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/e0/ee/a2b3c5caab3418c1ccd8c4de573d4cbe13315d7e8b0a55fbc2\n",
            "Successfully built filterpy\n",
            "Installing collected packages: fonttools, matplotlib, ruamel.yaml.clib, msgpack-numpy, tf-slim, tensorpack, statsmodels, ruamel.yaml, filterpy, deeplabcut\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "Successfully installed deeplabcut-2.2.1.1 filterpy-1.4.5 fonttools-4.34.4 matplotlib-3.5.2 msgpack-numpy-0.4.8 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 statsmodels-0.13.1 tensorpack-0.11 tf-slim-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25wSj6TlVclR"
      },
      "source": [
        "Attention: Click **RESTART RUNTIME**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ-nlTkri4HZ"
      },
      "source": [
        "## Link your Google Drive (with your labeled data, or the demo data):\n",
        "\n",
        "### First, place your project folder into you google drive! \"i.e. move the folder named \"Project-YourName-TheDate\" into google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS4Q4UkR9rgG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "890213de-6030-4009-bf08-7a2c29b84787"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frnj1RVDyEqs"
      },
      "source": [
        "YOU WILL NEED TO EDIT THE PROJECT PATH **in the config.yaml file** TO BE SET TO YOUR GOOGLE DRIVE LINK!\n",
        "\n",
        "Typically, this will be: /content/drive/My Drive/yourProjectFolderName\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhENAlQnFENJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd813db0-391a-464f-fa5f-be391fbaf131"
      },
      "source": [
        "#Setup your project variables:\n",
        "# PLEASE EDIT THESE:\n",
        "  \n",
        "ProjectFolderName = 'Test1_8bp-PN-2022-05-19'\n",
        "VideoType = 'avi' \n",
        "\n",
        "#don't edit these:\n",
        "videofile_path = ['/content/drive/My Drive/Genzel Internship/Test/Test1_8bp-PN-2022-05-19/videos/'] #Enter the list of videos or folder to analyze.\n",
        "videofile_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Genzel Internship/Test/Test1_8bp-PN-2022-05-19/videos/']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXufoX6INe6w"
      },
      "source": [
        "#GUIs don't work on the cloud, so label your data locally on your computer! This will suppress the GUI support\n",
        "import os\n",
        "os.environ[\"DLClight\"]=\"True\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K9Ndy1beyfG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64469108-3832-42ca-c8f5-91d879235f60"
      },
      "source": [
        "import deeplabcut"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading DLC 2.2.1.1...\n",
            "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4orkg9QTHKK"
      },
      "source": [
        "deeplabcut.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7ZlDr3wV4D1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "314b0eeb-6e41-4784-8049-140b19a7ca75"
      },
      "source": [
        "#This creates a path variable that links to your google drive copy\n",
        "#No need to edit this, as you set it up before: \n",
        "path_config_file = '/content/drive/My Drive/Genzel Internship/Test/Test1_8bp-PN-2022-05-19/config.yaml'\n",
        "path_config_file"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Genzel Internship/Test/Test1_8bp-PN-2022-05-19/config.yaml'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNi9s1dboEJN"
      },
      "source": [
        "## Create a training dataset:\n",
        "### You must do this step inside of Colab:\n",
        "After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'**\n",
        "\n",
        "This function also creates new subdirectories under **dlc-models** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pose_cfg.yaml**.\n",
        "\n",
        "Now it is the time to start training the network!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "eMeUwgxPoEJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66cf51e1-0381-435f-a90b-04c5349d4ab1"
      },
      "source": [
        "#There are many more functions you can set here, including which netowkr to use!\n",
        "#check the docstring for full options you can do!\n",
        "deeplabcut.create_training_dataset(path_config_file, net_type='resnet_50', augmenter_type='imgaug')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz....\n",
            "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.95,\n",
              "  1,\n",
              "  (array([26, 35, 59, 28, 11,  2, 34, 58, 40, 22,  4, 10, 30, 41, 33, 43, 49,\n",
              "           7, 14, 32, 50, 29, 42, 54, 18, 56, 27, 15,  5, 31, 16, 51, 20, 52,\n",
              "           8, 13, 25, 37, 17, 45, 48, 57, 38,  1, 12, 46, 24,  6, 23, 36, 21,\n",
              "          19,  9, 39, 55,  3,  0]), array([53, 47, 44])))]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4FczXGDoEJU"
      },
      "source": [
        "## Start training:\n",
        "This function trains the network for a specific shuffle of the training dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pOvDq_2oEJW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "920dd490-105d-43d7-934c-e55d24ac67fa"
      },
      "source": [
        "#let's also change the display and save_iters just in case Colab takes away the GPU... \n",
        "#if that happens, you can reload from a saved point. Typically, you want to train to 200,000 + iterations.\n",
        "#more info and there are more things you can set: https://github.com/DeepLabCut/DeepLabCut/wiki/DOCSTRINGS#train_network\n",
        "\n",
        "deeplabcut.train_network(path_config_file, shuffle=1, displayiters=10,saveiters=1000)\n",
        "\n",
        "#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.03M iterations). \n",
        "#Whichever you chose, you will see what looks like an error message, but it's not an error - don't worry...."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7]],\n",
            " 'all_joints_names': ['Nose',\n",
            "                      'Left_ear',\n",
            "                      'Right_ear',\n",
            "                      'Left_lateral',\n",
            "                      'Right_lateral',\n",
            "                      'Centroid',\n",
            "                      'Tail_base',\n",
            "                      'Tail_end'],\n",
            " 'alpha_r': 0.02,\n",
            " 'apply_prob': 0.5,\n",
            " 'batch_size': 1,\n",
            " 'clahe': True,\n",
            " 'claheratio': 0.1,\n",
            " 'crop_pad': 0,\n",
            " 'cropratio': 0.4,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_Test1_8bpMay19/Test1_8bp_PN95shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'decay_steps': 30000,\n",
            " 'deterministic': False,\n",
            " 'display_iters': 1000,\n",
            " 'edge': False,\n",
            " 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]},\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'histeq': True,\n",
            " 'histeqratio': 0.1,\n",
            " 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 0.05,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'lr_init': 0.0005,\n",
            " 'max_input_size': 1500,\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_Test1_8bpMay19/Documentation_data-Test1_8bp_95shuffle1.pickle',\n",
            " 'min_input_size': 64,\n",
            " 'mirror': False,\n",
            " 'multi_stage': False,\n",
            " 'multi_step': [[0.005, 10000],\n",
            "                [0.02, 430000],\n",
            "                [0.002, 730000],\n",
            "                [0.001, 1030000]],\n",
            " 'net_type': 'resnet_50',\n",
            " 'num_joints': 8,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': False,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'pos_dist_thresh': 17,\n",
            " 'project_path': '/content/drive/My Drive/Genzel '\n",
            "                 'Internship/Test/Test1_8bp-PN-2022-05-19',\n",
            " 'regularize': False,\n",
            " 'rotation': 25,\n",
            " 'rotratio': 0.4,\n",
            " 'save_iters': 50000,\n",
            " 'scale_jitter_lo': 0.5,\n",
            " 'scale_jitter_up': 1.25,\n",
            " 'scoremap_dir': 'test',\n",
            " 'sharpen': False,\n",
            " 'sharpenratio': 0.3,\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/My Drive/Genzel '\n",
            "                    'Internship/Test/Test1_8bp-PN-2022-05-19/dlc-models/iteration-0/Test1_8bpMay19-trainset95shuffle1/train/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting single-animal trainer\n",
            "Batch Size is 1\n",
            "Loading ImageNet-pretrained resnet_50\n",
            "Display_iters overwritten as 10\n",
            "Save_iters overwritten as 1000\n",
            "Training parameter:\n",
            "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/My Drive/Genzel Internship/Test/Test1_8bp-PN-2022-05-19/dlc-models/iteration-0/Test1_8bpMay19-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7]], 'all_joints_names': ['Nose', 'Left_ear', 'Right_ear', 'Left_lateral', 'Right_lateral', 'Centroid', 'Tail_base', 'Tail_end'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'clahe': True, 'claheratio': 0.1, 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_Test1_8bpMay19/Test1_8bp_PN95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]}, 'histeq': True, 'histeqratio': 0.1, 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_Test1_8bpMay19/Documentation_data-Test1_8bp_95shuffle1.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 8, 'pos_dist_thresh': 17, 'project_path': '/content/drive/My Drive/Genzel Internship/Test/Test1_8bp-PN-2022-05-19', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'sharpen': False, 'sharpenratio': 0.3, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
            "Starting training....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iteration: 215600 loss: 0.0035 lr: 0.02\n",
            "iteration: 215610 loss: 0.0035 lr: 0.02\n",
            "iteration: 215620 loss: 0.0029 lr: 0.02\n",
            "iteration: 215630 loss: 0.0031 lr: 0.02\n",
            "iteration: 215640 loss: 0.0028 lr: 0.02\n",
            "iteration: 215650 loss: 0.0030 lr: 0.02\n",
            "iteration: 215660 loss: 0.0030 lr: 0.02\n",
            "iteration: 215670 loss: 0.0036 lr: 0.02\n",
            "iteration: 215680 loss: 0.0031 lr: 0.02\n",
            "iteration: 215690 loss: 0.0038 lr: 0.02\n",
            "iteration: 215700 loss: 0.0036 lr: 0.02\n",
            "iteration: 215710 loss: 0.0032 lr: 0.02\n",
            "iteration: 215720 loss: 0.0037 lr: 0.02\n",
            "iteration: 215730 loss: 0.0040 lr: 0.02\n",
            "iteration: 215740 loss: 0.0041 lr: 0.02\n",
            "iteration: 215750 loss: 0.0039 lr: 0.02\n",
            "iteration: 215760 loss: 0.0037 lr: 0.02\n",
            "iteration: 215770 loss: 0.0035 lr: 0.02\n",
            "iteration: 215780 loss: 0.0035 lr: 0.02\n",
            "iteration: 215790 loss: 0.0032 lr: 0.02\n",
            "iteration: 215800 loss: 0.0035 lr: 0.02\n",
            "iteration: 215810 loss: 0.0031 lr: 0.02\n",
            "iteration: 215820 loss: 0.0032 lr: 0.02\n",
            "iteration: 215830 loss: 0.0034 lr: 0.02\n",
            "iteration: 215840 loss: 0.0027 lr: 0.02\n",
            "iteration: 215850 loss: 0.0035 lr: 0.02\n",
            "iteration: 215860 loss: 0.0030 lr: 0.02\n",
            "iteration: 215870 loss: 0.0035 lr: 0.02\n",
            "iteration: 215880 loss: 0.0034 lr: 0.02\n",
            "iteration: 215890 loss: 0.0035 lr: 0.02\n",
            "iteration: 215900 loss: 0.0036 lr: 0.02\n",
            "iteration: 215910 loss: 0.0030 lr: 0.02\n",
            "iteration: 215920 loss: 0.0038 lr: 0.02\n",
            "iteration: 215930 loss: 0.0032 lr: 0.02\n",
            "iteration: 215940 loss: 0.0032 lr: 0.02\n",
            "iteration: 215950 loss: 0.0031 lr: 0.02\n",
            "iteration: 215960 loss: 0.0030 lr: 0.02\n",
            "iteration: 215970 loss: 0.0035 lr: 0.02\n",
            "iteration: 215980 loss: 0.0036 lr: 0.02\n",
            "iteration: 215990 loss: 0.0030 lr: 0.02\n",
            "iteration: 216000 loss: 0.0035 lr: 0.02\n",
            "iteration: 216010 loss: 0.0047 lr: 0.02\n",
            "iteration: 216020 loss: 0.0032 lr: 0.02\n",
            "iteration: 216030 loss: 0.0034 lr: 0.02\n",
            "iteration: 216040 loss: 0.0028 lr: 0.02\n",
            "iteration: 216050 loss: 0.0033 lr: 0.02\n",
            "iteration: 216060 loss: 0.0036 lr: 0.02\n",
            "iteration: 216070 loss: 0.0034 lr: 0.02\n",
            "iteration: 216080 loss: 0.0035 lr: 0.02\n",
            "iteration: 216090 loss: 0.0036 lr: 0.02\n",
            "iteration: 216100 loss: 0.0035 lr: 0.02\n",
            "iteration: 216110 loss: 0.0034 lr: 0.02\n",
            "iteration: 216120 loss: 0.0031 lr: 0.02\n",
            "iteration: 216130 loss: 0.0026 lr: 0.02\n",
            "iteration: 216140 loss: 0.0041 lr: 0.02\n",
            "iteration: 216150 loss: 0.0029 lr: 0.02\n",
            "iteration: 216160 loss: 0.0042 lr: 0.02\n",
            "iteration: 216170 loss: 0.0035 lr: 0.02\n",
            "iteration: 216180 loss: 0.0031 lr: 0.02\n",
            "iteration: 216190 loss: 0.0037 lr: 0.02\n",
            "iteration: 216200 loss: 0.0025 lr: 0.02\n",
            "iteration: 216210 loss: 0.0031 lr: 0.02\n",
            "iteration: 216220 loss: 0.0034 lr: 0.02\n",
            "iteration: 216230 loss: 0.0028 lr: 0.02\n",
            "iteration: 216240 loss: 0.0046 lr: 0.02\n",
            "iteration: 216250 loss: 0.0032 lr: 0.02\n",
            "iteration: 216260 loss: 0.0056 lr: 0.02\n",
            "iteration: 216270 loss: 0.0052 lr: 0.02\n",
            "iteration: 216280 loss: 0.0033 lr: 0.02\n",
            "iteration: 216290 loss: 0.0038 lr: 0.02\n",
            "iteration: 216300 loss: 0.0030 lr: 0.02\n",
            "iteration: 216310 loss: 0.0033 lr: 0.02\n",
            "iteration: 216320 loss: 0.0043 lr: 0.02\n",
            "iteration: 216330 loss: 0.0037 lr: 0.02\n",
            "iteration: 216340 loss: 0.0026 lr: 0.02\n",
            "iteration: 216350 loss: 0.0039 lr: 0.02\n",
            "iteration: 216360 loss: 0.0043 lr: 0.02\n",
            "iteration: 216370 loss: 0.0030 lr: 0.02\n",
            "iteration: 216380 loss: 0.0029 lr: 0.02\n",
            "iteration: 216390 loss: 0.0030 lr: 0.02\n",
            "iteration: 216400 loss: 0.0030 lr: 0.02\n",
            "iteration: 216410 loss: 0.0032 lr: 0.02\n",
            "iteration: 216420 loss: 0.0030 lr: 0.02\n",
            "iteration: 216430 loss: 0.0032 lr: 0.02\n",
            "iteration: 216440 loss: 0.0031 lr: 0.02\n",
            "iteration: 216450 loss: 0.0030 lr: 0.02\n",
            "iteration: 216460 loss: 0.0044 lr: 0.02\n",
            "iteration: 216470 loss: 0.0038 lr: 0.02\n",
            "iteration: 216480 loss: 0.0029 lr: 0.02\n",
            "iteration: 216490 loss: 0.0034 lr: 0.02\n",
            "iteration: 216500 loss: 0.0040 lr: 0.02\n",
            "iteration: 216510 loss: 0.0030 lr: 0.02\n",
            "iteration: 216520 loss: 0.0032 lr: 0.02\n",
            "iteration: 216530 loss: 0.0033 lr: 0.02\n",
            "iteration: 216540 loss: 0.0028 lr: 0.02\n",
            "iteration: 216550 loss: 0.0028 lr: 0.02\n",
            "iteration: 216560 loss: 0.0032 lr: 0.02\n",
            "iteration: 216570 loss: 0.0045 lr: 0.02\n",
            "iteration: 216580 loss: 0.0031 lr: 0.02\n",
            "iteration: 216590 loss: 0.0024 lr: 0.02\n",
            "iteration: 216600 loss: 0.0034 lr: 0.02\n",
            "iteration: 216610 loss: 0.0037 lr: 0.02\n",
            "iteration: 216620 loss: 0.0027 lr: 0.02\n",
            "iteration: 216630 loss: 0.0029 lr: 0.02\n",
            "iteration: 216640 loss: 0.0026 lr: 0.02\n",
            "iteration: 216650 loss: 0.0041 lr: 0.02\n",
            "iteration: 216660 loss: 0.0030 lr: 0.02\n",
            "iteration: 216670 loss: 0.0036 lr: 0.02\n",
            "iteration: 216680 loss: 0.0036 lr: 0.02\n",
            "iteration: 216690 loss: 0.0030 lr: 0.02\n",
            "iteration: 216700 loss: 0.0030 lr: 0.02\n",
            "iteration: 216710 loss: 0.0030 lr: 0.02\n",
            "iteration: 216720 loss: 0.0031 lr: 0.02\n",
            "iteration: 216730 loss: 0.0030 lr: 0.02\n",
            "iteration: 216740 loss: 0.0039 lr: 0.02\n",
            "iteration: 216750 loss: 0.0023 lr: 0.02\n",
            "iteration: 216760 loss: 0.0036 lr: 0.02\n",
            "iteration: 216770 loss: 0.0041 lr: 0.02\n",
            "iteration: 216780 loss: 0.0037 lr: 0.02\n",
            "iteration: 216790 loss: 0.0039 lr: 0.02\n",
            "iteration: 216800 loss: 0.0035 lr: 0.02\n",
            "iteration: 216810 loss: 0.0040 lr: 0.02\n",
            "iteration: 216820 loss: 0.0027 lr: 0.02\n",
            "iteration: 216830 loss: 0.0035 lr: 0.02\n",
            "iteration: 216840 loss: 0.0027 lr: 0.02\n",
            "iteration: 216850 loss: 0.0031 lr: 0.02\n",
            "iteration: 216860 loss: 0.0046 lr: 0.02\n",
            "iteration: 216870 loss: 0.0038 lr: 0.02\n",
            "iteration: 216880 loss: 0.0033 lr: 0.02\n",
            "iteration: 216890 loss: 0.0037 lr: 0.02\n",
            "iteration: 216900 loss: 0.0037 lr: 0.02\n",
            "iteration: 216910 loss: 0.0032 lr: 0.02\n",
            "iteration: 216920 loss: 0.0031 lr: 0.02\n",
            "iteration: 216930 loss: 0.0034 lr: 0.02\n",
            "iteration: 216940 loss: 0.0044 lr: 0.02\n",
            "iteration: 216950 loss: 0.0043 lr: 0.02\n",
            "iteration: 216960 loss: 0.0027 lr: 0.02\n",
            "iteration: 216970 loss: 0.0028 lr: 0.02\n",
            "iteration: 216980 loss: 0.0035 lr: 0.02\n",
            "iteration: 216990 loss: 0.0046 lr: 0.02\n",
            "iteration: 217000 loss: 0.0030 lr: 0.02\n",
            "iteration: 217010 loss: 0.0033 lr: 0.02\n",
            "iteration: 217020 loss: 0.0040 lr: 0.02\n",
            "iteration: 217030 loss: 0.0029 lr: 0.02\n",
            "iteration: 217040 loss: 0.0029 lr: 0.02\n",
            "iteration: 217050 loss: 0.0027 lr: 0.02\n",
            "iteration: 217060 loss: 0.0028 lr: 0.02\n",
            "iteration: 217070 loss: 0.0034 lr: 0.02\n",
            "iteration: 217080 loss: 0.0033 lr: 0.02\n",
            "iteration: 217090 loss: 0.0035 lr: 0.02\n",
            "iteration: 217100 loss: 0.0032 lr: 0.02\n",
            "iteration: 217110 loss: 0.0038 lr: 0.02\n",
            "iteration: 217120 loss: 0.0030 lr: 0.02\n",
            "iteration: 217130 loss: 0.0034 lr: 0.02\n",
            "iteration: 217140 loss: 0.0036 lr: 0.02\n",
            "iteration: 217150 loss: 0.0029 lr: 0.02\n",
            "iteration: 217160 loss: 0.0031 lr: 0.02\n",
            "iteration: 217170 loss: 0.0037 lr: 0.02\n",
            "iteration: 217180 loss: 0.0032 lr: 0.02\n",
            "iteration: 217190 loss: 0.0027 lr: 0.02\n",
            "iteration: 217200 loss: 0.0029 lr: 0.02\n",
            "iteration: 217210 loss: 0.0030 lr: 0.02\n",
            "iteration: 217220 loss: 0.0031 lr: 0.02\n",
            "iteration: 217230 loss: 0.0045 lr: 0.02\n",
            "iteration: 217240 loss: 0.0036 lr: 0.02\n",
            "iteration: 217250 loss: 0.0038 lr: 0.02\n",
            "iteration: 217260 loss: 0.0029 lr: 0.02\n",
            "iteration: 217270 loss: 0.0033 lr: 0.02\n",
            "iteration: 217280 loss: 0.0030 lr: 0.02\n",
            "iteration: 217290 loss: 0.0034 lr: 0.02\n",
            "iteration: 217300 loss: 0.0035 lr: 0.02\n",
            "iteration: 217310 loss: 0.0036 lr: 0.02\n",
            "iteration: 217320 loss: 0.0036 lr: 0.02\n",
            "iteration: 217330 loss: 0.0031 lr: 0.02\n",
            "iteration: 217340 loss: 0.0031 lr: 0.02\n",
            "iteration: 217350 loss: 0.0034 lr: 0.02\n",
            "iteration: 217360 loss: 0.0033 lr: 0.02\n",
            "iteration: 217370 loss: 0.0032 lr: 0.02\n",
            "iteration: 217380 loss: 0.0035 lr: 0.02\n",
            "iteration: 217390 loss: 0.0031 lr: 0.02\n",
            "iteration: 217400 loss: 0.0025 lr: 0.02\n",
            "iteration: 217410 loss: 0.0044 lr: 0.02\n",
            "iteration: 217420 loss: 0.0037 lr: 0.02\n",
            "iteration: 217430 loss: 0.0039 lr: 0.02\n",
            "iteration: 217440 loss: 0.0034 lr: 0.02\n",
            "iteration: 217450 loss: 0.0031 lr: 0.02\n",
            "iteration: 217460 loss: 0.0027 lr: 0.02\n",
            "iteration: 217470 loss: 0.0037 lr: 0.02\n",
            "iteration: 217480 loss: 0.0030 lr: 0.02\n",
            "iteration: 217490 loss: 0.0045 lr: 0.02\n",
            "iteration: 217500 loss: 0.0026 lr: 0.02\n",
            "iteration: 217510 loss: 0.0036 lr: 0.02\n",
            "iteration: 217520 loss: 0.0030 lr: 0.02\n",
            "iteration: 217530 loss: 0.0030 lr: 0.02\n",
            "iteration: 217540 loss: 0.0043 lr: 0.02\n",
            "iteration: 217550 loss: 0.0026 lr: 0.02\n",
            "iteration: 217560 loss: 0.0034 lr: 0.02\n",
            "iteration: 217570 loss: 0.0036 lr: 0.02\n",
            "iteration: 217580 loss: 0.0031 lr: 0.02\n",
            "iteration: 217590 loss: 0.0032 lr: 0.02\n",
            "iteration: 217600 loss: 0.0035 lr: 0.02\n",
            "iteration: 217610 loss: 0.0033 lr: 0.02\n",
            "iteration: 217620 loss: 0.0035 lr: 0.02\n",
            "iteration: 217630 loss: 0.0031 lr: 0.02\n",
            "iteration: 217640 loss: 0.0041 lr: 0.02\n",
            "iteration: 217650 loss: 0.0037 lr: 0.02\n",
            "iteration: 217660 loss: 0.0028 lr: 0.02\n",
            "iteration: 217670 loss: 0.0044 lr: 0.02\n",
            "iteration: 217680 loss: 0.0033 lr: 0.02\n",
            "iteration: 217690 loss: 0.0033 lr: 0.02\n",
            "iteration: 217700 loss: 0.0029 lr: 0.02\n",
            "iteration: 217710 loss: 0.0037 lr: 0.02\n",
            "iteration: 217720 loss: 0.0031 lr: 0.02\n",
            "iteration: 217730 loss: 0.0038 lr: 0.02\n",
            "iteration: 217740 loss: 0.0029 lr: 0.02\n",
            "iteration: 217750 loss: 0.0029 lr: 0.02\n",
            "iteration: 217760 loss: 0.0042 lr: 0.02\n",
            "iteration: 217770 loss: 0.0036 lr: 0.02\n",
            "iteration: 217780 loss: 0.0036 lr: 0.02\n",
            "iteration: 217790 loss: 0.0037 lr: 0.02\n",
            "iteration: 217800 loss: 0.0034 lr: 0.02\n",
            "iteration: 217810 loss: 0.0034 lr: 0.02\n",
            "iteration: 217820 loss: 0.0047 lr: 0.02\n",
            "iteration: 217830 loss: 0.0029 lr: 0.02\n",
            "iteration: 217840 loss: 0.0030 lr: 0.02\n",
            "iteration: 217850 loss: 0.0040 lr: 0.02\n",
            "iteration: 217860 loss: 0.0024 lr: 0.02\n",
            "iteration: 217870 loss: 0.0039 lr: 0.02\n",
            "iteration: 217880 loss: 0.0034 lr: 0.02\n",
            "iteration: 217890 loss: 0.0030 lr: 0.02\n",
            "iteration: 217900 loss: 0.0037 lr: 0.02\n",
            "iteration: 217910 loss: 0.0035 lr: 0.02\n",
            "iteration: 217920 loss: 0.0034 lr: 0.02\n",
            "iteration: 217930 loss: 0.0038 lr: 0.02\n",
            "iteration: 217940 loss: 0.0030 lr: 0.02\n",
            "iteration: 217950 loss: 0.0032 lr: 0.02\n",
            "iteration: 217960 loss: 0.0029 lr: 0.02\n",
            "iteration: 217970 loss: 0.0037 lr: 0.02\n",
            "iteration: 217980 loss: 0.0038 lr: 0.02\n",
            "iteration: 217990 loss: 0.0031 lr: 0.02\n",
            "iteration: 218000 loss: 0.0034 lr: 0.02\n",
            "iteration: 218010 loss: 0.0032 lr: 0.02\n",
            "iteration: 218020 loss: 0.0041 lr: 0.02\n",
            "iteration: 218030 loss: 0.0033 lr: 0.02\n",
            "iteration: 218040 loss: 0.0038 lr: 0.02\n",
            "iteration: 218050 loss: 0.0035 lr: 0.02\n",
            "iteration: 218060 loss: 0.0037 lr: 0.02\n",
            "iteration: 218070 loss: 0.0035 lr: 0.02\n",
            "iteration: 218080 loss: 0.0034 lr: 0.02\n",
            "iteration: 218090 loss: 0.0032 lr: 0.02\n",
            "iteration: 218100 loss: 0.0042 lr: 0.02\n",
            "iteration: 218110 loss: 0.0031 lr: 0.02\n",
            "iteration: 218120 loss: 0.0033 lr: 0.02\n",
            "iteration: 218130 loss: 0.0031 lr: 0.02\n",
            "iteration: 218140 loss: 0.0029 lr: 0.02\n",
            "iteration: 218150 loss: 0.0042 lr: 0.02\n",
            "iteration: 218160 loss: 0.0030 lr: 0.02\n",
            "iteration: 218170 loss: 0.0032 lr: 0.02\n",
            "iteration: 218180 loss: 0.0036 lr: 0.02\n",
            "iteration: 218190 loss: 0.0029 lr: 0.02\n",
            "iteration: 218200 loss: 0.0038 lr: 0.02\n",
            "iteration: 218210 loss: 0.0053 lr: 0.02\n",
            "iteration: 218220 loss: 0.0038 lr: 0.02\n",
            "iteration: 218230 loss: 0.0037 lr: 0.02\n",
            "iteration: 218240 loss: 0.0037 lr: 0.02\n",
            "iteration: 218250 loss: 0.0038 lr: 0.02\n",
            "iteration: 218260 loss: 0.0034 lr: 0.02\n",
            "iteration: 218270 loss: 0.0031 lr: 0.02\n",
            "iteration: 218280 loss: 0.0040 lr: 0.02\n",
            "iteration: 218290 loss: 0.0035 lr: 0.02\n",
            "iteration: 218300 loss: 0.0041 lr: 0.02\n",
            "iteration: 218310 loss: 0.0039 lr: 0.02\n",
            "iteration: 218320 loss: 0.0036 lr: 0.02\n",
            "iteration: 218330 loss: 0.0043 lr: 0.02\n",
            "iteration: 218340 loss: 0.0023 lr: 0.02\n",
            "iteration: 218350 loss: 0.0037 lr: 0.02\n",
            "iteration: 218360 loss: 0.0029 lr: 0.02\n",
            "iteration: 218370 loss: 0.0038 lr: 0.02\n",
            "iteration: 218380 loss: 0.0028 lr: 0.02\n",
            "iteration: 218390 loss: 0.0030 lr: 0.02\n",
            "iteration: 218400 loss: 0.0043 lr: 0.02\n",
            "iteration: 218410 loss: 0.0026 lr: 0.02\n",
            "iteration: 218420 loss: 0.0029 lr: 0.02\n",
            "iteration: 218430 loss: 0.0033 lr: 0.02\n",
            "iteration: 218440 loss: 0.0053 lr: 0.02\n",
            "iteration: 218450 loss: 0.0026 lr: 0.02\n",
            "iteration: 218460 loss: 0.0033 lr: 0.02\n",
            "iteration: 218470 loss: 0.0033 lr: 0.02\n",
            "iteration: 218480 loss: 0.0041 lr: 0.02\n",
            "iteration: 218490 loss: 0.0038 lr: 0.02\n",
            "iteration: 218500 loss: 0.0030 lr: 0.02\n",
            "iteration: 218510 loss: 0.0037 lr: 0.02\n",
            "iteration: 218520 loss: 0.0030 lr: 0.02\n",
            "iteration: 218530 loss: 0.0037 lr: 0.02\n",
            "iteration: 218540 loss: 0.0032 lr: 0.02\n",
            "iteration: 218550 loss: 0.0044 lr: 0.02\n",
            "iteration: 218560 loss: 0.0031 lr: 0.02\n",
            "iteration: 218570 loss: 0.0035 lr: 0.02\n",
            "iteration: 218580 loss: 0.0031 lr: 0.02\n",
            "iteration: 218590 loss: 0.0033 lr: 0.02\n",
            "iteration: 218600 loss: 0.0040 lr: 0.02\n",
            "iteration: 218610 loss: 0.0034 lr: 0.02\n",
            "iteration: 218620 loss: 0.0037 lr: 0.02\n",
            "iteration: 218630 loss: 0.0032 lr: 0.02\n",
            "iteration: 218640 loss: 0.0031 lr: 0.02\n",
            "iteration: 218650 loss: 0.0034 lr: 0.02\n",
            "iteration: 218660 loss: 0.0030 lr: 0.02\n",
            "iteration: 218670 loss: 0.0041 lr: 0.02\n",
            "iteration: 218680 loss: 0.0034 lr: 0.02\n",
            "iteration: 218690 loss: 0.0035 lr: 0.02\n",
            "iteration: 218700 loss: 0.0029 lr: 0.02\n",
            "iteration: 218710 loss: 0.0031 lr: 0.02\n",
            "iteration: 218720 loss: 0.0040 lr: 0.02\n",
            "iteration: 218730 loss: 0.0031 lr: 0.02\n",
            "iteration: 218740 loss: 0.0035 lr: 0.02\n",
            "iteration: 218750 loss: 0.0041 lr: 0.02\n",
            "iteration: 218760 loss: 0.0034 lr: 0.02\n",
            "iteration: 218770 loss: 0.0029 lr: 0.02\n",
            "iteration: 218780 loss: 0.0031 lr: 0.02\n",
            "iteration: 218790 loss: 0.0040 lr: 0.02\n",
            "iteration: 218800 loss: 0.0040 lr: 0.02\n",
            "iteration: 218810 loss: 0.0042 lr: 0.02\n",
            "iteration: 218820 loss: 0.0029 lr: 0.02\n",
            "iteration: 218830 loss: 0.0034 lr: 0.02\n",
            "iteration: 218840 loss: 0.0039 lr: 0.02\n",
            "iteration: 218850 loss: 0.0036 lr: 0.02\n",
            "iteration: 218860 loss: 0.0038 lr: 0.02\n",
            "iteration: 218870 loss: 0.0033 lr: 0.02\n",
            "iteration: 218880 loss: 0.0042 lr: 0.02\n",
            "iteration: 218890 loss: 0.0038 lr: 0.02\n",
            "iteration: 218900 loss: 0.0034 lr: 0.02\n",
            "iteration: 218910 loss: 0.0051 lr: 0.02\n",
            "iteration: 218920 loss: 0.0032 lr: 0.02\n",
            "iteration: 218930 loss: 0.0039 lr: 0.02\n",
            "iteration: 218940 loss: 0.0043 lr: 0.02\n",
            "iteration: 218950 loss: 0.0034 lr: 0.02\n",
            "iteration: 218960 loss: 0.0031 lr: 0.02\n",
            "iteration: 218970 loss: 0.0035 lr: 0.02\n",
            "iteration: 218980 loss: 0.0034 lr: 0.02\n",
            "iteration: 218990 loss: 0.0032 lr: 0.02\n",
            "iteration: 219000 loss: 0.0037 lr: 0.02\n",
            "iteration: 219010 loss: 0.0033 lr: 0.02\n",
            "iteration: 219020 loss: 0.0028 lr: 0.02\n",
            "iteration: 219030 loss: 0.0025 lr: 0.02\n",
            "iteration: 219040 loss: 0.0039 lr: 0.02\n",
            "iteration: 219050 loss: 0.0045 lr: 0.02\n",
            "iteration: 219060 loss: 0.0037 lr: 0.02\n",
            "iteration: 219070 loss: 0.0037 lr: 0.02\n",
            "iteration: 219080 loss: 0.0038 lr: 0.02\n",
            "iteration: 219090 loss: 0.0036 lr: 0.02\n",
            "iteration: 219100 loss: 0.0041 lr: 0.02\n",
            "iteration: 219110 loss: 0.0031 lr: 0.02\n",
            "iteration: 219120 loss: 0.0043 lr: 0.02\n",
            "iteration: 219130 loss: 0.0037 lr: 0.02\n",
            "iteration: 219140 loss: 0.0033 lr: 0.02\n",
            "iteration: 219150 loss: 0.0033 lr: 0.02\n",
            "iteration: 219160 loss: 0.0036 lr: 0.02\n",
            "iteration: 219170 loss: 0.0036 lr: 0.02\n",
            "iteration: 219180 loss: 0.0038 lr: 0.02\n",
            "iteration: 219190 loss: 0.0035 lr: 0.02\n",
            "iteration: 219200 loss: 0.0049 lr: 0.02\n",
            "iteration: 219210 loss: 0.0041 lr: 0.02\n",
            "iteration: 219220 loss: 0.0040 lr: 0.02\n",
            "iteration: 219230 loss: 0.0039 lr: 0.02\n",
            "iteration: 219240 loss: 0.0030 lr: 0.02\n",
            "iteration: 219250 loss: 0.0035 lr: 0.02\n",
            "iteration: 219260 loss: 0.0036 lr: 0.02\n",
            "iteration: 219270 loss: 0.0037 lr: 0.02\n",
            "iteration: 219280 loss: 0.0030 lr: 0.02\n",
            "iteration: 219290 loss: 0.0027 lr: 0.02\n",
            "iteration: 219300 loss: 0.0031 lr: 0.02\n",
            "iteration: 219310 loss: 0.0030 lr: 0.02\n",
            "iteration: 219320 loss: 0.0032 lr: 0.02\n",
            "iteration: 219330 loss: 0.0027 lr: 0.02\n",
            "iteration: 219340 loss: 0.0031 lr: 0.02\n",
            "iteration: 219350 loss: 0.0031 lr: 0.02\n",
            "iteration: 219360 loss: 0.0034 lr: 0.02\n",
            "iteration: 219370 loss: 0.0032 lr: 0.02\n",
            "iteration: 219380 loss: 0.0031 lr: 0.02\n",
            "iteration: 219390 loss: 0.0036 lr: 0.02\n",
            "iteration: 219400 loss: 0.0025 lr: 0.02\n",
            "iteration: 219410 loss: 0.0041 lr: 0.02\n",
            "iteration: 219420 loss: 0.0036 lr: 0.02\n",
            "iteration: 219430 loss: 0.0034 lr: 0.02\n",
            "iteration: 219440 loss: 0.0025 lr: 0.02\n",
            "iteration: 219450 loss: 0.0040 lr: 0.02\n",
            "iteration: 219460 loss: 0.0026 lr: 0.02\n",
            "iteration: 219470 loss: 0.0042 lr: 0.02\n",
            "iteration: 219480 loss: 0.0036 lr: 0.02\n",
            "iteration: 219490 loss: 0.0034 lr: 0.02\n",
            "iteration: 219500 loss: 0.0036 lr: 0.02\n",
            "iteration: 219510 loss: 0.0035 lr: 0.02\n",
            "iteration: 219520 loss: 0.0033 lr: 0.02\n",
            "iteration: 219530 loss: 0.0037 lr: 0.02\n",
            "iteration: 219540 loss: 0.0030 lr: 0.02\n",
            "iteration: 219550 loss: 0.0038 lr: 0.02\n",
            "iteration: 219560 loss: 0.0035 lr: 0.02\n",
            "iteration: 219570 loss: 0.0034 lr: 0.02\n",
            "iteration: 219580 loss: 0.0041 lr: 0.02\n",
            "iteration: 219590 loss: 0.0029 lr: 0.02\n",
            "iteration: 219600 loss: 0.0031 lr: 0.02\n",
            "iteration: 219610 loss: 0.0030 lr: 0.02\n",
            "iteration: 219620 loss: 0.0043 lr: 0.02\n",
            "iteration: 219630 loss: 0.0032 lr: 0.02\n",
            "iteration: 219640 loss: 0.0032 lr: 0.02\n",
            "iteration: 219650 loss: 0.0043 lr: 0.02\n",
            "iteration: 219660 loss: 0.0042 lr: 0.02\n",
            "iteration: 219670 loss: 0.0041 lr: 0.02\n",
            "iteration: 219680 loss: 0.0033 lr: 0.02\n",
            "iteration: 219690 loss: 0.0037 lr: 0.02\n",
            "iteration: 219700 loss: 0.0038 lr: 0.02\n",
            "iteration: 219710 loss: 0.0036 lr: 0.02\n",
            "iteration: 219720 loss: 0.0031 lr: 0.02\n",
            "iteration: 219730 loss: 0.0025 lr: 0.02\n",
            "iteration: 219740 loss: 0.0031 lr: 0.02\n",
            "iteration: 219750 loss: 0.0028 lr: 0.02\n",
            "iteration: 219760 loss: 0.0037 lr: 0.02\n",
            "iteration: 219770 loss: 0.0034 lr: 0.02\n",
            "iteration: 219780 loss: 0.0032 lr: 0.02\n",
            "iteration: 219790 loss: 0.0033 lr: 0.02\n",
            "iteration: 219800 loss: 0.0033 lr: 0.02\n",
            "iteration: 219810 loss: 0.0039 lr: 0.02\n",
            "iteration: 219820 loss: 0.0033 lr: 0.02\n",
            "iteration: 219830 loss: 0.0029 lr: 0.02\n",
            "iteration: 219840 loss: 0.0023 lr: 0.02\n",
            "iteration: 219850 loss: 0.0028 lr: 0.02\n",
            "iteration: 219860 loss: 0.0041 lr: 0.02\n",
            "iteration: 219870 loss: 0.0040 lr: 0.02\n",
            "iteration: 219880 loss: 0.0031 lr: 0.02\n",
            "iteration: 219890 loss: 0.0031 lr: 0.02\n",
            "iteration: 219900 loss: 0.0028 lr: 0.02\n",
            "iteration: 219910 loss: 0.0034 lr: 0.02\n",
            "iteration: 219920 loss: 0.0041 lr: 0.02\n",
            "iteration: 219930 loss: 0.0037 lr: 0.02\n",
            "iteration: 219940 loss: 0.0033 lr: 0.02\n",
            "iteration: 219950 loss: 0.0035 lr: 0.02\n",
            "iteration: 219960 loss: 0.0032 lr: 0.02\n",
            "iteration: 219970 loss: 0.0034 lr: 0.02\n",
            "iteration: 219980 loss: 0.0039 lr: 0.02\n",
            "iteration: 219990 loss: 0.0034 lr: 0.02\n",
            "iteration: 220000 loss: 0.0042 lr: 0.02\n",
            "iteration: 220010 loss: 0.0024 lr: 0.02\n",
            "iteration: 220020 loss: 0.0036 lr: 0.02\n",
            "iteration: 220030 loss: 0.0041 lr: 0.02\n",
            "iteration: 220040 loss: 0.0039 lr: 0.02\n",
            "iteration: 220050 loss: 0.0032 lr: 0.02\n",
            "iteration: 220060 loss: 0.0032 lr: 0.02\n",
            "iteration: 220070 loss: 0.0032 lr: 0.02\n",
            "iteration: 220080 loss: 0.0033 lr: 0.02\n",
            "iteration: 220090 loss: 0.0033 lr: 0.02\n",
            "iteration: 220100 loss: 0.0040 lr: 0.02\n",
            "iteration: 220110 loss: 0.0030 lr: 0.02\n",
            "iteration: 220120 loss: 0.0036 lr: 0.02\n",
            "iteration: 220130 loss: 0.0036 lr: 0.02\n",
            "iteration: 220140 loss: 0.0037 lr: 0.02\n",
            "iteration: 220150 loss: 0.0033 lr: 0.02\n",
            "iteration: 220160 loss: 0.0033 lr: 0.02\n",
            "iteration: 220170 loss: 0.0038 lr: 0.02\n",
            "iteration: 220180 loss: 0.0042 lr: 0.02\n",
            "iteration: 220190 loss: 0.0034 lr: 0.02\n",
            "iteration: 220200 loss: 0.0031 lr: 0.02\n",
            "iteration: 220210 loss: 0.0048 lr: 0.02\n",
            "iteration: 220220 loss: 0.0033 lr: 0.02\n",
            "iteration: 220230 loss: 0.0034 lr: 0.02\n",
            "iteration: 220240 loss: 0.0033 lr: 0.02\n",
            "iteration: 220250 loss: 0.0035 lr: 0.02\n",
            "iteration: 220260 loss: 0.0054 lr: 0.02\n",
            "iteration: 220270 loss: 0.0057 lr: 0.02\n",
            "iteration: 220280 loss: 0.0042 lr: 0.02\n",
            "iteration: 220290 loss: 0.0047 lr: 0.02\n",
            "iteration: 220300 loss: 0.0042 lr: 0.02\n",
            "iteration: 220310 loss: 0.0043 lr: 0.02\n",
            "iteration: 220320 loss: 0.0036 lr: 0.02\n",
            "iteration: 220330 loss: 0.0037 lr: 0.02\n",
            "iteration: 220340 loss: 0.0039 lr: 0.02\n",
            "iteration: 220350 loss: 0.0031 lr: 0.02\n",
            "iteration: 220360 loss: 0.0032 lr: 0.02\n",
            "iteration: 220370 loss: 0.0035 lr: 0.02\n",
            "iteration: 220380 loss: 0.0032 lr: 0.02\n",
            "iteration: 220390 loss: 0.0037 lr: 0.02\n",
            "iteration: 220400 loss: 0.0027 lr: 0.02\n",
            "iteration: 220410 loss: 0.0039 lr: 0.02\n",
            "iteration: 220420 loss: 0.0029 lr: 0.02\n",
            "iteration: 220430 loss: 0.0044 lr: 0.02\n",
            "iteration: 220440 loss: 0.0031 lr: 0.02\n",
            "iteration: 220450 loss: 0.0032 lr: 0.02\n",
            "iteration: 220460 loss: 0.0029 lr: 0.02\n",
            "iteration: 220470 loss: 0.0029 lr: 0.02\n",
            "iteration: 220480 loss: 0.0041 lr: 0.02\n",
            "iteration: 220490 loss: 0.0028 lr: 0.02\n",
            "iteration: 220500 loss: 0.0032 lr: 0.02\n",
            "iteration: 220510 loss: 0.0034 lr: 0.02\n",
            "iteration: 220520 loss: 0.0037 lr: 0.02\n",
            "iteration: 220530 loss: 0.0027 lr: 0.02\n",
            "iteration: 220540 loss: 0.0034 lr: 0.02\n",
            "iteration: 220550 loss: 0.0034 lr: 0.02\n",
            "iteration: 220560 loss: 0.0039 lr: 0.02\n",
            "iteration: 220570 loss: 0.0032 lr: 0.02\n",
            "iteration: 220580 loss: 0.0034 lr: 0.02\n",
            "iteration: 220590 loss: 0.0043 lr: 0.02\n",
            "iteration: 220600 loss: 0.0030 lr: 0.02\n",
            "iteration: 220610 loss: 0.0038 lr: 0.02\n",
            "iteration: 220620 loss: 0.0042 lr: 0.02\n",
            "iteration: 220630 loss: 0.0036 lr: 0.02\n",
            "iteration: 220640 loss: 0.0036 lr: 0.02\n",
            "iteration: 220650 loss: 0.0029 lr: 0.02\n",
            "iteration: 220660 loss: 0.0025 lr: 0.02\n",
            "iteration: 220670 loss: 0.0032 lr: 0.02\n",
            "iteration: 220680 loss: 0.0026 lr: 0.02\n",
            "iteration: 220690 loss: 0.0036 lr: 0.02\n",
            "iteration: 220700 loss: 0.0031 lr: 0.02\n",
            "iteration: 220710 loss: 0.0030 lr: 0.02\n",
            "iteration: 220720 loss: 0.0036 lr: 0.02\n",
            "iteration: 220730 loss: 0.0032 lr: 0.02\n",
            "iteration: 220740 loss: 0.0036 lr: 0.02\n",
            "iteration: 220750 loss: 0.0039 lr: 0.02\n",
            "iteration: 220760 loss: 0.0033 lr: 0.02\n",
            "iteration: 220770 loss: 0.0039 lr: 0.02\n",
            "iteration: 220780 loss: 0.0033 lr: 0.02\n",
            "iteration: 220790 loss: 0.0047 lr: 0.02\n",
            "iteration: 220800 loss: 0.0039 lr: 0.02\n",
            "iteration: 220810 loss: 0.0050 lr: 0.02\n",
            "iteration: 220820 loss: 0.0040 lr: 0.02\n",
            "iteration: 220830 loss: 0.0033 lr: 0.02\n",
            "iteration: 220840 loss: 0.0035 lr: 0.02\n",
            "iteration: 220850 loss: 0.0027 lr: 0.02\n",
            "iteration: 220860 loss: 0.0027 lr: 0.02\n",
            "iteration: 220870 loss: 0.0040 lr: 0.02\n",
            "iteration: 220880 loss: 0.0045 lr: 0.02\n",
            "iteration: 220890 loss: 0.0035 lr: 0.02\n",
            "iteration: 220900 loss: 0.0040 lr: 0.02\n",
            "iteration: 220910 loss: 0.0037 lr: 0.02\n",
            "iteration: 220920 loss: 0.0027 lr: 0.02\n",
            "iteration: 220930 loss: 0.0029 lr: 0.02\n",
            "iteration: 220940 loss: 0.0030 lr: 0.02\n",
            "iteration: 220950 loss: 0.0039 lr: 0.02\n",
            "iteration: 220960 loss: 0.0030 lr: 0.02\n",
            "iteration: 220970 loss: 0.0025 lr: 0.02\n",
            "iteration: 220980 loss: 0.0033 lr: 0.02\n",
            "iteration: 220990 loss: 0.0040 lr: 0.02\n",
            "iteration: 221000 loss: 0.0030 lr: 0.02\n",
            "iteration: 221010 loss: 0.0029 lr: 0.02\n",
            "iteration: 221020 loss: 0.0036 lr: 0.02\n",
            "iteration: 221030 loss: 0.0032 lr: 0.02\n",
            "iteration: 221040 loss: 0.0032 lr: 0.02\n",
            "iteration: 221050 loss: 0.0041 lr: 0.02\n",
            "iteration: 221060 loss: 0.0041 lr: 0.02\n",
            "iteration: 221070 loss: 0.0035 lr: 0.02\n",
            "iteration: 221080 loss: 0.0033 lr: 0.02\n",
            "iteration: 221090 loss: 0.0031 lr: 0.02\n",
            "iteration: 221100 loss: 0.0033 lr: 0.02\n",
            "iteration: 221110 loss: 0.0031 lr: 0.02\n",
            "iteration: 221120 loss: 0.0035 lr: 0.02\n",
            "iteration: 221130 loss: 0.0027 lr: 0.02\n",
            "iteration: 221140 loss: 0.0048 lr: 0.02\n",
            "iteration: 221150 loss: 0.0030 lr: 0.02\n",
            "iteration: 221160 loss: 0.0028 lr: 0.02\n",
            "iteration: 221170 loss: 0.0040 lr: 0.02\n",
            "iteration: 221180 loss: 0.0030 lr: 0.02\n",
            "iteration: 221190 loss: 0.0031 lr: 0.02\n",
            "iteration: 221200 loss: 0.0036 lr: 0.02\n",
            "iteration: 221210 loss: 0.0029 lr: 0.02\n",
            "iteration: 221220 loss: 0.0029 lr: 0.02\n",
            "iteration: 221230 loss: 0.0032 lr: 0.02\n",
            "iteration: 221240 loss: 0.0028 lr: 0.02\n",
            "iteration: 221250 loss: 0.0036 lr: 0.02\n",
            "iteration: 221260 loss: 0.0032 lr: 0.02\n",
            "iteration: 221270 loss: 0.0032 lr: 0.02\n",
            "iteration: 221280 loss: 0.0033 lr: 0.02\n",
            "iteration: 221290 loss: 0.0042 lr: 0.02\n",
            "iteration: 221300 loss: 0.0038 lr: 0.02\n",
            "iteration: 221310 loss: 0.0046 lr: 0.02\n",
            "iteration: 221320 loss: 0.0037 lr: 0.02\n",
            "iteration: 221330 loss: 0.0027 lr: 0.02\n",
            "iteration: 221340 loss: 0.0040 lr: 0.02\n",
            "iteration: 221350 loss: 0.0041 lr: 0.02\n",
            "iteration: 221360 loss: 0.0035 lr: 0.02\n",
            "iteration: 221370 loss: 0.0034 lr: 0.02\n",
            "iteration: 221380 loss: 0.0036 lr: 0.02\n",
            "iteration: 221390 loss: 0.0046 lr: 0.02\n",
            "iteration: 221400 loss: 0.0040 lr: 0.02\n",
            "iteration: 221410 loss: 0.0036 lr: 0.02\n",
            "iteration: 221420 loss: 0.0037 lr: 0.02\n",
            "iteration: 221430 loss: 0.0033 lr: 0.02\n",
            "iteration: 221440 loss: 0.0036 lr: 0.02\n",
            "iteration: 221450 loss: 0.0033 lr: 0.02\n",
            "iteration: 221460 loss: 0.0031 lr: 0.02\n",
            "iteration: 221470 loss: 0.0049 lr: 0.02\n",
            "iteration: 221480 loss: 0.0030 lr: 0.02\n",
            "iteration: 221490 loss: 0.0034 lr: 0.02\n",
            "iteration: 221500 loss: 0.0041 lr: 0.02\n",
            "iteration: 221510 loss: 0.0038 lr: 0.02\n",
            "iteration: 221520 loss: 0.0035 lr: 0.02\n",
            "iteration: 221530 loss: 0.0038 lr: 0.02\n",
            "iteration: 221540 loss: 0.0037 lr: 0.02\n",
            "iteration: 221550 loss: 0.0049 lr: 0.02\n",
            "iteration: 221560 loss: 0.0033 lr: 0.02\n",
            "iteration: 221570 loss: 0.0031 lr: 0.02\n",
            "iteration: 221580 loss: 0.0031 lr: 0.02\n",
            "iteration: 221590 loss: 0.0039 lr: 0.02\n",
            "iteration: 221600 loss: 0.0030 lr: 0.02\n",
            "iteration: 221610 loss: 0.0040 lr: 0.02\n",
            "iteration: 221620 loss: 0.0035 lr: 0.02\n",
            "iteration: 221630 loss: 0.0026 lr: 0.02\n",
            "iteration: 221640 loss: 0.0038 lr: 0.02\n",
            "iteration: 221650 loss: 0.0041 lr: 0.02\n",
            "iteration: 221660 loss: 0.0040 lr: 0.02\n",
            "iteration: 221670 loss: 0.0039 lr: 0.02\n",
            "iteration: 221680 loss: 0.0035 lr: 0.02\n",
            "iteration: 221690 loss: 0.0036 lr: 0.02\n",
            "iteration: 221700 loss: 0.0035 lr: 0.02\n",
            "iteration: 221710 loss: 0.0032 lr: 0.02\n",
            "iteration: 221720 loss: 0.0037 lr: 0.02\n",
            "iteration: 221730 loss: 0.0039 lr: 0.02\n",
            "iteration: 221740 loss: 0.0034 lr: 0.02\n",
            "iteration: 221750 loss: 0.0037 lr: 0.02\n",
            "iteration: 221760 loss: 0.0028 lr: 0.02\n",
            "iteration: 221770 loss: 0.0042 lr: 0.02\n",
            "iteration: 221780 loss: 0.0039 lr: 0.02\n",
            "iteration: 221790 loss: 0.0044 lr: 0.02\n",
            "iteration: 221800 loss: 0.0042 lr: 0.02\n",
            "iteration: 221810 loss: 0.0038 lr: 0.02\n",
            "iteration: 221820 loss: 0.0046 lr: 0.02\n",
            "iteration: 221830 loss: 0.0035 lr: 0.02\n",
            "iteration: 221840 loss: 0.0044 lr: 0.02\n",
            "iteration: 221850 loss: 0.0036 lr: 0.02\n",
            "iteration: 221860 loss: 0.0048 lr: 0.02\n",
            "iteration: 221870 loss: 0.0029 lr: 0.02\n",
            "iteration: 221880 loss: 0.0037 lr: 0.02\n",
            "iteration: 221890 loss: 0.0030 lr: 0.02\n",
            "iteration: 221900 loss: 0.0034 lr: 0.02\n",
            "iteration: 221910 loss: 0.0036 lr: 0.02\n",
            "iteration: 221920 loss: 0.0038 lr: 0.02\n",
            "iteration: 221930 loss: 0.0037 lr: 0.02\n",
            "iteration: 221940 loss: 0.0031 lr: 0.02\n",
            "iteration: 221950 loss: 0.0028 lr: 0.02\n",
            "iteration: 221960 loss: 0.0034 lr: 0.02\n",
            "iteration: 221970 loss: 0.0037 lr: 0.02\n",
            "iteration: 221980 loss: 0.0032 lr: 0.02\n",
            "iteration: 221990 loss: 0.0032 lr: 0.02\n",
            "iteration: 222000 loss: 0.0040 lr: 0.02\n",
            "iteration: 222010 loss: 0.0038 lr: 0.02\n",
            "iteration: 222020 loss: 0.0037 lr: 0.02\n",
            "iteration: 222030 loss: 0.0041 lr: 0.02\n",
            "iteration: 222040 loss: 0.0034 lr: 0.02\n",
            "iteration: 222050 loss: 0.0029 lr: 0.02\n",
            "iteration: 222060 loss: 0.0034 lr: 0.02\n",
            "iteration: 222070 loss: 0.0036 lr: 0.02\n",
            "iteration: 222080 loss: 0.0031 lr: 0.02\n",
            "iteration: 222090 loss: 0.0029 lr: 0.02\n",
            "iteration: 222100 loss: 0.0029 lr: 0.02\n",
            "iteration: 222110 loss: 0.0025 lr: 0.02\n",
            "iteration: 222120 loss: 0.0035 lr: 0.02\n",
            "iteration: 222130 loss: 0.0024 lr: 0.02\n",
            "iteration: 222140 loss: 0.0036 lr: 0.02\n",
            "iteration: 222150 loss: 0.0031 lr: 0.02\n",
            "iteration: 222160 loss: 0.0043 lr: 0.02\n",
            "iteration: 222170 loss: 0.0042 lr: 0.02\n",
            "iteration: 222180 loss: 0.0047 lr: 0.02\n",
            "iteration: 222190 loss: 0.0038 lr: 0.02\n",
            "iteration: 222200 loss: 0.0047 lr: 0.02\n",
            "iteration: 222210 loss: 0.0036 lr: 0.02\n",
            "iteration: 222220 loss: 0.0036 lr: 0.02\n",
            "iteration: 222230 loss: 0.0031 lr: 0.02\n",
            "iteration: 222240 loss: 0.0040 lr: 0.02\n",
            "iteration: 222250 loss: 0.0044 lr: 0.02\n",
            "iteration: 222260 loss: 0.0033 lr: 0.02\n",
            "iteration: 222270 loss: 0.0032 lr: 0.02\n",
            "iteration: 222280 loss: 0.0036 lr: 0.02\n",
            "iteration: 222290 loss: 0.0031 lr: 0.02\n",
            "iteration: 222300 loss: 0.0029 lr: 0.02\n",
            "iteration: 222310 loss: 0.0034 lr: 0.02\n",
            "iteration: 222320 loss: 0.0033 lr: 0.02\n",
            "iteration: 222330 loss: 0.0030 lr: 0.02\n",
            "iteration: 222340 loss: 0.0028 lr: 0.02\n",
            "iteration: 222350 loss: 0.0033 lr: 0.02\n",
            "iteration: 222360 loss: 0.0034 lr: 0.02\n",
            "iteration: 222370 loss: 0.0033 lr: 0.02\n",
            "iteration: 222380 loss: 0.0032 lr: 0.02\n",
            "iteration: 222390 loss: 0.0042 lr: 0.02\n",
            "iteration: 222400 loss: 0.0033 lr: 0.02\n",
            "iteration: 222410 loss: 0.0026 lr: 0.02\n",
            "iteration: 222420 loss: 0.0037 lr: 0.02\n",
            "iteration: 222430 loss: 0.0038 lr: 0.02\n",
            "iteration: 222440 loss: 0.0036 lr: 0.02\n",
            "iteration: 222450 loss: 0.0027 lr: 0.02\n",
            "iteration: 222460 loss: 0.0032 lr: 0.02\n",
            "iteration: 222470 loss: 0.0031 lr: 0.02\n",
            "iteration: 222480 loss: 0.0040 lr: 0.02\n",
            "iteration: 222490 loss: 0.0034 lr: 0.02\n",
            "iteration: 222500 loss: 0.0034 lr: 0.02\n",
            "iteration: 222510 loss: 0.0035 lr: 0.02\n",
            "iteration: 222520 loss: 0.0031 lr: 0.02\n",
            "iteration: 222530 loss: 0.0034 lr: 0.02\n",
            "iteration: 222540 loss: 0.0035 lr: 0.02\n",
            "iteration: 222550 loss: 0.0027 lr: 0.02\n",
            "iteration: 222560 loss: 0.0029 lr: 0.02\n",
            "iteration: 222570 loss: 0.0030 lr: 0.02\n",
            "iteration: 222580 loss: 0.0029 lr: 0.02\n",
            "iteration: 222590 loss: 0.0036 lr: 0.02\n",
            "iteration: 222600 loss: 0.0029 lr: 0.02\n",
            "iteration: 222610 loss: 0.0032 lr: 0.02\n",
            "iteration: 222620 loss: 0.0037 lr: 0.02\n",
            "iteration: 222630 loss: 0.0033 lr: 0.02\n",
            "iteration: 222640 loss: 0.0035 lr: 0.02\n",
            "iteration: 222650 loss: 0.0025 lr: 0.02\n",
            "iteration: 222660 loss: 0.0036 lr: 0.02\n",
            "iteration: 222670 loss: 0.0042 lr: 0.02\n",
            "iteration: 222680 loss: 0.0033 lr: 0.02\n",
            "iteration: 222690 loss: 0.0031 lr: 0.02\n",
            "iteration: 222700 loss: 0.0038 lr: 0.02\n",
            "iteration: 222710 loss: 0.0027 lr: 0.02\n",
            "iteration: 222720 loss: 0.0032 lr: 0.02\n",
            "iteration: 222730 loss: 0.0040 lr: 0.02\n",
            "iteration: 222740 loss: 0.0039 lr: 0.02\n",
            "iteration: 222750 loss: 0.0031 lr: 0.02\n",
            "iteration: 222760 loss: 0.0039 lr: 0.02\n",
            "iteration: 222770 loss: 0.0028 lr: 0.02\n",
            "iteration: 222780 loss: 0.0028 lr: 0.02\n",
            "iteration: 222790 loss: 0.0035 lr: 0.02\n",
            "iteration: 222800 loss: 0.0037 lr: 0.02\n",
            "iteration: 222810 loss: 0.0039 lr: 0.02\n",
            "iteration: 222820 loss: 0.0026 lr: 0.02\n",
            "iteration: 222830 loss: 0.0041 lr: 0.02\n",
            "iteration: 222840 loss: 0.0034 lr: 0.02\n",
            "iteration: 222850 loss: 0.0039 lr: 0.02\n",
            "iteration: 222860 loss: 0.0035 lr: 0.02\n",
            "iteration: 222870 loss: 0.0034 lr: 0.02\n",
            "iteration: 222880 loss: 0.0027 lr: 0.02\n",
            "iteration: 222890 loss: 0.0030 lr: 0.02\n",
            "iteration: 222900 loss: 0.0034 lr: 0.02\n",
            "iteration: 222910 loss: 0.0024 lr: 0.02\n",
            "iteration: 222920 loss: 0.0030 lr: 0.02\n",
            "iteration: 222930 loss: 0.0034 lr: 0.02\n",
            "iteration: 222940 loss: 0.0034 lr: 0.02\n",
            "iteration: 222950 loss: 0.0033 lr: 0.02\n",
            "iteration: 222960 loss: 0.0037 lr: 0.02\n",
            "iteration: 222970 loss: 0.0037 lr: 0.02\n",
            "iteration: 222980 loss: 0.0040 lr: 0.02\n",
            "iteration: 222990 loss: 0.0031 lr: 0.02\n",
            "iteration: 223000 loss: 0.0039 lr: 0.02\n",
            "iteration: 223010 loss: 0.0044 lr: 0.02\n",
            "iteration: 223020 loss: 0.0036 lr: 0.02\n",
            "iteration: 223030 loss: 0.0036 lr: 0.02\n",
            "iteration: 223040 loss: 0.0029 lr: 0.02\n",
            "iteration: 223050 loss: 0.0037 lr: 0.02\n",
            "iteration: 223060 loss: 0.0040 lr: 0.02\n",
            "iteration: 223070 loss: 0.0038 lr: 0.02\n",
            "iteration: 223080 loss: 0.0039 lr: 0.02\n",
            "iteration: 223090 loss: 0.0036 lr: 0.02\n",
            "iteration: 223100 loss: 0.0036 lr: 0.02\n",
            "iteration: 223110 loss: 0.0029 lr: 0.02\n",
            "iteration: 223120 loss: 0.0036 lr: 0.02\n",
            "iteration: 223130 loss: 0.0033 lr: 0.02\n",
            "iteration: 223140 loss: 0.0035 lr: 0.02\n",
            "iteration: 223150 loss: 0.0035 lr: 0.02\n",
            "iteration: 223160 loss: 0.0028 lr: 0.02\n",
            "iteration: 223170 loss: 0.0032 lr: 0.02\n",
            "iteration: 223180 loss: 0.0027 lr: 0.02\n",
            "iteration: 223190 loss: 0.0024 lr: 0.02\n",
            "iteration: 223200 loss: 0.0039 lr: 0.02\n",
            "iteration: 223210 loss: 0.0041 lr: 0.02\n",
            "iteration: 223220 loss: 0.0036 lr: 0.02\n",
            "iteration: 223230 loss: 0.0033 lr: 0.02\n",
            "iteration: 223240 loss: 0.0030 lr: 0.02\n",
            "iteration: 223250 loss: 0.0032 lr: 0.02\n",
            "iteration: 223260 loss: 0.0037 lr: 0.02\n",
            "iteration: 223270 loss: 0.0037 lr: 0.02\n",
            "iteration: 223280 loss: 0.0036 lr: 0.02\n",
            "iteration: 223290 loss: 0.0029 lr: 0.02\n",
            "iteration: 223300 loss: 0.0031 lr: 0.02\n",
            "iteration: 223310 loss: 0.0031 lr: 0.02\n",
            "iteration: 223320 loss: 0.0033 lr: 0.02\n",
            "iteration: 223330 loss: 0.0035 lr: 0.02\n",
            "iteration: 223340 loss: 0.0038 lr: 0.02\n",
            "iteration: 223350 loss: 0.0044 lr: 0.02\n",
            "iteration: 223360 loss: 0.0033 lr: 0.02\n",
            "iteration: 223370 loss: 0.0031 lr: 0.02\n",
            "iteration: 223380 loss: 0.0029 lr: 0.02\n",
            "iteration: 223390 loss: 0.0032 lr: 0.02\n",
            "iteration: 223400 loss: 0.0025 lr: 0.02\n",
            "iteration: 223410 loss: 0.0039 lr: 0.02\n",
            "iteration: 223420 loss: 0.0036 lr: 0.02\n",
            "iteration: 223430 loss: 0.0034 lr: 0.02\n",
            "iteration: 223440 loss: 0.0039 lr: 0.02\n",
            "iteration: 223450 loss: 0.0034 lr: 0.02\n",
            "iteration: 223460 loss: 0.0033 lr: 0.02\n",
            "iteration: 223470 loss: 0.0034 lr: 0.02\n",
            "iteration: 223480 loss: 0.0030 lr: 0.02\n",
            "iteration: 223490 loss: 0.0035 lr: 0.02\n",
            "iteration: 223500 loss: 0.0026 lr: 0.02\n",
            "iteration: 223510 loss: 0.0033 lr: 0.02\n",
            "iteration: 223520 loss: 0.0030 lr: 0.02\n",
            "iteration: 223530 loss: 0.0026 lr: 0.02\n",
            "iteration: 223540 loss: 0.0031 lr: 0.02\n",
            "iteration: 223550 loss: 0.0030 lr: 0.02\n",
            "iteration: 223560 loss: 0.0026 lr: 0.02\n",
            "iteration: 223570 loss: 0.0038 lr: 0.02\n",
            "iteration: 223580 loss: 0.0027 lr: 0.02\n",
            "iteration: 223590 loss: 0.0036 lr: 0.02\n",
            "iteration: 223600 loss: 0.0033 lr: 0.02\n",
            "iteration: 223610 loss: 0.0040 lr: 0.02\n",
            "iteration: 223620 loss: 0.0031 lr: 0.02\n",
            "iteration: 223630 loss: 0.0040 lr: 0.02\n",
            "iteration: 223640 loss: 0.0028 lr: 0.02\n",
            "iteration: 223650 loss: 0.0038 lr: 0.02\n",
            "iteration: 223660 loss: 0.0035 lr: 0.02\n",
            "iteration: 223670 loss: 0.0031 lr: 0.02\n",
            "iteration: 223680 loss: 0.0039 lr: 0.02\n",
            "iteration: 223690 loss: 0.0036 lr: 0.02\n",
            "iteration: 223700 loss: 0.0033 lr: 0.02\n",
            "iteration: 223710 loss: 0.0031 lr: 0.02\n",
            "iteration: 223720 loss: 0.0040 lr: 0.02\n",
            "iteration: 223730 loss: 0.0040 lr: 0.02\n",
            "iteration: 223740 loss: 0.0037 lr: 0.02\n",
            "iteration: 223750 loss: 0.0035 lr: 0.02\n",
            "iteration: 223760 loss: 0.0037 lr: 0.02\n",
            "iteration: 223770 loss: 0.0035 lr: 0.02\n",
            "iteration: 223780 loss: 0.0029 lr: 0.02\n",
            "iteration: 223790 loss: 0.0046 lr: 0.02\n",
            "iteration: 223800 loss: 0.0031 lr: 0.02\n",
            "iteration: 223810 loss: 0.0028 lr: 0.02\n",
            "iteration: 223820 loss: 0.0030 lr: 0.02\n",
            "iteration: 223830 loss: 0.0026 lr: 0.02\n",
            "iteration: 223840 loss: 0.0030 lr: 0.02\n",
            "iteration: 223850 loss: 0.0031 lr: 0.02\n",
            "iteration: 223860 loss: 0.0031 lr: 0.02\n",
            "iteration: 223870 loss: 0.0036 lr: 0.02\n",
            "iteration: 223880 loss: 0.0036 lr: 0.02\n",
            "iteration: 223890 loss: 0.0031 lr: 0.02\n",
            "iteration: 223900 loss: 0.0035 lr: 0.02\n",
            "iteration: 223910 loss: 0.0048 lr: 0.02\n",
            "iteration: 223920 loss: 0.0046 lr: 0.02\n",
            "iteration: 223930 loss: 0.0033 lr: 0.02\n",
            "iteration: 223940 loss: 0.0034 lr: 0.02\n",
            "iteration: 223950 loss: 0.0031 lr: 0.02\n",
            "iteration: 223960 loss: 0.0029 lr: 0.02\n",
            "iteration: 223970 loss: 0.0033 lr: 0.02\n",
            "iteration: 223980 loss: 0.0031 lr: 0.02\n",
            "iteration: 223990 loss: 0.0044 lr: 0.02\n",
            "iteration: 224000 loss: 0.0036 lr: 0.02\n",
            "iteration: 224010 loss: 0.0034 lr: 0.02\n",
            "iteration: 224020 loss: 0.0028 lr: 0.02\n",
            "iteration: 224030 loss: 0.0023 lr: 0.02\n",
            "iteration: 224040 loss: 0.0042 lr: 0.02\n",
            "iteration: 224050 loss: 0.0038 lr: 0.02\n",
            "iteration: 224060 loss: 0.0028 lr: 0.02\n",
            "iteration: 224070 loss: 0.0043 lr: 0.02\n",
            "iteration: 224080 loss: 0.0032 lr: 0.02\n",
            "iteration: 224090 loss: 0.0033 lr: 0.02\n",
            "iteration: 224100 loss: 0.0031 lr: 0.02\n",
            "iteration: 224110 loss: 0.0041 lr: 0.02\n",
            "iteration: 224120 loss: 0.0026 lr: 0.02\n",
            "iteration: 224130 loss: 0.0032 lr: 0.02\n",
            "iteration: 224140 loss: 0.0038 lr: 0.02\n",
            "iteration: 224150 loss: 0.0042 lr: 0.02\n",
            "iteration: 224160 loss: 0.0034 lr: 0.02\n",
            "iteration: 224170 loss: 0.0029 lr: 0.02\n",
            "iteration: 224180 loss: 0.0030 lr: 0.02\n",
            "iteration: 224190 loss: 0.0034 lr: 0.02\n",
            "iteration: 224200 loss: 0.0031 lr: 0.02\n",
            "iteration: 224210 loss: 0.0031 lr: 0.02\n",
            "iteration: 224220 loss: 0.0038 lr: 0.02\n",
            "iteration: 224230 loss: 0.0032 lr: 0.02\n",
            "iteration: 224240 loss: 0.0034 lr: 0.02\n",
            "iteration: 224250 loss: 0.0030 lr: 0.02\n",
            "iteration: 224260 loss: 0.0032 lr: 0.02\n",
            "iteration: 224270 loss: 0.0036 lr: 0.02\n",
            "iteration: 224280 loss: 0.0033 lr: 0.02\n",
            "iteration: 224290 loss: 0.0031 lr: 0.02\n",
            "iteration: 224300 loss: 0.0030 lr: 0.02\n",
            "iteration: 224310 loss: 0.0039 lr: 0.02\n",
            "iteration: 224320 loss: 0.0030 lr: 0.02\n",
            "iteration: 224330 loss: 0.0040 lr: 0.02\n",
            "iteration: 224340 loss: 0.0038 lr: 0.02\n",
            "iteration: 224350 loss: 0.0033 lr: 0.02\n",
            "iteration: 224360 loss: 0.0032 lr: 0.02\n",
            "iteration: 224370 loss: 0.0038 lr: 0.02\n",
            "iteration: 224380 loss: 0.0041 lr: 0.02\n",
            "iteration: 224390 loss: 0.0031 lr: 0.02\n",
            "iteration: 224400 loss: 0.0037 lr: 0.02\n",
            "iteration: 224410 loss: 0.0035 lr: 0.02\n",
            "iteration: 224420 loss: 0.0042 lr: 0.02\n",
            "iteration: 224430 loss: 0.0035 lr: 0.02\n",
            "iteration: 224440 loss: 0.0035 lr: 0.02\n",
            "iteration: 224450 loss: 0.0040 lr: 0.02\n",
            "iteration: 224460 loss: 0.0046 lr: 0.02\n",
            "iteration: 224470 loss: 0.0031 lr: 0.02\n",
            "iteration: 224480 loss: 0.0038 lr: 0.02\n",
            "iteration: 224490 loss: 0.0031 lr: 0.02\n",
            "iteration: 224500 loss: 0.0033 lr: 0.02\n",
            "iteration: 224510 loss: 0.0030 lr: 0.02\n",
            "iteration: 224520 loss: 0.0037 lr: 0.02\n",
            "iteration: 224530 loss: 0.0035 lr: 0.02\n",
            "iteration: 224540 loss: 0.0033 lr: 0.02\n",
            "iteration: 224550 loss: 0.0033 lr: 0.02\n",
            "iteration: 224560 loss: 0.0028 lr: 0.02\n",
            "iteration: 224570 loss: 0.0040 lr: 0.02\n",
            "iteration: 224580 loss: 0.0030 lr: 0.02\n",
            "iteration: 224590 loss: 0.0031 lr: 0.02\n",
            "iteration: 224600 loss: 0.0030 lr: 0.02\n",
            "iteration: 224610 loss: 0.0026 lr: 0.02\n",
            "iteration: 224620 loss: 0.0035 lr: 0.02\n",
            "iteration: 224630 loss: 0.0031 lr: 0.02\n",
            "iteration: 224640 loss: 0.0038 lr: 0.02\n",
            "iteration: 224650 loss: 0.0036 lr: 0.02\n",
            "iteration: 224660 loss: 0.0041 lr: 0.02\n",
            "iteration: 224670 loss: 0.0027 lr: 0.02\n",
            "iteration: 224680 loss: 0.0029 lr: 0.02\n",
            "iteration: 224690 loss: 0.0042 lr: 0.02\n",
            "iteration: 224700 loss: 0.0035 lr: 0.02\n",
            "iteration: 224710 loss: 0.0033 lr: 0.02\n",
            "iteration: 224720 loss: 0.0036 lr: 0.02\n",
            "iteration: 224730 loss: 0.0035 lr: 0.02\n",
            "iteration: 224740 loss: 0.0037 lr: 0.02\n",
            "iteration: 224750 loss: 0.0042 lr: 0.02\n",
            "iteration: 224760 loss: 0.0031 lr: 0.02\n",
            "iteration: 224770 loss: 0.0035 lr: 0.02\n",
            "iteration: 224780 loss: 0.0051 lr: 0.02\n",
            "iteration: 224790 loss: 0.0035 lr: 0.02\n",
            "iteration: 224800 loss: 0.0036 lr: 0.02\n",
            "iteration: 224810 loss: 0.0039 lr: 0.02\n",
            "iteration: 224820 loss: 0.0041 lr: 0.02\n",
            "iteration: 224830 loss: 0.0031 lr: 0.02\n",
            "iteration: 224840 loss: 0.0032 lr: 0.02\n",
            "iteration: 224850 loss: 0.0029 lr: 0.02\n",
            "iteration: 224860 loss: 0.0038 lr: 0.02\n",
            "iteration: 224870 loss: 0.0032 lr: 0.02\n",
            "iteration: 224880 loss: 0.0033 lr: 0.02\n",
            "iteration: 224890 loss: 0.0036 lr: 0.02\n",
            "iteration: 224900 loss: 0.0040 lr: 0.02\n",
            "iteration: 224910 loss: 0.0033 lr: 0.02\n",
            "iteration: 224920 loss: 0.0032 lr: 0.02\n",
            "iteration: 224930 loss: 0.0039 lr: 0.02\n",
            "iteration: 224940 loss: 0.0031 lr: 0.02\n",
            "iteration: 224950 loss: 0.0035 lr: 0.02\n",
            "iteration: 224960 loss: 0.0038 lr: 0.02\n",
            "iteration: 224970 loss: 0.0056 lr: 0.02\n",
            "iteration: 224980 loss: 0.0050 lr: 0.02\n",
            "iteration: 224990 loss: 0.0036 lr: 0.02\n",
            "iteration: 225000 loss: 0.0033 lr: 0.02\n",
            "iteration: 225010 loss: 0.0033 lr: 0.02\n",
            "iteration: 225020 loss: 0.0036 lr: 0.02\n",
            "iteration: 225030 loss: 0.0038 lr: 0.02\n",
            "iteration: 225040 loss: 0.0037 lr: 0.02\n",
            "iteration: 225050 loss: 0.0043 lr: 0.02\n",
            "iteration: 225060 loss: 0.0037 lr: 0.02\n",
            "iteration: 225070 loss: 0.0039 lr: 0.02\n",
            "iteration: 225080 loss: 0.0026 lr: 0.02\n",
            "iteration: 225090 loss: 0.0036 lr: 0.02\n",
            "iteration: 225100 loss: 0.0021 lr: 0.02\n",
            "iteration: 225110 loss: 0.0036 lr: 0.02\n",
            "iteration: 225120 loss: 0.0039 lr: 0.02\n",
            "iteration: 225130 loss: 0.0036 lr: 0.02\n",
            "iteration: 225140 loss: 0.0033 lr: 0.02\n",
            "iteration: 225150 loss: 0.0044 lr: 0.02\n",
            "iteration: 225160 loss: 0.0040 lr: 0.02\n",
            "iteration: 225170 loss: 0.0033 lr: 0.02\n",
            "iteration: 225180 loss: 0.0041 lr: 0.02\n",
            "iteration: 225190 loss: 0.0029 lr: 0.02\n",
            "iteration: 225200 loss: 0.0031 lr: 0.02\n",
            "iteration: 225210 loss: 0.0034 lr: 0.02\n",
            "iteration: 225220 loss: 0.0031 lr: 0.02\n",
            "iteration: 225230 loss: 0.0035 lr: 0.02\n",
            "iteration: 225240 loss: 0.0049 lr: 0.02\n",
            "iteration: 225250 loss: 0.0043 lr: 0.02\n",
            "iteration: 225260 loss: 0.0037 lr: 0.02\n",
            "iteration: 225270 loss: 0.0031 lr: 0.02\n",
            "iteration: 225280 loss: 0.0043 lr: 0.02\n",
            "iteration: 225290 loss: 0.0029 lr: 0.02\n",
            "iteration: 225300 loss: 0.0041 lr: 0.02\n",
            "iteration: 225310 loss: 0.0039 lr: 0.02\n",
            "iteration: 225320 loss: 0.0038 lr: 0.02\n",
            "iteration: 225330 loss: 0.0029 lr: 0.02\n",
            "iteration: 225340 loss: 0.0031 lr: 0.02\n",
            "iteration: 225350 loss: 0.0039 lr: 0.02\n",
            "iteration: 225360 loss: 0.0034 lr: 0.02\n",
            "iteration: 225370 loss: 0.0037 lr: 0.02\n",
            "iteration: 225380 loss: 0.0033 lr: 0.02\n",
            "iteration: 225390 loss: 0.0039 lr: 0.02\n",
            "iteration: 225400 loss: 0.0041 lr: 0.02\n",
            "iteration: 225410 loss: 0.0033 lr: 0.02\n",
            "iteration: 225420 loss: 0.0036 lr: 0.02\n",
            "iteration: 225430 loss: 0.0033 lr: 0.02\n",
            "iteration: 225440 loss: 0.0034 lr: 0.02\n",
            "iteration: 225450 loss: 0.0038 lr: 0.02\n",
            "iteration: 225460 loss: 0.0031 lr: 0.02\n",
            "iteration: 225470 loss: 0.0027 lr: 0.02\n",
            "iteration: 225480 loss: 0.0035 lr: 0.02\n",
            "iteration: 225490 loss: 0.0033 lr: 0.02\n",
            "iteration: 225500 loss: 0.0040 lr: 0.02\n",
            "iteration: 225510 loss: 0.0028 lr: 0.02\n",
            "iteration: 225520 loss: 0.0038 lr: 0.02\n",
            "iteration: 225530 loss: 0.0042 lr: 0.02\n",
            "iteration: 225540 loss: 0.0039 lr: 0.02\n",
            "iteration: 225550 loss: 0.0037 lr: 0.02\n",
            "iteration: 225560 loss: 0.0037 lr: 0.02\n",
            "iteration: 225570 loss: 0.0037 lr: 0.02\n",
            "iteration: 225580 loss: 0.0038 lr: 0.02\n",
            "iteration: 225590 loss: 0.0030 lr: 0.02\n",
            "iteration: 225600 loss: 0.0034 lr: 0.02\n",
            "iteration: 225610 loss: 0.0037 lr: 0.02\n",
            "iteration: 225620 loss: 0.0027 lr: 0.02\n",
            "iteration: 225630 loss: 0.0028 lr: 0.02\n",
            "iteration: 225640 loss: 0.0031 lr: 0.02\n",
            "iteration: 225650 loss: 0.0030 lr: 0.02\n",
            "iteration: 225660 loss: 0.0034 lr: 0.02\n",
            "iteration: 225670 loss: 0.0036 lr: 0.02\n",
            "iteration: 225680 loss: 0.0037 lr: 0.02\n",
            "iteration: 225690 loss: 0.0029 lr: 0.02\n",
            "iteration: 225700 loss: 0.0029 lr: 0.02\n",
            "iteration: 225710 loss: 0.0037 lr: 0.02\n",
            "iteration: 225720 loss: 0.0031 lr: 0.02\n",
            "iteration: 225730 loss: 0.0029 lr: 0.02\n",
            "iteration: 225740 loss: 0.0040 lr: 0.02\n",
            "iteration: 225750 loss: 0.0038 lr: 0.02\n",
            "iteration: 225760 loss: 0.0032 lr: 0.02\n",
            "iteration: 225770 loss: 0.0037 lr: 0.02\n",
            "iteration: 225780 loss: 0.0036 lr: 0.02\n",
            "iteration: 225790 loss: 0.0034 lr: 0.02\n",
            "iteration: 225800 loss: 0.0033 lr: 0.02\n",
            "iteration: 225810 loss: 0.0034 lr: 0.02\n",
            "iteration: 225820 loss: 0.0040 lr: 0.02\n",
            "iteration: 225830 loss: 0.0042 lr: 0.02\n",
            "iteration: 225840 loss: 0.0033 lr: 0.02\n",
            "iteration: 225850 loss: 0.0033 lr: 0.02\n",
            "iteration: 225860 loss: 0.0034 lr: 0.02\n",
            "iteration: 225870 loss: 0.0041 lr: 0.02\n",
            "iteration: 225880 loss: 0.0032 lr: 0.02\n",
            "iteration: 225890 loss: 0.0032 lr: 0.02\n",
            "iteration: 225900 loss: 0.0031 lr: 0.02\n",
            "iteration: 225910 loss: 0.0031 lr: 0.02\n",
            "iteration: 225920 loss: 0.0037 lr: 0.02\n",
            "iteration: 225930 loss: 0.0032 lr: 0.02\n",
            "iteration: 225940 loss: 0.0043 lr: 0.02\n",
            "iteration: 225950 loss: 0.0037 lr: 0.02\n",
            "iteration: 225960 loss: 0.0036 lr: 0.02\n",
            "iteration: 225970 loss: 0.0032 lr: 0.02\n",
            "iteration: 225980 loss: 0.0032 lr: 0.02\n",
            "iteration: 225990 loss: 0.0027 lr: 0.02\n",
            "iteration: 226000 loss: 0.0038 lr: 0.02\n",
            "iteration: 226010 loss: 0.0039 lr: 0.02\n",
            "iteration: 226020 loss: 0.0028 lr: 0.02\n",
            "iteration: 226030 loss: 0.0031 lr: 0.02\n",
            "iteration: 226040 loss: 0.0024 lr: 0.02\n",
            "iteration: 226050 loss: 0.0032 lr: 0.02\n",
            "iteration: 226060 loss: 0.0036 lr: 0.02\n",
            "iteration: 226070 loss: 0.0033 lr: 0.02\n",
            "iteration: 226080 loss: 0.0031 lr: 0.02\n",
            "iteration: 226090 loss: 0.0033 lr: 0.02\n",
            "iteration: 226100 loss: 0.0038 lr: 0.02\n",
            "iteration: 226110 loss: 0.0029 lr: 0.02\n",
            "iteration: 226120 loss: 0.0032 lr: 0.02\n",
            "iteration: 226130 loss: 0.0027 lr: 0.02\n",
            "iteration: 226140 loss: 0.0032 lr: 0.02\n",
            "iteration: 226150 loss: 0.0032 lr: 0.02\n",
            "iteration: 226160 loss: 0.0031 lr: 0.02\n",
            "iteration: 226170 loss: 0.0047 lr: 0.02\n",
            "iteration: 226180 loss: 0.0034 lr: 0.02\n",
            "iteration: 226190 loss: 0.0037 lr: 0.02\n",
            "iteration: 226200 loss: 0.0025 lr: 0.02\n",
            "iteration: 226210 loss: 0.0038 lr: 0.02\n",
            "iteration: 226220 loss: 0.0038 lr: 0.02\n",
            "iteration: 226230 loss: 0.0036 lr: 0.02\n",
            "iteration: 226240 loss: 0.0042 lr: 0.02\n",
            "iteration: 226250 loss: 0.0039 lr: 0.02\n",
            "iteration: 226260 loss: 0.0044 lr: 0.02\n",
            "iteration: 226270 loss: 0.0031 lr: 0.02\n",
            "iteration: 226280 loss: 0.0028 lr: 0.02\n",
            "iteration: 226290 loss: 0.0033 lr: 0.02\n",
            "iteration: 226300 loss: 0.0028 lr: 0.02\n",
            "iteration: 226310 loss: 0.0026 lr: 0.02\n",
            "iteration: 226320 loss: 0.0024 lr: 0.02\n",
            "iteration: 226330 loss: 0.0035 lr: 0.02\n",
            "iteration: 226340 loss: 0.0032 lr: 0.02\n",
            "iteration: 226350 loss: 0.0039 lr: 0.02\n",
            "iteration: 226360 loss: 0.0039 lr: 0.02\n",
            "iteration: 226370 loss: 0.0033 lr: 0.02\n",
            "iteration: 226380 loss: 0.0034 lr: 0.02\n",
            "iteration: 226390 loss: 0.0025 lr: 0.02\n",
            "iteration: 226400 loss: 0.0035 lr: 0.02\n",
            "iteration: 226410 loss: 0.0027 lr: 0.02\n",
            "iteration: 226420 loss: 0.0032 lr: 0.02\n",
            "iteration: 226430 loss: 0.0035 lr: 0.02\n",
            "iteration: 226440 loss: 0.0036 lr: 0.02\n",
            "iteration: 226450 loss: 0.0034 lr: 0.02\n",
            "iteration: 226460 loss: 0.0029 lr: 0.02\n",
            "iteration: 226470 loss: 0.0029 lr: 0.02\n",
            "iteration: 226480 loss: 0.0036 lr: 0.02\n",
            "iteration: 226490 loss: 0.0026 lr: 0.02\n",
            "iteration: 226500 loss: 0.0034 lr: 0.02\n",
            "iteration: 226510 loss: 0.0042 lr: 0.02\n",
            "iteration: 226520 loss: 0.0030 lr: 0.02\n",
            "iteration: 226530 loss: 0.0042 lr: 0.02\n",
            "iteration: 226540 loss: 0.0031 lr: 0.02\n",
            "iteration: 226550 loss: 0.0033 lr: 0.02\n",
            "iteration: 226560 loss: 0.0025 lr: 0.02\n",
            "iteration: 226570 loss: 0.0035 lr: 0.02\n",
            "iteration: 226580 loss: 0.0038 lr: 0.02\n",
            "iteration: 226590 loss: 0.0037 lr: 0.02\n",
            "iteration: 226600 loss: 0.0035 lr: 0.02\n",
            "iteration: 226610 loss: 0.0036 lr: 0.02\n",
            "iteration: 226620 loss: 0.0032 lr: 0.02\n",
            "iteration: 226630 loss: 0.0037 lr: 0.02\n",
            "iteration: 226640 loss: 0.0031 lr: 0.02\n",
            "iteration: 226650 loss: 0.0033 lr: 0.02\n",
            "iteration: 226660 loss: 0.0039 lr: 0.02\n",
            "iteration: 226670 loss: 0.0034 lr: 0.02\n",
            "iteration: 226680 loss: 0.0040 lr: 0.02\n",
            "iteration: 226690 loss: 0.0028 lr: 0.02\n",
            "iteration: 226700 loss: 0.0037 lr: 0.02\n",
            "iteration: 226710 loss: 0.0035 lr: 0.02\n",
            "iteration: 226720 loss: 0.0049 lr: 0.02\n",
            "iteration: 226730 loss: 0.0033 lr: 0.02\n",
            "iteration: 226740 loss: 0.0035 lr: 0.02\n",
            "iteration: 226750 loss: 0.0033 lr: 0.02\n",
            "iteration: 226760 loss: 0.0036 lr: 0.02\n",
            "iteration: 226770 loss: 0.0038 lr: 0.02\n",
            "iteration: 226780 loss: 0.0035 lr: 0.02\n",
            "iteration: 226790 loss: 0.0033 lr: 0.02\n",
            "iteration: 226800 loss: 0.0032 lr: 0.02\n",
            "iteration: 226810 loss: 0.0042 lr: 0.02\n",
            "iteration: 226820 loss: 0.0027 lr: 0.02\n",
            "iteration: 226830 loss: 0.0035 lr: 0.02\n",
            "iteration: 226840 loss: 0.0034 lr: 0.02\n",
            "iteration: 226850 loss: 0.0027 lr: 0.02\n",
            "iteration: 226860 loss: 0.0030 lr: 0.02\n",
            "iteration: 226870 loss: 0.0030 lr: 0.02\n",
            "iteration: 226880 loss: 0.0028 lr: 0.02\n",
            "iteration: 226890 loss: 0.0028 lr: 0.02\n",
            "iteration: 226900 loss: 0.0040 lr: 0.02\n",
            "iteration: 226910 loss: 0.0036 lr: 0.02\n",
            "iteration: 226920 loss: 0.0029 lr: 0.02\n",
            "iteration: 226930 loss: 0.0031 lr: 0.02\n",
            "iteration: 226940 loss: 0.0033 lr: 0.02\n",
            "iteration: 226950 loss: 0.0031 lr: 0.02\n",
            "iteration: 226960 loss: 0.0027 lr: 0.02\n",
            "iteration: 226970 loss: 0.0036 lr: 0.02\n",
            "iteration: 226980 loss: 0.0028 lr: 0.02\n",
            "iteration: 226990 loss: 0.0036 lr: 0.02\n",
            "iteration: 227000 loss: 0.0028 lr: 0.02\n",
            "iteration: 227010 loss: 0.0030 lr: 0.02\n",
            "iteration: 227020 loss: 0.0038 lr: 0.02\n",
            "iteration: 227030 loss: 0.0030 lr: 0.02\n",
            "iteration: 227040 loss: 0.0027 lr: 0.02\n",
            "iteration: 227050 loss: 0.0033 lr: 0.02\n",
            "iteration: 227060 loss: 0.0033 lr: 0.02\n",
            "iteration: 227070 loss: 0.0038 lr: 0.02\n",
            "iteration: 227080 loss: 0.0026 lr: 0.02\n",
            "iteration: 227090 loss: 0.0038 lr: 0.02\n",
            "iteration: 227100 loss: 0.0040 lr: 0.02\n",
            "iteration: 227110 loss: 0.0032 lr: 0.02\n",
            "iteration: 227120 loss: 0.0033 lr: 0.02\n",
            "iteration: 227130 loss: 0.0028 lr: 0.02\n",
            "iteration: 227140 loss: 0.0039 lr: 0.02\n",
            "iteration: 227150 loss: 0.0042 lr: 0.02\n",
            "iteration: 227160 loss: 0.0028 lr: 0.02\n",
            "iteration: 227170 loss: 0.0029 lr: 0.02\n",
            "iteration: 227180 loss: 0.0037 lr: 0.02\n",
            "iteration: 227190 loss: 0.0034 lr: 0.02\n",
            "iteration: 227200 loss: 0.0027 lr: 0.02\n",
            "iteration: 227210 loss: 0.0033 lr: 0.02\n",
            "iteration: 227220 loss: 0.0027 lr: 0.02\n",
            "iteration: 227230 loss: 0.0036 lr: 0.02\n",
            "iteration: 227240 loss: 0.0032 lr: 0.02\n",
            "iteration: 227250 loss: 0.0035 lr: 0.02\n",
            "iteration: 227260 loss: 0.0040 lr: 0.02\n",
            "iteration: 227270 loss: 0.0031 lr: 0.02\n",
            "iteration: 227280 loss: 0.0033 lr: 0.02\n",
            "iteration: 227290 loss: 0.0032 lr: 0.02\n",
            "iteration: 227300 loss: 0.0036 lr: 0.02\n",
            "iteration: 227310 loss: 0.0040 lr: 0.02\n",
            "iteration: 227320 loss: 0.0037 lr: 0.02\n",
            "iteration: 227330 loss: 0.0034 lr: 0.02\n",
            "iteration: 227340 loss: 0.0035 lr: 0.02\n",
            "iteration: 227350 loss: 0.0035 lr: 0.02\n",
            "iteration: 227360 loss: 0.0033 lr: 0.02\n",
            "iteration: 227370 loss: 0.0031 lr: 0.02\n",
            "iteration: 227380 loss: 0.0036 lr: 0.02\n",
            "iteration: 227390 loss: 0.0033 lr: 0.02\n",
            "iteration: 227400 loss: 0.0034 lr: 0.02\n",
            "iteration: 227410 loss: 0.0034 lr: 0.02\n",
            "iteration: 227420 loss: 0.0032 lr: 0.02\n",
            "iteration: 227430 loss: 0.0043 lr: 0.02\n",
            "iteration: 227440 loss: 0.0032 lr: 0.02\n",
            "iteration: 227450 loss: 0.0033 lr: 0.02\n",
            "iteration: 227460 loss: 0.0026 lr: 0.02\n",
            "iteration: 227470 loss: 0.0035 lr: 0.02\n",
            "iteration: 227480 loss: 0.0032 lr: 0.02\n",
            "iteration: 227490 loss: 0.0031 lr: 0.02\n",
            "iteration: 227500 loss: 0.0027 lr: 0.02\n",
            "iteration: 227510 loss: 0.0036 lr: 0.02\n",
            "iteration: 227520 loss: 0.0029 lr: 0.02\n",
            "iteration: 227530 loss: 0.0031 lr: 0.02\n",
            "iteration: 227540 loss: 0.0031 lr: 0.02\n",
            "iteration: 227550 loss: 0.0031 lr: 0.02\n",
            "iteration: 227560 loss: 0.0030 lr: 0.02\n",
            "iteration: 227570 loss: 0.0034 lr: 0.02\n",
            "iteration: 227580 loss: 0.0037 lr: 0.02\n",
            "iteration: 227590 loss: 0.0041 lr: 0.02\n",
            "iteration: 227600 loss: 0.0040 lr: 0.02\n",
            "iteration: 227610 loss: 0.0032 lr: 0.02\n",
            "iteration: 227620 loss: 0.0031 lr: 0.02\n",
            "iteration: 227630 loss: 0.0031 lr: 0.02\n",
            "iteration: 227640 loss: 0.0025 lr: 0.02\n",
            "iteration: 227650 loss: 0.0035 lr: 0.02\n",
            "iteration: 227660 loss: 0.0037 lr: 0.02\n",
            "iteration: 227670 loss: 0.0036 lr: 0.02\n",
            "iteration: 227680 loss: 0.0036 lr: 0.02\n",
            "iteration: 227690 loss: 0.0039 lr: 0.02\n",
            "iteration: 227700 loss: 0.0034 lr: 0.02\n",
            "iteration: 227710 loss: 0.0036 lr: 0.02\n",
            "iteration: 227720 loss: 0.0033 lr: 0.02\n",
            "iteration: 227730 loss: 0.0034 lr: 0.02\n",
            "iteration: 227740 loss: 0.0031 lr: 0.02\n",
            "iteration: 227750 loss: 0.0049 lr: 0.02\n",
            "iteration: 227760 loss: 0.0039 lr: 0.02\n",
            "iteration: 227770 loss: 0.0036 lr: 0.02\n",
            "iteration: 227780 loss: 0.0034 lr: 0.02\n",
            "iteration: 227790 loss: 0.0035 lr: 0.02\n",
            "iteration: 227800 loss: 0.0033 lr: 0.02\n",
            "iteration: 227810 loss: 0.0038 lr: 0.02\n",
            "iteration: 227820 loss: 0.0033 lr: 0.02\n",
            "iteration: 227830 loss: 0.0031 lr: 0.02\n",
            "iteration: 227840 loss: 0.0036 lr: 0.02\n",
            "iteration: 227850 loss: 0.0031 lr: 0.02\n",
            "iteration: 227860 loss: 0.0032 lr: 0.02\n",
            "iteration: 227870 loss: 0.0036 lr: 0.02\n",
            "iteration: 227880 loss: 0.0033 lr: 0.02\n",
            "iteration: 227890 loss: 0.0037 lr: 0.02\n",
            "iteration: 227900 loss: 0.0041 lr: 0.02\n",
            "iteration: 227910 loss: 0.0033 lr: 0.02\n",
            "iteration: 227920 loss: 0.0037 lr: 0.02\n",
            "iteration: 227930 loss: 0.0027 lr: 0.02\n",
            "iteration: 227940 loss: 0.0040 lr: 0.02\n",
            "iteration: 227950 loss: 0.0037 lr: 0.02\n",
            "iteration: 227960 loss: 0.0026 lr: 0.02\n",
            "iteration: 227970 loss: 0.0029 lr: 0.02\n",
            "iteration: 227980 loss: 0.0030 lr: 0.02\n",
            "iteration: 227990 loss: 0.0035 lr: 0.02\n",
            "iteration: 228000 loss: 0.0040 lr: 0.02\n",
            "iteration: 228010 loss: 0.0037 lr: 0.02\n",
            "iteration: 228020 loss: 0.0042 lr: 0.02\n",
            "iteration: 228030 loss: 0.0031 lr: 0.02\n",
            "iteration: 228040 loss: 0.0033 lr: 0.02\n",
            "iteration: 228050 loss: 0.0034 lr: 0.02\n",
            "iteration: 228060 loss: 0.0030 lr: 0.02\n",
            "iteration: 228070 loss: 0.0031 lr: 0.02\n",
            "iteration: 228080 loss: 0.0035 lr: 0.02\n",
            "iteration: 228090 loss: 0.0036 lr: 0.02\n",
            "iteration: 228100 loss: 0.0046 lr: 0.02\n",
            "iteration: 228110 loss: 0.0032 lr: 0.02\n",
            "iteration: 228120 loss: 0.0029 lr: 0.02\n",
            "iteration: 228130 loss: 0.0033 lr: 0.02\n",
            "iteration: 228140 loss: 0.0043 lr: 0.02\n",
            "iteration: 228150 loss: 0.0038 lr: 0.02\n",
            "iteration: 228160 loss: 0.0028 lr: 0.02\n",
            "iteration: 228170 loss: 0.0030 lr: 0.02\n",
            "iteration: 228180 loss: 0.0028 lr: 0.02\n",
            "iteration: 228190 loss: 0.0027 lr: 0.02\n",
            "iteration: 228200 loss: 0.0032 lr: 0.02\n",
            "iteration: 228210 loss: 0.0040 lr: 0.02\n",
            "iteration: 228220 loss: 0.0030 lr: 0.02\n",
            "iteration: 228230 loss: 0.0033 lr: 0.02\n",
            "iteration: 228240 loss: 0.0044 lr: 0.02\n",
            "iteration: 228250 loss: 0.0030 lr: 0.02\n",
            "iteration: 228260 loss: 0.0040 lr: 0.02\n",
            "iteration: 228270 loss: 0.0029 lr: 0.02\n",
            "iteration: 228280 loss: 0.0035 lr: 0.02\n",
            "iteration: 228290 loss: 0.0035 lr: 0.02\n",
            "iteration: 228300 loss: 0.0039 lr: 0.02\n",
            "iteration: 228310 loss: 0.0030 lr: 0.02\n",
            "iteration: 228320 loss: 0.0034 lr: 0.02\n",
            "iteration: 228330 loss: 0.0030 lr: 0.02\n",
            "iteration: 228340 loss: 0.0029 lr: 0.02\n",
            "iteration: 228350 loss: 0.0027 lr: 0.02\n",
            "iteration: 228360 loss: 0.0049 lr: 0.02\n",
            "iteration: 228370 loss: 0.0040 lr: 0.02\n",
            "iteration: 228380 loss: 0.0033 lr: 0.02\n",
            "iteration: 228390 loss: 0.0038 lr: 0.02\n",
            "iteration: 228400 loss: 0.0029 lr: 0.02\n",
            "iteration: 228410 loss: 0.0024 lr: 0.02\n",
            "iteration: 228420 loss: 0.0038 lr: 0.02\n",
            "iteration: 228430 loss: 0.0033 lr: 0.02\n",
            "iteration: 228440 loss: 0.0036 lr: 0.02\n",
            "iteration: 228450 loss: 0.0035 lr: 0.02\n",
            "iteration: 228460 loss: 0.0030 lr: 0.02\n",
            "iteration: 228470 loss: 0.0035 lr: 0.02\n",
            "iteration: 228480 loss: 0.0037 lr: 0.02\n",
            "iteration: 228490 loss: 0.0033 lr: 0.02\n",
            "iteration: 228500 loss: 0.0041 lr: 0.02\n",
            "iteration: 228510 loss: 0.0039 lr: 0.02\n",
            "iteration: 228520 loss: 0.0034 lr: 0.02\n",
            "iteration: 228530 loss: 0.0027 lr: 0.02\n",
            "iteration: 228540 loss: 0.0031 lr: 0.02\n",
            "iteration: 228550 loss: 0.0044 lr: 0.02\n",
            "iteration: 228560 loss: 0.0036 lr: 0.02\n",
            "iteration: 228570 loss: 0.0027 lr: 0.02\n",
            "iteration: 228580 loss: 0.0031 lr: 0.02\n",
            "iteration: 228590 loss: 0.0044 lr: 0.02\n",
            "iteration: 228600 loss: 0.0034 lr: 0.02\n",
            "iteration: 228610 loss: 0.0037 lr: 0.02\n",
            "iteration: 228620 loss: 0.0040 lr: 0.02\n",
            "iteration: 228630 loss: 0.0030 lr: 0.02\n",
            "iteration: 228640 loss: 0.0034 lr: 0.02\n",
            "iteration: 228650 loss: 0.0036 lr: 0.02\n",
            "iteration: 228660 loss: 0.0033 lr: 0.02\n",
            "iteration: 228670 loss: 0.0050 lr: 0.02\n",
            "iteration: 228680 loss: 0.0038 lr: 0.02\n",
            "iteration: 228690 loss: 0.0036 lr: 0.02\n",
            "iteration: 228700 loss: 0.0035 lr: 0.02\n",
            "iteration: 228710 loss: 0.0034 lr: 0.02\n",
            "iteration: 228720 loss: 0.0032 lr: 0.02\n",
            "iteration: 228730 loss: 0.0029 lr: 0.02\n",
            "iteration: 228740 loss: 0.0029 lr: 0.02\n",
            "iteration: 228750 loss: 0.0038 lr: 0.02\n",
            "iteration: 228760 loss: 0.0033 lr: 0.02\n",
            "iteration: 228770 loss: 0.0042 lr: 0.02\n",
            "iteration: 228780 loss: 0.0026 lr: 0.02\n",
            "iteration: 228790 loss: 0.0027 lr: 0.02\n",
            "iteration: 228800 loss: 0.0041 lr: 0.02\n",
            "iteration: 228810 loss: 0.0033 lr: 0.02\n",
            "iteration: 228820 loss: 0.0030 lr: 0.02\n",
            "iteration: 228830 loss: 0.0025 lr: 0.02\n",
            "iteration: 228840 loss: 0.0045 lr: 0.02\n",
            "iteration: 228850 loss: 0.0042 lr: 0.02\n",
            "iteration: 228860 loss: 0.0030 lr: 0.02\n",
            "iteration: 228870 loss: 0.0047 lr: 0.02\n",
            "iteration: 228880 loss: 0.0039 lr: 0.02\n",
            "iteration: 228890 loss: 0.0044 lr: 0.02\n",
            "iteration: 228900 loss: 0.0033 lr: 0.02\n",
            "iteration: 228910 loss: 0.0033 lr: 0.02\n",
            "iteration: 228920 loss: 0.0041 lr: 0.02\n",
            "iteration: 228930 loss: 0.0038 lr: 0.02\n",
            "iteration: 228940 loss: 0.0040 lr: 0.02\n",
            "iteration: 228950 loss: 0.0034 lr: 0.02\n",
            "iteration: 228960 loss: 0.0036 lr: 0.02\n",
            "iteration: 228970 loss: 0.0027 lr: 0.02\n",
            "iteration: 228980 loss: 0.0027 lr: 0.02\n",
            "iteration: 228990 loss: 0.0033 lr: 0.02\n",
            "iteration: 229000 loss: 0.0029 lr: 0.02\n",
            "iteration: 229010 loss: 0.0034 lr: 0.02\n",
            "iteration: 229020 loss: 0.0028 lr: 0.02\n",
            "iteration: 229030 loss: 0.0032 lr: 0.02\n",
            "iteration: 229040 loss: 0.0035 lr: 0.02\n",
            "iteration: 229050 loss: 0.0031 lr: 0.02\n",
            "iteration: 229060 loss: 0.0027 lr: 0.02\n",
            "iteration: 229070 loss: 0.0041 lr: 0.02\n",
            "iteration: 229080 loss: 0.0044 lr: 0.02\n",
            "iteration: 229090 loss: 0.0029 lr: 0.02\n",
            "iteration: 229100 loss: 0.0025 lr: 0.02\n",
            "iteration: 229110 loss: 0.0031 lr: 0.02\n",
            "iteration: 229120 loss: 0.0031 lr: 0.02\n",
            "iteration: 229130 loss: 0.0033 lr: 0.02\n",
            "iteration: 229140 loss: 0.0033 lr: 0.02\n",
            "iteration: 229150 loss: 0.0032 lr: 0.02\n",
            "iteration: 229160 loss: 0.0030 lr: 0.02\n",
            "iteration: 229170 loss: 0.0032 lr: 0.02\n",
            "iteration: 229180 loss: 0.0031 lr: 0.02\n",
            "iteration: 229190 loss: 0.0033 lr: 0.02\n",
            "iteration: 229200 loss: 0.0039 lr: 0.02\n",
            "iteration: 229210 loss: 0.0031 lr: 0.02\n",
            "iteration: 229220 loss: 0.0020 lr: 0.02\n",
            "iteration: 229230 loss: 0.0036 lr: 0.02\n",
            "iteration: 229240 loss: 0.0031 lr: 0.02\n",
            "iteration: 229250 loss: 0.0030 lr: 0.02\n",
            "iteration: 229260 loss: 0.0029 lr: 0.02\n",
            "iteration: 229270 loss: 0.0031 lr: 0.02\n",
            "iteration: 229280 loss: 0.0033 lr: 0.02\n",
            "iteration: 229290 loss: 0.0032 lr: 0.02\n",
            "iteration: 229300 loss: 0.0025 lr: 0.02\n",
            "iteration: 229310 loss: 0.0032 lr: 0.02\n",
            "iteration: 229320 loss: 0.0033 lr: 0.02\n",
            "iteration: 229330 loss: 0.0031 lr: 0.02\n",
            "iteration: 229340 loss: 0.0031 lr: 0.02\n",
            "iteration: 229350 loss: 0.0041 lr: 0.02\n",
            "iteration: 229360 loss: 0.0042 lr: 0.02\n",
            "iteration: 229370 loss: 0.0031 lr: 0.02\n",
            "iteration: 229380 loss: 0.0031 lr: 0.02\n",
            "iteration: 229390 loss: 0.0035 lr: 0.02\n",
            "iteration: 229400 loss: 0.0036 lr: 0.02\n",
            "iteration: 229410 loss: 0.0023 lr: 0.02\n",
            "iteration: 229420 loss: 0.0027 lr: 0.02\n",
            "iteration: 229430 loss: 0.0037 lr: 0.02\n",
            "iteration: 229440 loss: 0.0042 lr: 0.02\n",
            "iteration: 229450 loss: 0.0038 lr: 0.02\n",
            "iteration: 229460 loss: 0.0039 lr: 0.02\n",
            "iteration: 229470 loss: 0.0040 lr: 0.02\n",
            "iteration: 229480 loss: 0.0031 lr: 0.02\n",
            "iteration: 229490 loss: 0.0032 lr: 0.02\n",
            "iteration: 229500 loss: 0.0046 lr: 0.02\n",
            "iteration: 229510 loss: 0.0038 lr: 0.02\n",
            "iteration: 229520 loss: 0.0040 lr: 0.02\n",
            "iteration: 229530 loss: 0.0030 lr: 0.02\n",
            "iteration: 229540 loss: 0.0040 lr: 0.02\n",
            "iteration: 229550 loss: 0.0034 lr: 0.02\n",
            "iteration: 229560 loss: 0.0033 lr: 0.02\n",
            "iteration: 229570 loss: 0.0029 lr: 0.02\n",
            "iteration: 229580 loss: 0.0036 lr: 0.02\n",
            "iteration: 229590 loss: 0.0028 lr: 0.02\n",
            "iteration: 229600 loss: 0.0037 lr: 0.02\n",
            "iteration: 229610 loss: 0.0025 lr: 0.02\n",
            "iteration: 229620 loss: 0.0038 lr: 0.02\n",
            "iteration: 229630 loss: 0.0033 lr: 0.02\n",
            "iteration: 229640 loss: 0.0030 lr: 0.02\n",
            "iteration: 229650 loss: 0.0039 lr: 0.02\n",
            "iteration: 229660 loss: 0.0031 lr: 0.02\n",
            "iteration: 229670 loss: 0.0029 lr: 0.02\n",
            "iteration: 229680 loss: 0.0046 lr: 0.02\n",
            "iteration: 229690 loss: 0.0029 lr: 0.02\n",
            "iteration: 229700 loss: 0.0034 lr: 0.02\n",
            "iteration: 229710 loss: 0.0029 lr: 0.02\n",
            "iteration: 229720 loss: 0.0026 lr: 0.02\n",
            "iteration: 229730 loss: 0.0032 lr: 0.02\n",
            "iteration: 229740 loss: 0.0037 lr: 0.02\n",
            "iteration: 229750 loss: 0.0040 lr: 0.02\n",
            "iteration: 229760 loss: 0.0033 lr: 0.02\n",
            "iteration: 229770 loss: 0.0039 lr: 0.02\n",
            "iteration: 229780 loss: 0.0032 lr: 0.02\n",
            "iteration: 229790 loss: 0.0030 lr: 0.02\n",
            "iteration: 229800 loss: 0.0032 lr: 0.02\n",
            "iteration: 229810 loss: 0.0030 lr: 0.02\n",
            "iteration: 229820 loss: 0.0035 lr: 0.02\n",
            "iteration: 229830 loss: 0.0034 lr: 0.02\n",
            "iteration: 229840 loss: 0.0035 lr: 0.02\n",
            "iteration: 229850 loss: 0.0033 lr: 0.02\n",
            "iteration: 229860 loss: 0.0026 lr: 0.02\n",
            "iteration: 229870 loss: 0.0028 lr: 0.02\n",
            "iteration: 229880 loss: 0.0027 lr: 0.02\n",
            "iteration: 229890 loss: 0.0037 lr: 0.02\n",
            "iteration: 229900 loss: 0.0023 lr: 0.02\n",
            "iteration: 229910 loss: 0.0033 lr: 0.02\n",
            "iteration: 229920 loss: 0.0034 lr: 0.02\n",
            "iteration: 229930 loss: 0.0030 lr: 0.02\n",
            "iteration: 229940 loss: 0.0029 lr: 0.02\n",
            "iteration: 229950 loss: 0.0034 lr: 0.02\n",
            "iteration: 229960 loss: 0.0031 lr: 0.02\n",
            "iteration: 229970 loss: 0.0028 lr: 0.02\n",
            "iteration: 229980 loss: 0.0026 lr: 0.02\n",
            "iteration: 229990 loss: 0.0029 lr: 0.02\n",
            "iteration: 230000 loss: 0.0041 lr: 0.02\n",
            "iteration: 230010 loss: 0.0035 lr: 0.02\n",
            "iteration: 230020 loss: 0.0026 lr: 0.02\n",
            "iteration: 230030 loss: 0.0033 lr: 0.02\n",
            "iteration: 230040 loss: 0.0031 lr: 0.02\n",
            "iteration: 230050 loss: 0.0030 lr: 0.02\n",
            "iteration: 230060 loss: 0.0040 lr: 0.02\n",
            "iteration: 230070 loss: 0.0034 lr: 0.02\n",
            "iteration: 230080 loss: 0.0050 lr: 0.02\n",
            "iteration: 230090 loss: 0.0035 lr: 0.02\n",
            "iteration: 230100 loss: 0.0042 lr: 0.02\n",
            "iteration: 230110 loss: 0.0030 lr: 0.02\n",
            "iteration: 230120 loss: 0.0038 lr: 0.02\n",
            "iteration: 230130 loss: 0.0033 lr: 0.02\n",
            "iteration: 230140 loss: 0.0033 lr: 0.02\n",
            "iteration: 230150 loss: 0.0041 lr: 0.02\n",
            "iteration: 230160 loss: 0.0027 lr: 0.02\n",
            "iteration: 230170 loss: 0.0033 lr: 0.02\n",
            "iteration: 230180 loss: 0.0040 lr: 0.02\n",
            "iteration: 230190 loss: 0.0037 lr: 0.02\n",
            "iteration: 230200 loss: 0.0030 lr: 0.02\n",
            "iteration: 230210 loss: 0.0038 lr: 0.02\n",
            "iteration: 230220 loss: 0.0033 lr: 0.02\n",
            "iteration: 230230 loss: 0.0036 lr: 0.02\n",
            "iteration: 230240 loss: 0.0032 lr: 0.02\n",
            "iteration: 230250 loss: 0.0032 lr: 0.02\n",
            "iteration: 230260 loss: 0.0042 lr: 0.02\n",
            "iteration: 230270 loss: 0.0038 lr: 0.02\n",
            "iteration: 230280 loss: 0.0037 lr: 0.02\n",
            "iteration: 230290 loss: 0.0038 lr: 0.02\n",
            "iteration: 230300 loss: 0.0030 lr: 0.02\n",
            "iteration: 230310 loss: 0.0035 lr: 0.02\n",
            "iteration: 230320 loss: 0.0035 lr: 0.02\n",
            "iteration: 230330 loss: 0.0032 lr: 0.02\n",
            "iteration: 230340 loss: 0.0032 lr: 0.02\n",
            "iteration: 230350 loss: 0.0035 lr: 0.02\n",
            "iteration: 230360 loss: 0.0043 lr: 0.02\n",
            "iteration: 230370 loss: 0.0038 lr: 0.02\n",
            "iteration: 230380 loss: 0.0037 lr: 0.02\n",
            "iteration: 230390 loss: 0.0037 lr: 0.02\n",
            "iteration: 230400 loss: 0.0051 lr: 0.02\n",
            "iteration: 230410 loss: 0.0033 lr: 0.02\n",
            "iteration: 230420 loss: 0.0029 lr: 0.02\n",
            "iteration: 230430 loss: 0.0030 lr: 0.02\n",
            "iteration: 230440 loss: 0.0033 lr: 0.02\n",
            "iteration: 230450 loss: 0.0034 lr: 0.02\n",
            "iteration: 230460 loss: 0.0034 lr: 0.02\n",
            "iteration: 230470 loss: 0.0035 lr: 0.02\n",
            "iteration: 230480 loss: 0.0035 lr: 0.02\n",
            "iteration: 230490 loss: 0.0028 lr: 0.02\n",
            "iteration: 230500 loss: 0.0036 lr: 0.02\n",
            "iteration: 230510 loss: 0.0032 lr: 0.02\n",
            "iteration: 230520 loss: 0.0032 lr: 0.02\n",
            "iteration: 230530 loss: 0.0031 lr: 0.02\n",
            "iteration: 230540 loss: 0.0034 lr: 0.02\n",
            "iteration: 230550 loss: 0.0035 lr: 0.02\n",
            "iteration: 230560 loss: 0.0036 lr: 0.02\n",
            "iteration: 230570 loss: 0.0038 lr: 0.02\n",
            "iteration: 230580 loss: 0.0034 lr: 0.02\n",
            "iteration: 230590 loss: 0.0031 lr: 0.02\n",
            "iteration: 230600 loss: 0.0034 lr: 0.02\n",
            "iteration: 230610 loss: 0.0027 lr: 0.02\n",
            "iteration: 230620 loss: 0.0036 lr: 0.02\n",
            "iteration: 230630 loss: 0.0041 lr: 0.02\n",
            "iteration: 230640 loss: 0.0027 lr: 0.02\n",
            "iteration: 230650 loss: 0.0036 lr: 0.02\n",
            "iteration: 230660 loss: 0.0028 lr: 0.02\n",
            "iteration: 230670 loss: 0.0036 lr: 0.02\n",
            "iteration: 230680 loss: 0.0024 lr: 0.02\n",
            "iteration: 230690 loss: 0.0034 lr: 0.02\n",
            "iteration: 230700 loss: 0.0030 lr: 0.02\n",
            "iteration: 230710 loss: 0.0030 lr: 0.02\n",
            "iteration: 230720 loss: 0.0030 lr: 0.02\n",
            "iteration: 230730 loss: 0.0036 lr: 0.02\n",
            "iteration: 230740 loss: 0.0031 lr: 0.02\n",
            "iteration: 230750 loss: 0.0030 lr: 0.02\n",
            "iteration: 230760 loss: 0.0046 lr: 0.02\n",
            "iteration: 230770 loss: 0.0028 lr: 0.02\n",
            "iteration: 230780 loss: 0.0027 lr: 0.02\n",
            "iteration: 230790 loss: 0.0026 lr: 0.02\n",
            "iteration: 230800 loss: 0.0034 lr: 0.02\n",
            "iteration: 230810 loss: 0.0032 lr: 0.02\n",
            "iteration: 230820 loss: 0.0030 lr: 0.02\n",
            "iteration: 230830 loss: 0.0032 lr: 0.02\n",
            "iteration: 230840 loss: 0.0027 lr: 0.02\n",
            "iteration: 230850 loss: 0.0031 lr: 0.02\n",
            "iteration: 230860 loss: 0.0029 lr: 0.02\n",
            "iteration: 230870 loss: 0.0031 lr: 0.02\n",
            "iteration: 230880 loss: 0.0039 lr: 0.02\n",
            "iteration: 230890 loss: 0.0029 lr: 0.02\n",
            "iteration: 230900 loss: 0.0023 lr: 0.02\n",
            "iteration: 230910 loss: 0.0032 lr: 0.02\n",
            "iteration: 230920 loss: 0.0029 lr: 0.02\n",
            "iteration: 230930 loss: 0.0041 lr: 0.02\n",
            "iteration: 230940 loss: 0.0037 lr: 0.02\n",
            "iteration: 230950 loss: 0.0022 lr: 0.02\n",
            "iteration: 230960 loss: 0.0034 lr: 0.02\n",
            "iteration: 230970 loss: 0.0026 lr: 0.02\n",
            "iteration: 230980 loss: 0.0033 lr: 0.02\n",
            "iteration: 230990 loss: 0.0043 lr: 0.02\n",
            "iteration: 231000 loss: 0.0036 lr: 0.02\n",
            "iteration: 231010 loss: 0.0030 lr: 0.02\n",
            "iteration: 231020 loss: 0.0030 lr: 0.02\n",
            "iteration: 231030 loss: 0.0035 lr: 0.02\n",
            "iteration: 231040 loss: 0.0031 lr: 0.02\n",
            "iteration: 231050 loss: 0.0037 lr: 0.02\n",
            "iteration: 231060 loss: 0.0053 lr: 0.02\n",
            "iteration: 231070 loss: 0.0034 lr: 0.02\n",
            "iteration: 231080 loss: 0.0039 lr: 0.02\n",
            "iteration: 231090 loss: 0.0032 lr: 0.02\n",
            "iteration: 231100 loss: 0.0042 lr: 0.02\n",
            "iteration: 231110 loss: 0.0031 lr: 0.02\n",
            "iteration: 231120 loss: 0.0031 lr: 0.02\n",
            "iteration: 231130 loss: 0.0040 lr: 0.02\n",
            "iteration: 231140 loss: 0.0036 lr: 0.02\n",
            "iteration: 231150 loss: 0.0028 lr: 0.02\n",
            "iteration: 231160 loss: 0.0028 lr: 0.02\n",
            "iteration: 231170 loss: 0.0036 lr: 0.02\n",
            "iteration: 231180 loss: 0.0033 lr: 0.02\n",
            "iteration: 231190 loss: 0.0034 lr: 0.02\n",
            "iteration: 231200 loss: 0.0028 lr: 0.02\n",
            "iteration: 231210 loss: 0.0035 lr: 0.02\n",
            "iteration: 231220 loss: 0.0036 lr: 0.02\n",
            "iteration: 231230 loss: 0.0030 lr: 0.02\n",
            "iteration: 231240 loss: 0.0030 lr: 0.02\n",
            "iteration: 231250 loss: 0.0027 lr: 0.02\n",
            "iteration: 231260 loss: 0.0029 lr: 0.02\n",
            "iteration: 231270 loss: 0.0037 lr: 0.02\n",
            "iteration: 231280 loss: 0.0033 lr: 0.02\n",
            "iteration: 231290 loss: 0.0037 lr: 0.02\n",
            "iteration: 231300 loss: 0.0033 lr: 0.02\n",
            "iteration: 231310 loss: 0.0044 lr: 0.02\n",
            "iteration: 231320 loss: 0.0029 lr: 0.02\n",
            "iteration: 231330 loss: 0.0034 lr: 0.02\n",
            "iteration: 231340 loss: 0.0032 lr: 0.02\n",
            "iteration: 231350 loss: 0.0030 lr: 0.02\n",
            "iteration: 231360 loss: 0.0032 lr: 0.02\n",
            "iteration: 231370 loss: 0.0031 lr: 0.02\n",
            "iteration: 231380 loss: 0.0032 lr: 0.02\n",
            "iteration: 231390 loss: 0.0032 lr: 0.02\n",
            "iteration: 231400 loss: 0.0026 lr: 0.02\n",
            "iteration: 231410 loss: 0.0031 lr: 0.02\n",
            "iteration: 231420 loss: 0.0029 lr: 0.02\n",
            "iteration: 231430 loss: 0.0036 lr: 0.02\n",
            "iteration: 231440 loss: 0.0028 lr: 0.02\n",
            "iteration: 231450 loss: 0.0041 lr: 0.02\n",
            "iteration: 231460 loss: 0.0031 lr: 0.02\n",
            "iteration: 231470 loss: 0.0031 lr: 0.02\n",
            "iteration: 231480 loss: 0.0038 lr: 0.02\n",
            "iteration: 231490 loss: 0.0035 lr: 0.02\n",
            "iteration: 231500 loss: 0.0034 lr: 0.02\n",
            "iteration: 231510 loss: 0.0031 lr: 0.02\n",
            "iteration: 231520 loss: 0.0028 lr: 0.02\n",
            "iteration: 231530 loss: 0.0031 lr: 0.02\n",
            "iteration: 231540 loss: 0.0040 lr: 0.02\n",
            "iteration: 231550 loss: 0.0035 lr: 0.02\n",
            "iteration: 231560 loss: 0.0029 lr: 0.02\n",
            "iteration: 231570 loss: 0.0040 lr: 0.02\n",
            "iteration: 231580 loss: 0.0038 lr: 0.02\n",
            "iteration: 231590 loss: 0.0036 lr: 0.02\n",
            "iteration: 231600 loss: 0.0034 lr: 0.02\n",
            "iteration: 231610 loss: 0.0041 lr: 0.02\n",
            "iteration: 231620 loss: 0.0036 lr: 0.02\n",
            "iteration: 231630 loss: 0.0045 lr: 0.02\n",
            "iteration: 231640 loss: 0.0043 lr: 0.02\n",
            "iteration: 231650 loss: 0.0035 lr: 0.02\n",
            "iteration: 231660 loss: 0.0043 lr: 0.02\n",
            "iteration: 231670 loss: 0.0037 lr: 0.02\n",
            "iteration: 231680 loss: 0.0036 lr: 0.02\n",
            "iteration: 231690 loss: 0.0035 lr: 0.02\n",
            "iteration: 231700 loss: 0.0029 lr: 0.02\n",
            "iteration: 231710 loss: 0.0036 lr: 0.02\n",
            "iteration: 231720 loss: 0.0027 lr: 0.02\n",
            "iteration: 231730 loss: 0.0036 lr: 0.02\n",
            "iteration: 231740 loss: 0.0043 lr: 0.02\n",
            "iteration: 231750 loss: 0.0030 lr: 0.02\n",
            "iteration: 231760 loss: 0.0031 lr: 0.02\n",
            "iteration: 231770 loss: 0.0033 lr: 0.02\n",
            "iteration: 231780 loss: 0.0028 lr: 0.02\n",
            "iteration: 231790 loss: 0.0030 lr: 0.02\n",
            "iteration: 231800 loss: 0.0028 lr: 0.02\n",
            "iteration: 231810 loss: 0.0039 lr: 0.02\n",
            "iteration: 231820 loss: 0.0033 lr: 0.02\n",
            "iteration: 231830 loss: 0.0029 lr: 0.02\n",
            "iteration: 231840 loss: 0.0053 lr: 0.02\n",
            "iteration: 231850 loss: 0.0044 lr: 0.02\n",
            "iteration: 231860 loss: 0.0031 lr: 0.02\n",
            "iteration: 231870 loss: 0.0045 lr: 0.02\n",
            "iteration: 231880 loss: 0.0032 lr: 0.02\n",
            "iteration: 231890 loss: 0.0032 lr: 0.02\n",
            "iteration: 231900 loss: 0.0038 lr: 0.02\n",
            "iteration: 231910 loss: 0.0032 lr: 0.02\n",
            "iteration: 231920 loss: 0.0040 lr: 0.02\n",
            "iteration: 231930 loss: 0.0031 lr: 0.02\n",
            "iteration: 231940 loss: 0.0028 lr: 0.02\n",
            "iteration: 231950 loss: 0.0027 lr: 0.02\n",
            "iteration: 231960 loss: 0.0041 lr: 0.02\n",
            "iteration: 231970 loss: 0.0033 lr: 0.02\n",
            "iteration: 231980 loss: 0.0039 lr: 0.02\n",
            "iteration: 231990 loss: 0.0028 lr: 0.02\n",
            "iteration: 232000 loss: 0.0030 lr: 0.02\n",
            "iteration: 232010 loss: 0.0033 lr: 0.02\n",
            "iteration: 232020 loss: 0.0031 lr: 0.02\n",
            "iteration: 232030 loss: 0.0032 lr: 0.02\n",
            "iteration: 232040 loss: 0.0037 lr: 0.02\n",
            "iteration: 232050 loss: 0.0027 lr: 0.02\n",
            "iteration: 232060 loss: 0.0028 lr: 0.02\n",
            "iteration: 232070 loss: 0.0043 lr: 0.02\n",
            "iteration: 232080 loss: 0.0035 lr: 0.02\n",
            "iteration: 232090 loss: 0.0027 lr: 0.02\n",
            "iteration: 232100 loss: 0.0039 lr: 0.02\n",
            "iteration: 232110 loss: 0.0032 lr: 0.02\n",
            "iteration: 232120 loss: 0.0027 lr: 0.02\n",
            "iteration: 232130 loss: 0.0026 lr: 0.02\n",
            "iteration: 232140 loss: 0.0030 lr: 0.02\n",
            "iteration: 232150 loss: 0.0030 lr: 0.02\n",
            "iteration: 232160 loss: 0.0042 lr: 0.02\n",
            "iteration: 232170 loss: 0.0042 lr: 0.02\n",
            "iteration: 232180 loss: 0.0037 lr: 0.02\n",
            "iteration: 232190 loss: 0.0044 lr: 0.02\n",
            "iteration: 232200 loss: 0.0037 lr: 0.02\n",
            "iteration: 232210 loss: 0.0032 lr: 0.02\n",
            "iteration: 232220 loss: 0.0047 lr: 0.02\n",
            "iteration: 232230 loss: 0.0030 lr: 0.02\n",
            "iteration: 232240 loss: 0.0036 lr: 0.02\n",
            "iteration: 232250 loss: 0.0041 lr: 0.02\n",
            "iteration: 232260 loss: 0.0036 lr: 0.02\n",
            "iteration: 232270 loss: 0.0042 lr: 0.02\n",
            "iteration: 232280 loss: 0.0029 lr: 0.02\n",
            "iteration: 232290 loss: 0.0031 lr: 0.02\n",
            "iteration: 232300 loss: 0.0033 lr: 0.02\n",
            "iteration: 232310 loss: 0.0032 lr: 0.02\n",
            "iteration: 232320 loss: 0.0040 lr: 0.02\n",
            "iteration: 232330 loss: 0.0034 lr: 0.02\n",
            "iteration: 232340 loss: 0.0039 lr: 0.02\n",
            "iteration: 232350 loss: 0.0027 lr: 0.02\n",
            "iteration: 232360 loss: 0.0041 lr: 0.02\n",
            "iteration: 232370 loss: 0.0036 lr: 0.02\n",
            "iteration: 232380 loss: 0.0036 lr: 0.02\n",
            "iteration: 232390 loss: 0.0038 lr: 0.02\n",
            "iteration: 232400 loss: 0.0028 lr: 0.02\n",
            "iteration: 232410 loss: 0.0028 lr: 0.02\n",
            "iteration: 232420 loss: 0.0034 lr: 0.02\n",
            "iteration: 232430 loss: 0.0032 lr: 0.02\n",
            "iteration: 232440 loss: 0.0032 lr: 0.02\n",
            "iteration: 232450 loss: 0.0038 lr: 0.02\n",
            "iteration: 232460 loss: 0.0035 lr: 0.02\n",
            "iteration: 232470 loss: 0.0033 lr: 0.02\n",
            "iteration: 232480 loss: 0.0031 lr: 0.02\n",
            "iteration: 232490 loss: 0.0025 lr: 0.02\n",
            "iteration: 232500 loss: 0.0026 lr: 0.02\n",
            "iteration: 232510 loss: 0.0022 lr: 0.02\n",
            "iteration: 232520 loss: 0.0035 lr: 0.02\n",
            "iteration: 232530 loss: 0.0041 lr: 0.02\n",
            "iteration: 232540 loss: 0.0037 lr: 0.02\n",
            "iteration: 232550 loss: 0.0033 lr: 0.02\n",
            "iteration: 232560 loss: 0.0037 lr: 0.02\n",
            "iteration: 232570 loss: 0.0030 lr: 0.02\n",
            "iteration: 232580 loss: 0.0036 lr: 0.02\n",
            "iteration: 232590 loss: 0.0035 lr: 0.02\n",
            "iteration: 232600 loss: 0.0042 lr: 0.02\n",
            "iteration: 232610 loss: 0.0031 lr: 0.02\n",
            "iteration: 232620 loss: 0.0039 lr: 0.02\n",
            "iteration: 232630 loss: 0.0036 lr: 0.02\n",
            "iteration: 232640 loss: 0.0035 lr: 0.02\n",
            "iteration: 232650 loss: 0.0034 lr: 0.02\n",
            "iteration: 232660 loss: 0.0033 lr: 0.02\n",
            "iteration: 232670 loss: 0.0033 lr: 0.02\n",
            "iteration: 232680 loss: 0.0028 lr: 0.02\n",
            "iteration: 232690 loss: 0.0033 lr: 0.02\n",
            "iteration: 232700 loss: 0.0032 lr: 0.02\n",
            "iteration: 232710 loss: 0.0037 lr: 0.02\n",
            "iteration: 232720 loss: 0.0037 lr: 0.02\n",
            "iteration: 232730 loss: 0.0032 lr: 0.02\n",
            "iteration: 232740 loss: 0.0039 lr: 0.02\n",
            "iteration: 232750 loss: 0.0033 lr: 0.02\n",
            "iteration: 232760 loss: 0.0028 lr: 0.02\n",
            "iteration: 232770 loss: 0.0028 lr: 0.02\n",
            "iteration: 232780 loss: 0.0035 lr: 0.02\n",
            "iteration: 232790 loss: 0.0026 lr: 0.02\n",
            "iteration: 232800 loss: 0.0034 lr: 0.02\n",
            "iteration: 232810 loss: 0.0021 lr: 0.02\n",
            "iteration: 232820 loss: 0.0030 lr: 0.02\n",
            "iteration: 232830 loss: 0.0047 lr: 0.02\n",
            "iteration: 232840 loss: 0.0030 lr: 0.02\n",
            "iteration: 232850 loss: 0.0035 lr: 0.02\n",
            "iteration: 232860 loss: 0.0027 lr: 0.02\n",
            "iteration: 232870 loss: 0.0038 lr: 0.02\n",
            "iteration: 232880 loss: 0.0036 lr: 0.02\n",
            "iteration: 232890 loss: 0.0033 lr: 0.02\n",
            "iteration: 232900 loss: 0.0037 lr: 0.02\n",
            "iteration: 232910 loss: 0.0032 lr: 0.02\n",
            "iteration: 232920 loss: 0.0039 lr: 0.02\n",
            "iteration: 232930 loss: 0.0045 lr: 0.02\n",
            "iteration: 232940 loss: 0.0036 lr: 0.02\n",
            "iteration: 232950 loss: 0.0046 lr: 0.02\n",
            "iteration: 232960 loss: 0.0031 lr: 0.02\n",
            "iteration: 232970 loss: 0.0037 lr: 0.02\n",
            "iteration: 232980 loss: 0.0029 lr: 0.02\n",
            "iteration: 232990 loss: 0.0033 lr: 0.02\n",
            "iteration: 233000 loss: 0.0035 lr: 0.02\n",
            "iteration: 233010 loss: 0.0042 lr: 0.02\n",
            "iteration: 233020 loss: 0.0033 lr: 0.02\n",
            "iteration: 233030 loss: 0.0034 lr: 0.02\n",
            "iteration: 233040 loss: 0.0039 lr: 0.02\n",
            "iteration: 233050 loss: 0.0033 lr: 0.02\n",
            "iteration: 233060 loss: 0.0027 lr: 0.02\n",
            "iteration: 233070 loss: 0.0037 lr: 0.02\n",
            "iteration: 233080 loss: 0.0031 lr: 0.02\n",
            "iteration: 233090 loss: 0.0034 lr: 0.02\n",
            "iteration: 233100 loss: 0.0029 lr: 0.02\n",
            "iteration: 233110 loss: 0.0030 lr: 0.02\n",
            "iteration: 233120 loss: 0.0031 lr: 0.02\n",
            "iteration: 233130 loss: 0.0029 lr: 0.02\n",
            "iteration: 233140 loss: 0.0038 lr: 0.02\n",
            "iteration: 233150 loss: 0.0035 lr: 0.02\n",
            "iteration: 233160 loss: 0.0040 lr: 0.02\n",
            "iteration: 233170 loss: 0.0036 lr: 0.02\n",
            "iteration: 233180 loss: 0.0040 lr: 0.02\n",
            "iteration: 233190 loss: 0.0051 lr: 0.02\n",
            "iteration: 233200 loss: 0.0039 lr: 0.02\n",
            "iteration: 233210 loss: 0.0030 lr: 0.02\n",
            "iteration: 233220 loss: 0.0037 lr: 0.02\n",
            "iteration: 233230 loss: 0.0038 lr: 0.02\n",
            "iteration: 233240 loss: 0.0031 lr: 0.02\n",
            "iteration: 233250 loss: 0.0036 lr: 0.02\n",
            "iteration: 233260 loss: 0.0025 lr: 0.02\n",
            "iteration: 233270 loss: 0.0038 lr: 0.02\n",
            "iteration: 233280 loss: 0.0025 lr: 0.02\n",
            "iteration: 233290 loss: 0.0031 lr: 0.02\n",
            "iteration: 233300 loss: 0.0038 lr: 0.02\n",
            "iteration: 233310 loss: 0.0040 lr: 0.02\n",
            "iteration: 233320 loss: 0.0028 lr: 0.02\n",
            "iteration: 233330 loss: 0.0041 lr: 0.02\n",
            "iteration: 233340 loss: 0.0040 lr: 0.02\n",
            "iteration: 233350 loss: 0.0032 lr: 0.02\n",
            "iteration: 233360 loss: 0.0031 lr: 0.02\n",
            "iteration: 233370 loss: 0.0025 lr: 0.02\n",
            "iteration: 233380 loss: 0.0031 lr: 0.02\n",
            "iteration: 233390 loss: 0.0030 lr: 0.02\n",
            "iteration: 233400 loss: 0.0034 lr: 0.02\n",
            "iteration: 233410 loss: 0.0030 lr: 0.02\n",
            "iteration: 233420 loss: 0.0027 lr: 0.02\n",
            "iteration: 233430 loss: 0.0028 lr: 0.02\n",
            "iteration: 233440 loss: 0.0040 lr: 0.02\n",
            "iteration: 233450 loss: 0.0034 lr: 0.02\n",
            "iteration: 233460 loss: 0.0025 lr: 0.02\n",
            "iteration: 233470 loss: 0.0031 lr: 0.02\n",
            "iteration: 233480 loss: 0.0037 lr: 0.02\n",
            "iteration: 233490 loss: 0.0027 lr: 0.02\n",
            "iteration: 233500 loss: 0.0035 lr: 0.02\n",
            "iteration: 233510 loss: 0.0032 lr: 0.02\n",
            "iteration: 233520 loss: 0.0036 lr: 0.02\n",
            "iteration: 233530 loss: 0.0026 lr: 0.02\n",
            "iteration: 233540 loss: 0.0029 lr: 0.02\n",
            "iteration: 233550 loss: 0.0034 lr: 0.02\n",
            "iteration: 233560 loss: 0.0027 lr: 0.02\n",
            "iteration: 233570 loss: 0.0035 lr: 0.02\n",
            "iteration: 233580 loss: 0.0031 lr: 0.02\n",
            "iteration: 233590 loss: 0.0039 lr: 0.02\n",
            "iteration: 233600 loss: 0.0032 lr: 0.02\n",
            "iteration: 233610 loss: 0.0027 lr: 0.02\n",
            "iteration: 233620 loss: 0.0033 lr: 0.02\n",
            "iteration: 233630 loss: 0.0044 lr: 0.02\n",
            "iteration: 233640 loss: 0.0033 lr: 0.02\n",
            "iteration: 233650 loss: 0.0036 lr: 0.02\n",
            "iteration: 233660 loss: 0.0041 lr: 0.02\n",
            "iteration: 233670 loss: 0.0030 lr: 0.02\n",
            "iteration: 233680 loss: 0.0040 lr: 0.02\n",
            "iteration: 233690 loss: 0.0039 lr: 0.02\n",
            "iteration: 233700 loss: 0.0037 lr: 0.02\n",
            "iteration: 233710 loss: 0.0040 lr: 0.02\n",
            "iteration: 233720 loss: 0.0032 lr: 0.02\n",
            "iteration: 233730 loss: 0.0028 lr: 0.02\n",
            "iteration: 233740 loss: 0.0035 lr: 0.02\n",
            "iteration: 233750 loss: 0.0023 lr: 0.02\n",
            "iteration: 233760 loss: 0.0034 lr: 0.02\n",
            "iteration: 233770 loss: 0.0037 lr: 0.02\n",
            "iteration: 233780 loss: 0.0040 lr: 0.02\n",
            "iteration: 233790 loss: 0.0025 lr: 0.02\n",
            "iteration: 233800 loss: 0.0032 lr: 0.02\n",
            "iteration: 233810 loss: 0.0028 lr: 0.02\n",
            "iteration: 233820 loss: 0.0032 lr: 0.02\n",
            "iteration: 233830 loss: 0.0046 lr: 0.02\n",
            "iteration: 233840 loss: 0.0030 lr: 0.02\n",
            "iteration: 233850 loss: 0.0040 lr: 0.02\n",
            "iteration: 233860 loss: 0.0034 lr: 0.02\n",
            "iteration: 233870 loss: 0.0038 lr: 0.02\n",
            "iteration: 233880 loss: 0.0033 lr: 0.02\n",
            "iteration: 233890 loss: 0.0043 lr: 0.02\n",
            "iteration: 233900 loss: 0.0036 lr: 0.02\n",
            "iteration: 233910 loss: 0.0025 lr: 0.02\n",
            "iteration: 233920 loss: 0.0032 lr: 0.02\n",
            "iteration: 233930 loss: 0.0032 lr: 0.02\n",
            "iteration: 233940 loss: 0.0028 lr: 0.02\n",
            "iteration: 233950 loss: 0.0028 lr: 0.02\n",
            "iteration: 233960 loss: 0.0035 lr: 0.02\n",
            "iteration: 233970 loss: 0.0030 lr: 0.02\n",
            "iteration: 233980 loss: 0.0049 lr: 0.02\n",
            "iteration: 233990 loss: 0.0029 lr: 0.02\n",
            "iteration: 234000 loss: 0.0035 lr: 0.02\n",
            "iteration: 234010 loss: 0.0037 lr: 0.02\n",
            "iteration: 234020 loss: 0.0032 lr: 0.02\n",
            "iteration: 234030 loss: 0.0031 lr: 0.02\n",
            "iteration: 234040 loss: 0.0031 lr: 0.02\n",
            "iteration: 234050 loss: 0.0033 lr: 0.02\n",
            "iteration: 234060 loss: 0.0035 lr: 0.02\n",
            "iteration: 234070 loss: 0.0027 lr: 0.02\n",
            "iteration: 234080 loss: 0.0031 lr: 0.02\n",
            "iteration: 234090 loss: 0.0034 lr: 0.02\n",
            "iteration: 234100 loss: 0.0026 lr: 0.02\n",
            "iteration: 234110 loss: 0.0033 lr: 0.02\n",
            "iteration: 234120 loss: 0.0042 lr: 0.02\n",
            "iteration: 234130 loss: 0.0040 lr: 0.02\n",
            "iteration: 234140 loss: 0.0029 lr: 0.02\n",
            "iteration: 234150 loss: 0.0032 lr: 0.02\n",
            "iteration: 234160 loss: 0.0035 lr: 0.02\n",
            "iteration: 234170 loss: 0.0036 lr: 0.02\n",
            "iteration: 234180 loss: 0.0034 lr: 0.02\n",
            "iteration: 234190 loss: 0.0029 lr: 0.02\n",
            "iteration: 234200 loss: 0.0027 lr: 0.02\n",
            "iteration: 234210 loss: 0.0032 lr: 0.02\n",
            "iteration: 234220 loss: 0.0033 lr: 0.02\n",
            "iteration: 234230 loss: 0.0031 lr: 0.02\n",
            "iteration: 234240 loss: 0.0032 lr: 0.02\n",
            "iteration: 234250 loss: 0.0037 lr: 0.02\n",
            "iteration: 234260 loss: 0.0028 lr: 0.02\n",
            "iteration: 234270 loss: 0.0027 lr: 0.02\n",
            "iteration: 234280 loss: 0.0031 lr: 0.02\n",
            "iteration: 234290 loss: 0.0042 lr: 0.02\n",
            "iteration: 234300 loss: 0.0026 lr: 0.02\n",
            "iteration: 234310 loss: 0.0024 lr: 0.02\n",
            "iteration: 234320 loss: 0.0029 lr: 0.02\n",
            "iteration: 234330 loss: 0.0025 lr: 0.02\n",
            "iteration: 234340 loss: 0.0034 lr: 0.02\n",
            "iteration: 234350 loss: 0.0037 lr: 0.02\n",
            "iteration: 234360 loss: 0.0029 lr: 0.02\n",
            "iteration: 234370 loss: 0.0032 lr: 0.02\n",
            "iteration: 234380 loss: 0.0043 lr: 0.02\n",
            "iteration: 234390 loss: 0.0032 lr: 0.02\n",
            "iteration: 234400 loss: 0.0040 lr: 0.02\n",
            "iteration: 234410 loss: 0.0033 lr: 0.02\n",
            "iteration: 234420 loss: 0.0027 lr: 0.02\n",
            "iteration: 234430 loss: 0.0033 lr: 0.02\n",
            "iteration: 234440 loss: 0.0029 lr: 0.02\n",
            "iteration: 234450 loss: 0.0036 lr: 0.02\n",
            "iteration: 234460 loss: 0.0028 lr: 0.02\n",
            "iteration: 234470 loss: 0.0042 lr: 0.02\n",
            "iteration: 234480 loss: 0.0030 lr: 0.02\n",
            "iteration: 234490 loss: 0.0029 lr: 0.02\n",
            "iteration: 234500 loss: 0.0024 lr: 0.02\n",
            "iteration: 234510 loss: 0.0033 lr: 0.02\n",
            "iteration: 234520 loss: 0.0029 lr: 0.02\n",
            "iteration: 234530 loss: 0.0030 lr: 0.02\n",
            "iteration: 234540 loss: 0.0039 lr: 0.02\n",
            "iteration: 234550 loss: 0.0031 lr: 0.02\n",
            "iteration: 234560 loss: 0.0041 lr: 0.02\n",
            "iteration: 234570 loss: 0.0032 lr: 0.02\n",
            "iteration: 234580 loss: 0.0032 lr: 0.02\n",
            "iteration: 234590 loss: 0.0024 lr: 0.02\n",
            "iteration: 234600 loss: 0.0035 lr: 0.02\n",
            "iteration: 234610 loss: 0.0034 lr: 0.02\n",
            "iteration: 234620 loss: 0.0028 lr: 0.02\n",
            "iteration: 234630 loss: 0.0034 lr: 0.02\n",
            "iteration: 234640 loss: 0.0028 lr: 0.02\n",
            "iteration: 234650 loss: 0.0028 lr: 0.02\n",
            "iteration: 234660 loss: 0.0029 lr: 0.02\n",
            "iteration: 234670 loss: 0.0035 lr: 0.02\n",
            "iteration: 234680 loss: 0.0031 lr: 0.02\n",
            "iteration: 234690 loss: 0.0030 lr: 0.02\n",
            "iteration: 234700 loss: 0.0036 lr: 0.02\n",
            "iteration: 234710 loss: 0.0028 lr: 0.02\n",
            "iteration: 234720 loss: 0.0030 lr: 0.02\n",
            "iteration: 234730 loss: 0.0034 lr: 0.02\n",
            "iteration: 234740 loss: 0.0038 lr: 0.02\n",
            "iteration: 234750 loss: 0.0038 lr: 0.02\n",
            "iteration: 234760 loss: 0.0034 lr: 0.02\n",
            "iteration: 234770 loss: 0.0031 lr: 0.02\n",
            "iteration: 234780 loss: 0.0036 lr: 0.02\n",
            "iteration: 234790 loss: 0.0032 lr: 0.02\n",
            "iteration: 234800 loss: 0.0048 lr: 0.02\n",
            "iteration: 234810 loss: 0.0031 lr: 0.02\n",
            "iteration: 234820 loss: 0.0032 lr: 0.02\n",
            "iteration: 234830 loss: 0.0032 lr: 0.02\n",
            "iteration: 234840 loss: 0.0031 lr: 0.02\n",
            "iteration: 234850 loss: 0.0033 lr: 0.02\n",
            "iteration: 234860 loss: 0.0033 lr: 0.02\n",
            "iteration: 234870 loss: 0.0025 lr: 0.02\n",
            "iteration: 234880 loss: 0.0032 lr: 0.02\n",
            "iteration: 234890 loss: 0.0025 lr: 0.02\n",
            "iteration: 234900 loss: 0.0030 lr: 0.02\n",
            "iteration: 234910 loss: 0.0033 lr: 0.02\n",
            "iteration: 234920 loss: 0.0031 lr: 0.02\n",
            "iteration: 234930 loss: 0.0031 lr: 0.02\n",
            "iteration: 234940 loss: 0.0038 lr: 0.02\n",
            "iteration: 234950 loss: 0.0031 lr: 0.02\n",
            "iteration: 234960 loss: 0.0032 lr: 0.02\n",
            "iteration: 234970 loss: 0.0025 lr: 0.02\n",
            "iteration: 234980 loss: 0.0034 lr: 0.02\n",
            "iteration: 234990 loss: 0.0028 lr: 0.02\n",
            "iteration: 235000 loss: 0.0028 lr: 0.02\n",
            "iteration: 235010 loss: 0.0035 lr: 0.02\n",
            "iteration: 235020 loss: 0.0028 lr: 0.02\n",
            "iteration: 235030 loss: 0.0031 lr: 0.02\n",
            "iteration: 235040 loss: 0.0035 lr: 0.02\n",
            "iteration: 235050 loss: 0.0036 lr: 0.02\n",
            "iteration: 235060 loss: 0.0033 lr: 0.02\n",
            "iteration: 235070 loss: 0.0031 lr: 0.02\n",
            "iteration: 235080 loss: 0.0024 lr: 0.02\n",
            "iteration: 235090 loss: 0.0030 lr: 0.02\n",
            "iteration: 235100 loss: 0.0034 lr: 0.02\n",
            "iteration: 235110 loss: 0.0037 lr: 0.02\n",
            "iteration: 235120 loss: 0.0031 lr: 0.02\n",
            "iteration: 235130 loss: 0.0040 lr: 0.02\n",
            "iteration: 235140 loss: 0.0028 lr: 0.02\n",
            "iteration: 235150 loss: 0.0031 lr: 0.02\n",
            "iteration: 235160 loss: 0.0026 lr: 0.02\n",
            "iteration: 235170 loss: 0.0043 lr: 0.02\n",
            "iteration: 235180 loss: 0.0041 lr: 0.02\n",
            "iteration: 235190 loss: 0.0031 lr: 0.02\n",
            "iteration: 235200 loss: 0.0046 lr: 0.02\n",
            "iteration: 235210 loss: 0.0038 lr: 0.02\n",
            "iteration: 235220 loss: 0.0030 lr: 0.02\n",
            "iteration: 235230 loss: 0.0029 lr: 0.02\n",
            "iteration: 235240 loss: 0.0036 lr: 0.02\n",
            "iteration: 235250 loss: 0.0042 lr: 0.02\n",
            "iteration: 235260 loss: 0.0035 lr: 0.02\n",
            "iteration: 235270 loss: 0.0034 lr: 0.02\n",
            "iteration: 235280 loss: 0.0036 lr: 0.02\n",
            "iteration: 235290 loss: 0.0036 lr: 0.02\n",
            "iteration: 235300 loss: 0.0028 lr: 0.02\n",
            "iteration: 235310 loss: 0.0036 lr: 0.02\n",
            "iteration: 235320 loss: 0.0033 lr: 0.02\n",
            "iteration: 235330 loss: 0.0023 lr: 0.02\n",
            "iteration: 235340 loss: 0.0031 lr: 0.02\n",
            "iteration: 235350 loss: 0.0039 lr: 0.02\n",
            "iteration: 235360 loss: 0.0028 lr: 0.02\n",
            "iteration: 235370 loss: 0.0030 lr: 0.02\n",
            "iteration: 235380 loss: 0.0032 lr: 0.02\n",
            "iteration: 235390 loss: 0.0033 lr: 0.02\n",
            "iteration: 235400 loss: 0.0038 lr: 0.02\n",
            "iteration: 235410 loss: 0.0032 lr: 0.02\n",
            "iteration: 235420 loss: 0.0037 lr: 0.02\n",
            "iteration: 235430 loss: 0.0032 lr: 0.02\n",
            "iteration: 235440 loss: 0.0038 lr: 0.02\n",
            "iteration: 235450 loss: 0.0034 lr: 0.02\n",
            "iteration: 235460 loss: 0.0033 lr: 0.02\n",
            "iteration: 235470 loss: 0.0034 lr: 0.02\n",
            "iteration: 235480 loss: 0.0031 lr: 0.02\n",
            "iteration: 235490 loss: 0.0023 lr: 0.02\n",
            "iteration: 235500 loss: 0.0041 lr: 0.02\n",
            "iteration: 235510 loss: 0.0029 lr: 0.02\n",
            "iteration: 235520 loss: 0.0035 lr: 0.02\n",
            "iteration: 235530 loss: 0.0033 lr: 0.02\n",
            "iteration: 235540 loss: 0.0032 lr: 0.02\n",
            "iteration: 235550 loss: 0.0028 lr: 0.02\n",
            "iteration: 235560 loss: 0.0036 lr: 0.02\n",
            "iteration: 235570 loss: 0.0034 lr: 0.02\n",
            "iteration: 235580 loss: 0.0035 lr: 0.02\n",
            "iteration: 235590 loss: 0.0034 lr: 0.02\n",
            "iteration: 235600 loss: 0.0036 lr: 0.02\n",
            "iteration: 235610 loss: 0.0031 lr: 0.02\n",
            "iteration: 235620 loss: 0.0038 lr: 0.02\n",
            "iteration: 235630 loss: 0.0028 lr: 0.02\n",
            "iteration: 235640 loss: 0.0029 lr: 0.02\n",
            "iteration: 235650 loss: 0.0033 lr: 0.02\n",
            "iteration: 235660 loss: 0.0029 lr: 0.02\n",
            "iteration: 235670 loss: 0.0035 lr: 0.02\n",
            "iteration: 235680 loss: 0.0028 lr: 0.02\n",
            "iteration: 235690 loss: 0.0028 lr: 0.02\n",
            "iteration: 235700 loss: 0.0039 lr: 0.02\n",
            "iteration: 235710 loss: 0.0034 lr: 0.02\n",
            "iteration: 235720 loss: 0.0041 lr: 0.02\n",
            "iteration: 235730 loss: 0.0038 lr: 0.02\n",
            "iteration: 235740 loss: 0.0032 lr: 0.02\n",
            "iteration: 235750 loss: 0.0031 lr: 0.02\n",
            "iteration: 235760 loss: 0.0031 lr: 0.02\n",
            "iteration: 235770 loss: 0.0024 lr: 0.02\n",
            "iteration: 235780 loss: 0.0030 lr: 0.02\n",
            "iteration: 235790 loss: 0.0027 lr: 0.02\n",
            "iteration: 235800 loss: 0.0028 lr: 0.02\n",
            "iteration: 235810 loss: 0.0024 lr: 0.02\n",
            "iteration: 235820 loss: 0.0030 lr: 0.02\n",
            "iteration: 235830 loss: 0.0029 lr: 0.02\n",
            "iteration: 235840 loss: 0.0031 lr: 0.02\n",
            "iteration: 235850 loss: 0.0036 lr: 0.02\n",
            "iteration: 235860 loss: 0.0045 lr: 0.02\n",
            "iteration: 235870 loss: 0.0037 lr: 0.02\n",
            "iteration: 235880 loss: 0.0031 lr: 0.02\n",
            "iteration: 235890 loss: 0.0029 lr: 0.02\n",
            "iteration: 235900 loss: 0.0031 lr: 0.02\n",
            "iteration: 235910 loss: 0.0032 lr: 0.02\n",
            "iteration: 235920 loss: 0.0037 lr: 0.02\n",
            "iteration: 235930 loss: 0.0031 lr: 0.02\n",
            "iteration: 235940 loss: 0.0030 lr: 0.02\n",
            "iteration: 235950 loss: 0.0031 lr: 0.02\n",
            "iteration: 235960 loss: 0.0027 lr: 0.02\n",
            "iteration: 235970 loss: 0.0032 lr: 0.02\n",
            "iteration: 235980 loss: 0.0032 lr: 0.02\n",
            "iteration: 235990 loss: 0.0031 lr: 0.02\n",
            "iteration: 236000 loss: 0.0037 lr: 0.02\n",
            "iteration: 236010 loss: 0.0033 lr: 0.02\n",
            "iteration: 236020 loss: 0.0037 lr: 0.02\n",
            "iteration: 236030 loss: 0.0028 lr: 0.02\n",
            "iteration: 236040 loss: 0.0034 lr: 0.02\n",
            "iteration: 236050 loss: 0.0038 lr: 0.02\n",
            "iteration: 236060 loss: 0.0035 lr: 0.02\n",
            "iteration: 236070 loss: 0.0039 lr: 0.02\n",
            "iteration: 236080 loss: 0.0029 lr: 0.02\n",
            "iteration: 236090 loss: 0.0042 lr: 0.02\n",
            "iteration: 236100 loss: 0.0032 lr: 0.02\n",
            "iteration: 236110 loss: 0.0033 lr: 0.02\n",
            "iteration: 236120 loss: 0.0027 lr: 0.02\n",
            "iteration: 236130 loss: 0.0033 lr: 0.02\n",
            "iteration: 236140 loss: 0.0035 lr: 0.02\n",
            "iteration: 236150 loss: 0.0032 lr: 0.02\n",
            "iteration: 236160 loss: 0.0044 lr: 0.02\n",
            "iteration: 236170 loss: 0.0042 lr: 0.02\n",
            "iteration: 236180 loss: 0.0037 lr: 0.02\n",
            "iteration: 236190 loss: 0.0037 lr: 0.02\n",
            "iteration: 236200 loss: 0.0034 lr: 0.02\n",
            "iteration: 236210 loss: 0.0033 lr: 0.02\n",
            "iteration: 236220 loss: 0.0035 lr: 0.02\n",
            "iteration: 236230 loss: 0.0032 lr: 0.02\n",
            "iteration: 236240 loss: 0.0030 lr: 0.02\n",
            "iteration: 236250 loss: 0.0033 lr: 0.02\n",
            "iteration: 236260 loss: 0.0032 lr: 0.02\n",
            "iteration: 236270 loss: 0.0043 lr: 0.02\n",
            "iteration: 236280 loss: 0.0041 lr: 0.02\n",
            "iteration: 236290 loss: 0.0043 lr: 0.02\n",
            "iteration: 236300 loss: 0.0035 lr: 0.02\n",
            "iteration: 236310 loss: 0.0031 lr: 0.02\n",
            "iteration: 236320 loss: 0.0033 lr: 0.02\n",
            "iteration: 236330 loss: 0.0046 lr: 0.02\n",
            "iteration: 236340 loss: 0.0041 lr: 0.02\n",
            "iteration: 236350 loss: 0.0039 lr: 0.02\n",
            "iteration: 236360 loss: 0.0031 lr: 0.02\n",
            "iteration: 236370 loss: 0.0037 lr: 0.02\n",
            "iteration: 236380 loss: 0.0031 lr: 0.02\n",
            "iteration: 236390 loss: 0.0033 lr: 0.02\n",
            "iteration: 236400 loss: 0.0028 lr: 0.02\n",
            "iteration: 236410 loss: 0.0032 lr: 0.02\n",
            "iteration: 236420 loss: 0.0036 lr: 0.02\n",
            "iteration: 236430 loss: 0.0032 lr: 0.02\n",
            "iteration: 236440 loss: 0.0033 lr: 0.02\n",
            "iteration: 236450 loss: 0.0026 lr: 0.02\n",
            "iteration: 236460 loss: 0.0035 lr: 0.02\n",
            "iteration: 236470 loss: 0.0035 lr: 0.02\n",
            "iteration: 236480 loss: 0.0034 lr: 0.02\n",
            "iteration: 236490 loss: 0.0030 lr: 0.02\n",
            "iteration: 236500 loss: 0.0035 lr: 0.02\n",
            "iteration: 236510 loss: 0.0033 lr: 0.02\n",
            "iteration: 236520 loss: 0.0034 lr: 0.02\n",
            "iteration: 236530 loss: 0.0033 lr: 0.02\n",
            "iteration: 236540 loss: 0.0032 lr: 0.02\n",
            "iteration: 236550 loss: 0.0030 lr: 0.02\n",
            "iteration: 236560 loss: 0.0036 lr: 0.02\n",
            "iteration: 236570 loss: 0.0028 lr: 0.02\n",
            "iteration: 236580 loss: 0.0035 lr: 0.02\n",
            "iteration: 236590 loss: 0.0037 lr: 0.02\n",
            "iteration: 236600 loss: 0.0040 lr: 0.02\n",
            "iteration: 236610 loss: 0.0038 lr: 0.02\n",
            "iteration: 236620 loss: 0.0036 lr: 0.02\n",
            "iteration: 236630 loss: 0.0034 lr: 0.02\n",
            "iteration: 236640 loss: 0.0034 lr: 0.02\n",
            "iteration: 236650 loss: 0.0040 lr: 0.02\n",
            "iteration: 236660 loss: 0.0030 lr: 0.02\n",
            "iteration: 236670 loss: 0.0037 lr: 0.02\n",
            "iteration: 236680 loss: 0.0040 lr: 0.02\n",
            "iteration: 236690 loss: 0.0042 lr: 0.02\n",
            "iteration: 236700 loss: 0.0034 lr: 0.02\n",
            "iteration: 236710 loss: 0.0031 lr: 0.02\n",
            "iteration: 236720 loss: 0.0032 lr: 0.02\n",
            "iteration: 236730 loss: 0.0047 lr: 0.02\n",
            "iteration: 236740 loss: 0.0033 lr: 0.02\n",
            "iteration: 236750 loss: 0.0030 lr: 0.02\n",
            "iteration: 236760 loss: 0.0027 lr: 0.02\n",
            "iteration: 236770 loss: 0.0042 lr: 0.02\n",
            "iteration: 236780 loss: 0.0029 lr: 0.02\n",
            "iteration: 236790 loss: 0.0028 lr: 0.02\n",
            "iteration: 236800 loss: 0.0027 lr: 0.02\n",
            "iteration: 236810 loss: 0.0042 lr: 0.02\n",
            "iteration: 236820 loss: 0.0028 lr: 0.02\n",
            "iteration: 236830 loss: 0.0028 lr: 0.02\n",
            "iteration: 236840 loss: 0.0029 lr: 0.02\n",
            "iteration: 236850 loss: 0.0032 lr: 0.02\n",
            "iteration: 236860 loss: 0.0035 lr: 0.02\n",
            "iteration: 236870 loss: 0.0030 lr: 0.02\n",
            "iteration: 236880 loss: 0.0028 lr: 0.02\n",
            "iteration: 236890 loss: 0.0031 lr: 0.02\n",
            "iteration: 236900 loss: 0.0041 lr: 0.02\n",
            "iteration: 236910 loss: 0.0043 lr: 0.02\n",
            "iteration: 236920 loss: 0.0039 lr: 0.02\n",
            "iteration: 236930 loss: 0.0032 lr: 0.02\n",
            "iteration: 236940 loss: 0.0038 lr: 0.02\n",
            "iteration: 236950 loss: 0.0032 lr: 0.02\n",
            "iteration: 236960 loss: 0.0028 lr: 0.02\n",
            "iteration: 236970 loss: 0.0034 lr: 0.02\n",
            "iteration: 236980 loss: 0.0032 lr: 0.02\n",
            "iteration: 236990 loss: 0.0038 lr: 0.02\n",
            "iteration: 237000 loss: 0.0039 lr: 0.02\n",
            "iteration: 237010 loss: 0.0036 lr: 0.02\n",
            "iteration: 237020 loss: 0.0025 lr: 0.02\n",
            "iteration: 237030 loss: 0.0035 lr: 0.02\n",
            "iteration: 237040 loss: 0.0037 lr: 0.02\n",
            "iteration: 237050 loss: 0.0033 lr: 0.02\n",
            "iteration: 237060 loss: 0.0041 lr: 0.02\n",
            "iteration: 237070 loss: 0.0027 lr: 0.02\n",
            "iteration: 237080 loss: 0.0036 lr: 0.02\n",
            "iteration: 237090 loss: 0.0025 lr: 0.02\n",
            "iteration: 237100 loss: 0.0032 lr: 0.02\n",
            "iteration: 237110 loss: 0.0033 lr: 0.02\n",
            "iteration: 237120 loss: 0.0039 lr: 0.02\n",
            "iteration: 237130 loss: 0.0036 lr: 0.02\n",
            "iteration: 237140 loss: 0.0027 lr: 0.02\n",
            "iteration: 237150 loss: 0.0032 lr: 0.02\n",
            "iteration: 237160 loss: 0.0029 lr: 0.02\n",
            "iteration: 237170 loss: 0.0033 lr: 0.02\n",
            "iteration: 237180 loss: 0.0038 lr: 0.02\n",
            "iteration: 237190 loss: 0.0030 lr: 0.02\n",
            "iteration: 237200 loss: 0.0032 lr: 0.02\n",
            "iteration: 237210 loss: 0.0029 lr: 0.02\n",
            "iteration: 237220 loss: 0.0027 lr: 0.02\n",
            "iteration: 237230 loss: 0.0031 lr: 0.02\n",
            "iteration: 237240 loss: 0.0033 lr: 0.02\n",
            "iteration: 237250 loss: 0.0035 lr: 0.02\n",
            "iteration: 237260 loss: 0.0037 lr: 0.02\n",
            "iteration: 237270 loss: 0.0026 lr: 0.02\n",
            "iteration: 237280 loss: 0.0042 lr: 0.02\n",
            "iteration: 237290 loss: 0.0028 lr: 0.02\n",
            "iteration: 237300 loss: 0.0037 lr: 0.02\n",
            "iteration: 237310 loss: 0.0035 lr: 0.02\n",
            "iteration: 237320 loss: 0.0031 lr: 0.02\n",
            "iteration: 237330 loss: 0.0030 lr: 0.02\n",
            "iteration: 237340 loss: 0.0027 lr: 0.02\n",
            "iteration: 237350 loss: 0.0033 lr: 0.02\n",
            "iteration: 237360 loss: 0.0039 lr: 0.02\n",
            "iteration: 237370 loss: 0.0032 lr: 0.02\n",
            "iteration: 237380 loss: 0.0031 lr: 0.02\n",
            "iteration: 237390 loss: 0.0029 lr: 0.02\n",
            "iteration: 237400 loss: 0.0033 lr: 0.02\n",
            "iteration: 237410 loss: 0.0039 lr: 0.02\n",
            "iteration: 237420 loss: 0.0035 lr: 0.02\n",
            "iteration: 237430 loss: 0.0033 lr: 0.02\n",
            "iteration: 237440 loss: 0.0036 lr: 0.02\n",
            "iteration: 237450 loss: 0.0023 lr: 0.02\n",
            "iteration: 237460 loss: 0.0040 lr: 0.02\n",
            "iteration: 237470 loss: 0.0026 lr: 0.02\n",
            "iteration: 237480 loss: 0.0033 lr: 0.02\n",
            "iteration: 237490 loss: 0.0038 lr: 0.02\n",
            "iteration: 237500 loss: 0.0034 lr: 0.02\n",
            "iteration: 237510 loss: 0.0032 lr: 0.02\n",
            "iteration: 237520 loss: 0.0034 lr: 0.02\n",
            "iteration: 237530 loss: 0.0035 lr: 0.02\n",
            "iteration: 237540 loss: 0.0033 lr: 0.02\n",
            "iteration: 237550 loss: 0.0032 lr: 0.02\n",
            "iteration: 237560 loss: 0.0042 lr: 0.02\n",
            "iteration: 237570 loss: 0.0032 lr: 0.02\n",
            "iteration: 237580 loss: 0.0025 lr: 0.02\n",
            "iteration: 237590 loss: 0.0030 lr: 0.02\n",
            "iteration: 237600 loss: 0.0036 lr: 0.02\n",
            "iteration: 237610 loss: 0.0035 lr: 0.02\n",
            "iteration: 237620 loss: 0.0032 lr: 0.02\n",
            "iteration: 237630 loss: 0.0041 lr: 0.02\n",
            "iteration: 237640 loss: 0.0039 lr: 0.02\n",
            "iteration: 237650 loss: 0.0036 lr: 0.02\n",
            "iteration: 237660 loss: 0.0028 lr: 0.02\n",
            "iteration: 237670 loss: 0.0035 lr: 0.02\n",
            "iteration: 237680 loss: 0.0028 lr: 0.02\n",
            "iteration: 237690 loss: 0.0031 lr: 0.02\n",
            "iteration: 237700 loss: 0.0034 lr: 0.02\n",
            "iteration: 237710 loss: 0.0027 lr: 0.02\n",
            "iteration: 237720 loss: 0.0032 lr: 0.02\n",
            "iteration: 237730 loss: 0.0030 lr: 0.02\n",
            "iteration: 237740 loss: 0.0028 lr: 0.02\n",
            "iteration: 237750 loss: 0.0031 lr: 0.02\n",
            "iteration: 237760 loss: 0.0039 lr: 0.02\n",
            "iteration: 237770 loss: 0.0030 lr: 0.02\n",
            "iteration: 237780 loss: 0.0037 lr: 0.02\n",
            "iteration: 237790 loss: 0.0028 lr: 0.02\n",
            "iteration: 237800 loss: 0.0031 lr: 0.02\n",
            "iteration: 237810 loss: 0.0031 lr: 0.02\n",
            "iteration: 237820 loss: 0.0039 lr: 0.02\n",
            "iteration: 237830 loss: 0.0036 lr: 0.02\n",
            "iteration: 237840 loss: 0.0031 lr: 0.02\n",
            "iteration: 237850 loss: 0.0039 lr: 0.02\n",
            "iteration: 237860 loss: 0.0032 lr: 0.02\n",
            "iteration: 237870 loss: 0.0044 lr: 0.02\n",
            "iteration: 237880 loss: 0.0026 lr: 0.02\n",
            "iteration: 237890 loss: 0.0037 lr: 0.02\n",
            "iteration: 237900 loss: 0.0038 lr: 0.02\n",
            "iteration: 237910 loss: 0.0034 lr: 0.02\n",
            "iteration: 237920 loss: 0.0028 lr: 0.02\n",
            "iteration: 237930 loss: 0.0025 lr: 0.02\n",
            "iteration: 237940 loss: 0.0027 lr: 0.02\n",
            "iteration: 237950 loss: 0.0033 lr: 0.02\n",
            "iteration: 237960 loss: 0.0033 lr: 0.02\n",
            "iteration: 237970 loss: 0.0034 lr: 0.02\n",
            "iteration: 237980 loss: 0.0038 lr: 0.02\n",
            "iteration: 237990 loss: 0.0032 lr: 0.02\n",
            "iteration: 238000 loss: 0.0032 lr: 0.02\n",
            "iteration: 238010 loss: 0.0028 lr: 0.02\n",
            "iteration: 238020 loss: 0.0039 lr: 0.02\n",
            "iteration: 238030 loss: 0.0032 lr: 0.02\n",
            "iteration: 238040 loss: 0.0034 lr: 0.02\n",
            "iteration: 238050 loss: 0.0034 lr: 0.02\n",
            "iteration: 238060 loss: 0.0030 lr: 0.02\n",
            "iteration: 238070 loss: 0.0029 lr: 0.02\n",
            "iteration: 238080 loss: 0.0036 lr: 0.02\n",
            "iteration: 238090 loss: 0.0042 lr: 0.02\n",
            "iteration: 238100 loss: 0.0032 lr: 0.02\n",
            "iteration: 238110 loss: 0.0026 lr: 0.02\n",
            "iteration: 238120 loss: 0.0040 lr: 0.02\n",
            "iteration: 238130 loss: 0.0029 lr: 0.02\n",
            "iteration: 238140 loss: 0.0028 lr: 0.02\n",
            "iteration: 238150 loss: 0.0027 lr: 0.02\n",
            "iteration: 238160 loss: 0.0029 lr: 0.02\n",
            "iteration: 238170 loss: 0.0048 lr: 0.02\n",
            "iteration: 238180 loss: 0.0030 lr: 0.02\n",
            "iteration: 238190 loss: 0.0038 lr: 0.02\n",
            "iteration: 238200 loss: 0.0035 lr: 0.02\n",
            "iteration: 238210 loss: 0.0034 lr: 0.02\n",
            "iteration: 238220 loss: 0.0029 lr: 0.02\n",
            "iteration: 238230 loss: 0.0029 lr: 0.02\n",
            "iteration: 238240 loss: 0.0035 lr: 0.02\n",
            "iteration: 238250 loss: 0.0033 lr: 0.02\n",
            "iteration: 238260 loss: 0.0039 lr: 0.02\n",
            "iteration: 238270 loss: 0.0034 lr: 0.02\n",
            "iteration: 238280 loss: 0.0035 lr: 0.02\n",
            "iteration: 238290 loss: 0.0040 lr: 0.02\n",
            "iteration: 238300 loss: 0.0029 lr: 0.02\n",
            "iteration: 238310 loss: 0.0042 lr: 0.02\n",
            "iteration: 238320 loss: 0.0039 lr: 0.02\n",
            "iteration: 238330 loss: 0.0031 lr: 0.02\n",
            "iteration: 238340 loss: 0.0031 lr: 0.02\n",
            "iteration: 238350 loss: 0.0028 lr: 0.02\n",
            "iteration: 238360 loss: 0.0033 lr: 0.02\n",
            "iteration: 238370 loss: 0.0033 lr: 0.02\n",
            "iteration: 238380 loss: 0.0034 lr: 0.02\n",
            "iteration: 238390 loss: 0.0031 lr: 0.02\n",
            "iteration: 238400 loss: 0.0034 lr: 0.02\n",
            "iteration: 238410 loss: 0.0036 lr: 0.02\n",
            "iteration: 238420 loss: 0.0029 lr: 0.02\n",
            "iteration: 238430 loss: 0.0034 lr: 0.02\n",
            "iteration: 238440 loss: 0.0041 lr: 0.02\n",
            "iteration: 238450 loss: 0.0036 lr: 0.02\n",
            "iteration: 238460 loss: 0.0033 lr: 0.02\n",
            "iteration: 238470 loss: 0.0039 lr: 0.02\n",
            "iteration: 238480 loss: 0.0033 lr: 0.02\n",
            "iteration: 238490 loss: 0.0036 lr: 0.02\n",
            "iteration: 238500 loss: 0.0035 lr: 0.02\n",
            "iteration: 238510 loss: 0.0037 lr: 0.02\n",
            "iteration: 238520 loss: 0.0033 lr: 0.02\n",
            "iteration: 238530 loss: 0.0034 lr: 0.02\n",
            "iteration: 238540 loss: 0.0038 lr: 0.02\n",
            "iteration: 238550 loss: 0.0033 lr: 0.02\n",
            "iteration: 238560 loss: 0.0030 lr: 0.02\n",
            "iteration: 238570 loss: 0.0040 lr: 0.02\n",
            "iteration: 238580 loss: 0.0033 lr: 0.02\n",
            "iteration: 238590 loss: 0.0029 lr: 0.02\n",
            "iteration: 238600 loss: 0.0030 lr: 0.02\n",
            "iteration: 238610 loss: 0.0031 lr: 0.02\n",
            "iteration: 238620 loss: 0.0038 lr: 0.02\n",
            "iteration: 238630 loss: 0.0034 lr: 0.02\n",
            "iteration: 238640 loss: 0.0029 lr: 0.02\n",
            "iteration: 238650 loss: 0.0039 lr: 0.02\n",
            "iteration: 238660 loss: 0.0031 lr: 0.02\n",
            "iteration: 238670 loss: 0.0025 lr: 0.02\n",
            "iteration: 238680 loss: 0.0027 lr: 0.02\n",
            "iteration: 238690 loss: 0.0022 lr: 0.02\n",
            "iteration: 238700 loss: 0.0030 lr: 0.02\n",
            "iteration: 238710 loss: 0.0037 lr: 0.02\n",
            "iteration: 238720 loss: 0.0033 lr: 0.02\n",
            "iteration: 238730 loss: 0.0032 lr: 0.02\n",
            "iteration: 238740 loss: 0.0031 lr: 0.02\n",
            "iteration: 238750 loss: 0.0030 lr: 0.02\n",
            "iteration: 238760 loss: 0.0029 lr: 0.02\n",
            "iteration: 238770 loss: 0.0028 lr: 0.02\n",
            "iteration: 238780 loss: 0.0030 lr: 0.02\n",
            "iteration: 238790 loss: 0.0028 lr: 0.02\n",
            "iteration: 238800 loss: 0.0031 lr: 0.02\n",
            "iteration: 238810 loss: 0.0029 lr: 0.02\n",
            "iteration: 238820 loss: 0.0027 lr: 0.02\n",
            "iteration: 238830 loss: 0.0030 lr: 0.02\n",
            "iteration: 238840 loss: 0.0024 lr: 0.02\n",
            "iteration: 238850 loss: 0.0042 lr: 0.02\n",
            "iteration: 238860 loss: 0.0025 lr: 0.02\n",
            "iteration: 238870 loss: 0.0027 lr: 0.02\n",
            "iteration: 238880 loss: 0.0031 lr: 0.02\n",
            "iteration: 238890 loss: 0.0031 lr: 0.02\n",
            "iteration: 238900 loss: 0.0034 lr: 0.02\n",
            "iteration: 238910 loss: 0.0037 lr: 0.02\n",
            "iteration: 238920 loss: 0.0037 lr: 0.02\n",
            "iteration: 238930 loss: 0.0031 lr: 0.02\n",
            "iteration: 238940 loss: 0.0033 lr: 0.02\n",
            "iteration: 238950 loss: 0.0034 lr: 0.02\n",
            "iteration: 238960 loss: 0.0035 lr: 0.02\n",
            "iteration: 238970 loss: 0.0031 lr: 0.02\n",
            "iteration: 238980 loss: 0.0030 lr: 0.02\n",
            "iteration: 238990 loss: 0.0033 lr: 0.02\n",
            "iteration: 239000 loss: 0.0031 lr: 0.02\n",
            "iteration: 239010 loss: 0.0043 lr: 0.02\n",
            "iteration: 239020 loss: 0.0039 lr: 0.02\n",
            "iteration: 239030 loss: 0.0034 lr: 0.02\n",
            "iteration: 239040 loss: 0.0024 lr: 0.02\n",
            "iteration: 239050 loss: 0.0032 lr: 0.02\n",
            "iteration: 239060 loss: 0.0044 lr: 0.02\n",
            "iteration: 239070 loss: 0.0029 lr: 0.02\n",
            "iteration: 239080 loss: 0.0034 lr: 0.02\n",
            "iteration: 239090 loss: 0.0028 lr: 0.02\n",
            "iteration: 239100 loss: 0.0036 lr: 0.02\n",
            "iteration: 239110 loss: 0.0026 lr: 0.02\n",
            "iteration: 239120 loss: 0.0039 lr: 0.02\n",
            "iteration: 239130 loss: 0.0034 lr: 0.02\n",
            "iteration: 239140 loss: 0.0040 lr: 0.02\n",
            "iteration: 239150 loss: 0.0031 lr: 0.02\n",
            "iteration: 239160 loss: 0.0036 lr: 0.02\n",
            "iteration: 239170 loss: 0.0032 lr: 0.02\n",
            "iteration: 239180 loss: 0.0036 lr: 0.02\n",
            "iteration: 239190 loss: 0.0032 lr: 0.02\n",
            "iteration: 239200 loss: 0.0029 lr: 0.02\n",
            "iteration: 239210 loss: 0.0033 lr: 0.02\n",
            "iteration: 239220 loss: 0.0047 lr: 0.02\n",
            "iteration: 239230 loss: 0.0030 lr: 0.02\n",
            "iteration: 239240 loss: 0.0032 lr: 0.02\n",
            "iteration: 239250 loss: 0.0039 lr: 0.02\n",
            "iteration: 239260 loss: 0.0027 lr: 0.02\n",
            "iteration: 239270 loss: 0.0048 lr: 0.02\n",
            "iteration: 239280 loss: 0.0030 lr: 0.02\n",
            "iteration: 239290 loss: 0.0032 lr: 0.02\n",
            "iteration: 239300 loss: 0.0026 lr: 0.02\n",
            "iteration: 239310 loss: 0.0032 lr: 0.02\n",
            "iteration: 239320 loss: 0.0035 lr: 0.02\n",
            "iteration: 239330 loss: 0.0031 lr: 0.02\n",
            "iteration: 239340 loss: 0.0026 lr: 0.02\n",
            "iteration: 239350 loss: 0.0031 lr: 0.02\n",
            "iteration: 239360 loss: 0.0031 lr: 0.02\n",
            "iteration: 239370 loss: 0.0029 lr: 0.02\n",
            "iteration: 239380 loss: 0.0031 lr: 0.02\n",
            "iteration: 239390 loss: 0.0022 lr: 0.02\n",
            "iteration: 239400 loss: 0.0032 lr: 0.02\n",
            "iteration: 239410 loss: 0.0026 lr: 0.02\n",
            "iteration: 239420 loss: 0.0027 lr: 0.02\n",
            "iteration: 239430 loss: 0.0028 lr: 0.02\n",
            "iteration: 239440 loss: 0.0031 lr: 0.02\n",
            "iteration: 239450 loss: 0.0026 lr: 0.02\n",
            "iteration: 239460 loss: 0.0036 lr: 0.02\n",
            "iteration: 239470 loss: 0.0030 lr: 0.02\n",
            "iteration: 239480 loss: 0.0035 lr: 0.02\n",
            "iteration: 239490 loss: 0.0035 lr: 0.02\n",
            "iteration: 239500 loss: 0.0030 lr: 0.02\n",
            "iteration: 239510 loss: 0.0035 lr: 0.02\n",
            "iteration: 239520 loss: 0.0032 lr: 0.02\n",
            "iteration: 239530 loss: 0.0029 lr: 0.02\n",
            "iteration: 239540 loss: 0.0036 lr: 0.02\n",
            "iteration: 239550 loss: 0.0033 lr: 0.02\n",
            "iteration: 239560 loss: 0.0027 lr: 0.02\n",
            "iteration: 239570 loss: 0.0034 lr: 0.02\n",
            "iteration: 239580 loss: 0.0033 lr: 0.02\n",
            "iteration: 239590 loss: 0.0036 lr: 0.02\n",
            "iteration: 239600 loss: 0.0031 lr: 0.02\n",
            "iteration: 239610 loss: 0.0041 lr: 0.02\n",
            "iteration: 239620 loss: 0.0036 lr: 0.02\n",
            "iteration: 239630 loss: 0.0031 lr: 0.02\n",
            "iteration: 239640 loss: 0.0032 lr: 0.02\n",
            "iteration: 239650 loss: 0.0028 lr: 0.02\n",
            "iteration: 239660 loss: 0.0042 lr: 0.02\n",
            "iteration: 239670 loss: 0.0031 lr: 0.02\n",
            "iteration: 239680 loss: 0.0028 lr: 0.02\n",
            "iteration: 239690 loss: 0.0032 lr: 0.02\n",
            "iteration: 239700 loss: 0.0030 lr: 0.02\n",
            "iteration: 239710 loss: 0.0026 lr: 0.02\n",
            "iteration: 239720 loss: 0.0037 lr: 0.02\n",
            "iteration: 239730 loss: 0.0027 lr: 0.02\n",
            "iteration: 239740 loss: 0.0033 lr: 0.02\n",
            "iteration: 239750 loss: 0.0027 lr: 0.02\n",
            "iteration: 239760 loss: 0.0029 lr: 0.02\n",
            "iteration: 239770 loss: 0.0026 lr: 0.02\n",
            "iteration: 239780 loss: 0.0027 lr: 0.02\n",
            "iteration: 239790 loss: 0.0026 lr: 0.02\n",
            "iteration: 239800 loss: 0.0027 lr: 0.02\n",
            "iteration: 239810 loss: 0.0041 lr: 0.02\n",
            "iteration: 239820 loss: 0.0030 lr: 0.02\n",
            "iteration: 239830 loss: 0.0031 lr: 0.02\n",
            "iteration: 239840 loss: 0.0031 lr: 0.02\n",
            "iteration: 239850 loss: 0.0027 lr: 0.02\n",
            "iteration: 239860 loss: 0.0027 lr: 0.02\n",
            "iteration: 239870 loss: 0.0034 lr: 0.02\n",
            "iteration: 239880 loss: 0.0034 lr: 0.02\n",
            "iteration: 239890 loss: 0.0034 lr: 0.02\n",
            "iteration: 239900 loss: 0.0024 lr: 0.02\n",
            "iteration: 239910 loss: 0.0026 lr: 0.02\n",
            "iteration: 239920 loss: 0.0031 lr: 0.02\n",
            "iteration: 239930 loss: 0.0029 lr: 0.02\n",
            "iteration: 239940 loss: 0.0042 lr: 0.02\n",
            "iteration: 239950 loss: 0.0031 lr: 0.02\n",
            "iteration: 239960 loss: 0.0037 lr: 0.02\n",
            "iteration: 239970 loss: 0.0032 lr: 0.02\n",
            "iteration: 239980 loss: 0.0036 lr: 0.02\n",
            "iteration: 239990 loss: 0.0034 lr: 0.02\n",
            "iteration: 240000 loss: 0.0028 lr: 0.02\n",
            "iteration: 240010 loss: 0.0043 lr: 0.02\n",
            "iteration: 240020 loss: 0.0039 lr: 0.02\n",
            "iteration: 240030 loss: 0.0036 lr: 0.02\n",
            "iteration: 240040 loss: 0.0043 lr: 0.02\n",
            "iteration: 240050 loss: 0.0042 lr: 0.02\n",
            "iteration: 240060 loss: 0.0032 lr: 0.02\n",
            "iteration: 240070 loss: 0.0038 lr: 0.02\n",
            "iteration: 240080 loss: 0.0033 lr: 0.02\n",
            "iteration: 240090 loss: 0.0028 lr: 0.02\n",
            "iteration: 240100 loss: 0.0033 lr: 0.02\n",
            "iteration: 240110 loss: 0.0027 lr: 0.02\n",
            "iteration: 240120 loss: 0.0033 lr: 0.02\n",
            "iteration: 240130 loss: 0.0034 lr: 0.02\n",
            "iteration: 240140 loss: 0.0037 lr: 0.02\n",
            "iteration: 240150 loss: 0.0043 lr: 0.02\n",
            "iteration: 240160 loss: 0.0026 lr: 0.02\n",
            "iteration: 240170 loss: 0.0037 lr: 0.02\n",
            "iteration: 240180 loss: 0.0050 lr: 0.02\n",
            "iteration: 240190 loss: 0.0031 lr: 0.02\n",
            "iteration: 240200 loss: 0.0036 lr: 0.02\n",
            "iteration: 240210 loss: 0.0028 lr: 0.02\n",
            "iteration: 240220 loss: 0.0043 lr: 0.02\n",
            "iteration: 240230 loss: 0.0034 lr: 0.02\n",
            "iteration: 240240 loss: 0.0034 lr: 0.02\n",
            "iteration: 240250 loss: 0.0042 lr: 0.02\n",
            "iteration: 240260 loss: 0.0037 lr: 0.02\n",
            "iteration: 240270 loss: 0.0037 lr: 0.02\n",
            "iteration: 240280 loss: 0.0029 lr: 0.02\n",
            "iteration: 240290 loss: 0.0043 lr: 0.02\n",
            "iteration: 240300 loss: 0.0041 lr: 0.02\n",
            "iteration: 240310 loss: 0.0030 lr: 0.02\n",
            "iteration: 240320 loss: 0.0040 lr: 0.02\n",
            "iteration: 240330 loss: 0.0029 lr: 0.02\n",
            "iteration: 240340 loss: 0.0032 lr: 0.02\n",
            "iteration: 240350 loss: 0.0033 lr: 0.02\n",
            "iteration: 240360 loss: 0.0034 lr: 0.02\n",
            "iteration: 240370 loss: 0.0037 lr: 0.02\n",
            "iteration: 240380 loss: 0.0032 lr: 0.02\n",
            "iteration: 240390 loss: 0.0036 lr: 0.02\n",
            "iteration: 240400 loss: 0.0028 lr: 0.02\n",
            "iteration: 240410 loss: 0.0033 lr: 0.02\n",
            "iteration: 240420 loss: 0.0038 lr: 0.02\n",
            "iteration: 240430 loss: 0.0031 lr: 0.02\n",
            "iteration: 240440 loss: 0.0032 lr: 0.02\n",
            "iteration: 240450 loss: 0.0030 lr: 0.02\n",
            "iteration: 240460 loss: 0.0031 lr: 0.02\n",
            "iteration: 240470 loss: 0.0039 lr: 0.02\n",
            "iteration: 240480 loss: 0.0033 lr: 0.02\n",
            "iteration: 240490 loss: 0.0039 lr: 0.02\n",
            "iteration: 240500 loss: 0.0029 lr: 0.02\n",
            "iteration: 240510 loss: 0.0038 lr: 0.02\n",
            "iteration: 240520 loss: 0.0024 lr: 0.02\n",
            "iteration: 240530 loss: 0.0038 lr: 0.02\n",
            "iteration: 240540 loss: 0.0032 lr: 0.02\n",
            "iteration: 240550 loss: 0.0027 lr: 0.02\n",
            "iteration: 240560 loss: 0.0029 lr: 0.02\n",
            "iteration: 240570 loss: 0.0040 lr: 0.02\n",
            "iteration: 240580 loss: 0.0033 lr: 0.02\n",
            "iteration: 240590 loss: 0.0027 lr: 0.02\n",
            "iteration: 240600 loss: 0.0037 lr: 0.02\n",
            "iteration: 240610 loss: 0.0026 lr: 0.02\n",
            "iteration: 240620 loss: 0.0033 lr: 0.02\n",
            "iteration: 240630 loss: 0.0033 lr: 0.02\n",
            "iteration: 240640 loss: 0.0022 lr: 0.02\n",
            "iteration: 240650 loss: 0.0037 lr: 0.02\n",
            "iteration: 240660 loss: 0.0034 lr: 0.02\n",
            "iteration: 240670 loss: 0.0032 lr: 0.02\n",
            "iteration: 240680 loss: 0.0029 lr: 0.02\n",
            "iteration: 240690 loss: 0.0031 lr: 0.02\n",
            "iteration: 240700 loss: 0.0029 lr: 0.02\n",
            "iteration: 240710 loss: 0.0030 lr: 0.02\n",
            "iteration: 240720 loss: 0.0035 lr: 0.02\n",
            "iteration: 240730 loss: 0.0039 lr: 0.02\n",
            "iteration: 240740 loss: 0.0042 lr: 0.02\n",
            "iteration: 240750 loss: 0.0026 lr: 0.02\n",
            "iteration: 240760 loss: 0.0036 lr: 0.02\n",
            "iteration: 240770 loss: 0.0036 lr: 0.02\n",
            "iteration: 240780 loss: 0.0024 lr: 0.02\n",
            "iteration: 240790 loss: 0.0030 lr: 0.02\n",
            "iteration: 240800 loss: 0.0035 lr: 0.02\n",
            "iteration: 240810 loss: 0.0031 lr: 0.02\n",
            "iteration: 240820 loss: 0.0034 lr: 0.02\n",
            "iteration: 240830 loss: 0.0026 lr: 0.02\n",
            "iteration: 240840 loss: 0.0028 lr: 0.02\n",
            "iteration: 240850 loss: 0.0040 lr: 0.02\n",
            "iteration: 240860 loss: 0.0031 lr: 0.02\n",
            "iteration: 240870 loss: 0.0030 lr: 0.02\n",
            "iteration: 240880 loss: 0.0033 lr: 0.02\n",
            "iteration: 240890 loss: 0.0038 lr: 0.02\n",
            "iteration: 240900 loss: 0.0041 lr: 0.02\n",
            "iteration: 240910 loss: 0.0039 lr: 0.02\n",
            "iteration: 240920 loss: 0.0039 lr: 0.02\n",
            "iteration: 240930 loss: 0.0040 lr: 0.02\n",
            "iteration: 240940 loss: 0.0042 lr: 0.02\n",
            "iteration: 240950 loss: 0.0045 lr: 0.02\n",
            "iteration: 240960 loss: 0.0043 lr: 0.02\n",
            "iteration: 240970 loss: 0.0036 lr: 0.02\n",
            "iteration: 240980 loss: 0.0028 lr: 0.02\n",
            "iteration: 240990 loss: 0.0030 lr: 0.02\n",
            "iteration: 241000 loss: 0.0041 lr: 0.02\n",
            "iteration: 241010 loss: 0.0033 lr: 0.02\n",
            "iteration: 241020 loss: 0.0027 lr: 0.02\n",
            "iteration: 241030 loss: 0.0026 lr: 0.02\n",
            "iteration: 241040 loss: 0.0026 lr: 0.02\n",
            "iteration: 241050 loss: 0.0039 lr: 0.02\n",
            "iteration: 241060 loss: 0.0034 lr: 0.02\n",
            "iteration: 241070 loss: 0.0037 lr: 0.02\n",
            "iteration: 241080 loss: 0.0027 lr: 0.02\n",
            "iteration: 241090 loss: 0.0028 lr: 0.02\n",
            "iteration: 241100 loss: 0.0036 lr: 0.02\n",
            "iteration: 241110 loss: 0.0031 lr: 0.02\n",
            "iteration: 241120 loss: 0.0031 lr: 0.02\n",
            "iteration: 241130 loss: 0.0029 lr: 0.02\n",
            "iteration: 241140 loss: 0.0037 lr: 0.02\n",
            "iteration: 241150 loss: 0.0035 lr: 0.02\n",
            "iteration: 241160 loss: 0.0041 lr: 0.02\n",
            "iteration: 241170 loss: 0.0031 lr: 0.02\n",
            "iteration: 241180 loss: 0.0033 lr: 0.02\n",
            "iteration: 241190 loss: 0.0030 lr: 0.02\n",
            "iteration: 241200 loss: 0.0024 lr: 0.02\n",
            "iteration: 241210 loss: 0.0033 lr: 0.02\n",
            "iteration: 241220 loss: 0.0032 lr: 0.02\n",
            "iteration: 241230 loss: 0.0035 lr: 0.02\n",
            "iteration: 241240 loss: 0.0030 lr: 0.02\n",
            "iteration: 241250 loss: 0.0039 lr: 0.02\n",
            "iteration: 241260 loss: 0.0035 lr: 0.02\n",
            "iteration: 241270 loss: 0.0038 lr: 0.02\n",
            "iteration: 241280 loss: 0.0035 lr: 0.02\n",
            "iteration: 241290 loss: 0.0033 lr: 0.02\n",
            "iteration: 241300 loss: 0.0033 lr: 0.02\n",
            "iteration: 241310 loss: 0.0028 lr: 0.02\n",
            "iteration: 241320 loss: 0.0036 lr: 0.02\n",
            "iteration: 241330 loss: 0.0039 lr: 0.02\n",
            "iteration: 241340 loss: 0.0035 lr: 0.02\n",
            "iteration: 241350 loss: 0.0034 lr: 0.02\n",
            "iteration: 241360 loss: 0.0038 lr: 0.02\n",
            "iteration: 241370 loss: 0.0032 lr: 0.02\n",
            "iteration: 241380 loss: 0.0041 lr: 0.02\n",
            "iteration: 241390 loss: 0.0028 lr: 0.02\n",
            "iteration: 241400 loss: 0.0038 lr: 0.02\n",
            "iteration: 241410 loss: 0.0037 lr: 0.02\n",
            "iteration: 241420 loss: 0.0029 lr: 0.02\n",
            "iteration: 241430 loss: 0.0033 lr: 0.02\n",
            "iteration: 241440 loss: 0.0034 lr: 0.02\n",
            "iteration: 241450 loss: 0.0032 lr: 0.02\n",
            "iteration: 241460 loss: 0.0028 lr: 0.02\n",
            "iteration: 241470 loss: 0.0034 lr: 0.02\n",
            "iteration: 241480 loss: 0.0031 lr: 0.02\n",
            "iteration: 241490 loss: 0.0033 lr: 0.02\n",
            "iteration: 241500 loss: 0.0036 lr: 0.02\n",
            "iteration: 241510 loss: 0.0037 lr: 0.02\n",
            "iteration: 241520 loss: 0.0024 lr: 0.02\n",
            "iteration: 241530 loss: 0.0043 lr: 0.02\n",
            "iteration: 241540 loss: 0.0035 lr: 0.02\n",
            "iteration: 241550 loss: 0.0026 lr: 0.02\n",
            "iteration: 241560 loss: 0.0031 lr: 0.02\n",
            "iteration: 241570 loss: 0.0032 lr: 0.02\n",
            "iteration: 241580 loss: 0.0035 lr: 0.02\n",
            "iteration: 241590 loss: 0.0035 lr: 0.02\n",
            "iteration: 241600 loss: 0.0027 lr: 0.02\n",
            "iteration: 241610 loss: 0.0034 lr: 0.02\n",
            "iteration: 241620 loss: 0.0035 lr: 0.02\n",
            "iteration: 241630 loss: 0.0031 lr: 0.02\n",
            "iteration: 241640 loss: 0.0031 lr: 0.02\n",
            "iteration: 241650 loss: 0.0026 lr: 0.02\n",
            "iteration: 241660 loss: 0.0044 lr: 0.02\n",
            "iteration: 241670 loss: 0.0038 lr: 0.02\n",
            "iteration: 241680 loss: 0.0033 lr: 0.02\n",
            "iteration: 241690 loss: 0.0038 lr: 0.02\n",
            "iteration: 241700 loss: 0.0030 lr: 0.02\n",
            "iteration: 241710 loss: 0.0025 lr: 0.02\n",
            "iteration: 241720 loss: 0.0027 lr: 0.02\n",
            "iteration: 241730 loss: 0.0024 lr: 0.02\n",
            "iteration: 241740 loss: 0.0029 lr: 0.02\n",
            "iteration: 241750 loss: 0.0033 lr: 0.02\n",
            "iteration: 241760 loss: 0.0051 lr: 0.02\n",
            "iteration: 241770 loss: 0.0030 lr: 0.02\n",
            "iteration: 241780 loss: 0.0036 lr: 0.02\n",
            "iteration: 241790 loss: 0.0034 lr: 0.02\n",
            "iteration: 241800 loss: 0.0033 lr: 0.02\n",
            "iteration: 241810 loss: 0.0031 lr: 0.02\n",
            "iteration: 241820 loss: 0.0038 lr: 0.02\n",
            "iteration: 241830 loss: 0.0037 lr: 0.02\n",
            "iteration: 241840 loss: 0.0032 lr: 0.02\n",
            "iteration: 241850 loss: 0.0036 lr: 0.02\n",
            "iteration: 241860 loss: 0.0037 lr: 0.02\n",
            "iteration: 241870 loss: 0.0032 lr: 0.02\n",
            "iteration: 241880 loss: 0.0036 lr: 0.02\n",
            "iteration: 241890 loss: 0.0039 lr: 0.02\n",
            "iteration: 241900 loss: 0.0033 lr: 0.02\n",
            "iteration: 241910 loss: 0.0039 lr: 0.02\n",
            "iteration: 241920 loss: 0.0034 lr: 0.02\n",
            "iteration: 241930 loss: 0.0030 lr: 0.02\n",
            "iteration: 241940 loss: 0.0027 lr: 0.02\n",
            "iteration: 241950 loss: 0.0031 lr: 0.02\n",
            "iteration: 241960 loss: 0.0028 lr: 0.02\n",
            "iteration: 241970 loss: 0.0033 lr: 0.02\n",
            "iteration: 241980 loss: 0.0029 lr: 0.02\n",
            "iteration: 241990 loss: 0.0030 lr: 0.02\n",
            "iteration: 242000 loss: 0.0036 lr: 0.02\n",
            "iteration: 242010 loss: 0.0036 lr: 0.02\n",
            "iteration: 242020 loss: 0.0034 lr: 0.02\n",
            "iteration: 242030 loss: 0.0032 lr: 0.02\n",
            "iteration: 242040 loss: 0.0031 lr: 0.02\n",
            "iteration: 242050 loss: 0.0037 lr: 0.02\n",
            "iteration: 242060 loss: 0.0033 lr: 0.02\n",
            "iteration: 242070 loss: 0.0040 lr: 0.02\n",
            "iteration: 242080 loss: 0.0043 lr: 0.02\n",
            "iteration: 242090 loss: 0.0043 lr: 0.02\n",
            "iteration: 242100 loss: 0.0035 lr: 0.02\n",
            "iteration: 242110 loss: 0.0030 lr: 0.02\n",
            "iteration: 242120 loss: 0.0032 lr: 0.02\n",
            "iteration: 242130 loss: 0.0038 lr: 0.02\n",
            "iteration: 242140 loss: 0.0025 lr: 0.02\n",
            "iteration: 242150 loss: 0.0032 lr: 0.02\n",
            "iteration: 242160 loss: 0.0034 lr: 0.02\n",
            "iteration: 242170 loss: 0.0035 lr: 0.02\n",
            "iteration: 242180 loss: 0.0029 lr: 0.02\n",
            "iteration: 242190 loss: 0.0028 lr: 0.02\n",
            "iteration: 242200 loss: 0.0032 lr: 0.02\n",
            "iteration: 242210 loss: 0.0024 lr: 0.02\n",
            "iteration: 242220 loss: 0.0027 lr: 0.02\n",
            "iteration: 242230 loss: 0.0034 lr: 0.02\n",
            "iteration: 242240 loss: 0.0030 lr: 0.02\n",
            "iteration: 242250 loss: 0.0033 lr: 0.02\n",
            "iteration: 242260 loss: 0.0050 lr: 0.02\n",
            "iteration: 242270 loss: 0.0029 lr: 0.02\n",
            "iteration: 242280 loss: 0.0028 lr: 0.02\n",
            "iteration: 242290 loss: 0.0028 lr: 0.02\n",
            "iteration: 242300 loss: 0.0037 lr: 0.02\n",
            "iteration: 242310 loss: 0.0037 lr: 0.02\n",
            "iteration: 242320 loss: 0.0035 lr: 0.02\n",
            "iteration: 242330 loss: 0.0023 lr: 0.02\n",
            "iteration: 242340 loss: 0.0036 lr: 0.02\n",
            "iteration: 242350 loss: 0.0035 lr: 0.02\n",
            "iteration: 242360 loss: 0.0033 lr: 0.02\n",
            "iteration: 242370 loss: 0.0038 lr: 0.02\n",
            "iteration: 242380 loss: 0.0039 lr: 0.02\n",
            "iteration: 242390 loss: 0.0030 lr: 0.02\n",
            "iteration: 242400 loss: 0.0026 lr: 0.02\n",
            "iteration: 242410 loss: 0.0035 lr: 0.02\n",
            "iteration: 242420 loss: 0.0031 lr: 0.02\n",
            "iteration: 242430 loss: 0.0028 lr: 0.02\n",
            "iteration: 242440 loss: 0.0033 lr: 0.02\n",
            "iteration: 242450 loss: 0.0038 lr: 0.02\n",
            "iteration: 242460 loss: 0.0030 lr: 0.02\n",
            "iteration: 242470 loss: 0.0031 lr: 0.02\n",
            "iteration: 242480 loss: 0.0032 lr: 0.02\n",
            "iteration: 242490 loss: 0.0044 lr: 0.02\n",
            "iteration: 242500 loss: 0.0036 lr: 0.02\n",
            "iteration: 242510 loss: 0.0033 lr: 0.02\n",
            "iteration: 242520 loss: 0.0038 lr: 0.02\n",
            "iteration: 242530 loss: 0.0034 lr: 0.02\n",
            "iteration: 242540 loss: 0.0036 lr: 0.02\n",
            "iteration: 242550 loss: 0.0033 lr: 0.02\n",
            "iteration: 242560 loss: 0.0033 lr: 0.02\n",
            "iteration: 242570 loss: 0.0029 lr: 0.02\n",
            "iteration: 242580 loss: 0.0034 lr: 0.02\n",
            "iteration: 242590 loss: 0.0037 lr: 0.02\n",
            "iteration: 242600 loss: 0.0035 lr: 0.02\n",
            "iteration: 242610 loss: 0.0041 lr: 0.02\n",
            "iteration: 242620 loss: 0.0035 lr: 0.02\n",
            "iteration: 242630 loss: 0.0024 lr: 0.02\n",
            "iteration: 242640 loss: 0.0032 lr: 0.02\n",
            "iteration: 242650 loss: 0.0032 lr: 0.02\n",
            "iteration: 242660 loss: 0.0034 lr: 0.02\n",
            "iteration: 242670 loss: 0.0031 lr: 0.02\n",
            "iteration: 242680 loss: 0.0027 lr: 0.02\n",
            "iteration: 242690 loss: 0.0029 lr: 0.02\n",
            "iteration: 242700 loss: 0.0041 lr: 0.02\n",
            "iteration: 242710 loss: 0.0035 lr: 0.02\n",
            "iteration: 242720 loss: 0.0029 lr: 0.02\n",
            "iteration: 242730 loss: 0.0028 lr: 0.02\n",
            "iteration: 242740 loss: 0.0026 lr: 0.02\n",
            "iteration: 242750 loss: 0.0037 lr: 0.02\n",
            "iteration: 242760 loss: 0.0027 lr: 0.02\n",
            "iteration: 242770 loss: 0.0032 lr: 0.02\n",
            "iteration: 242780 loss: 0.0043 lr: 0.02\n",
            "iteration: 242790 loss: 0.0029 lr: 0.02\n",
            "iteration: 242800 loss: 0.0026 lr: 0.02\n",
            "iteration: 242810 loss: 0.0032 lr: 0.02\n",
            "iteration: 242820 loss: 0.0038 lr: 0.02\n",
            "iteration: 242830 loss: 0.0031 lr: 0.02\n",
            "iteration: 242840 loss: 0.0030 lr: 0.02\n",
            "iteration: 242850 loss: 0.0036 lr: 0.02\n",
            "iteration: 242860 loss: 0.0031 lr: 0.02\n",
            "iteration: 242870 loss: 0.0038 lr: 0.02\n",
            "iteration: 242880 loss: 0.0036 lr: 0.02\n",
            "iteration: 242890 loss: 0.0035 lr: 0.02\n",
            "iteration: 242900 loss: 0.0033 lr: 0.02\n",
            "iteration: 242910 loss: 0.0032 lr: 0.02\n",
            "iteration: 242920 loss: 0.0029 lr: 0.02\n",
            "iteration: 242930 loss: 0.0030 lr: 0.02\n",
            "iteration: 242940 loss: 0.0028 lr: 0.02\n",
            "iteration: 242950 loss: 0.0042 lr: 0.02\n",
            "iteration: 242960 loss: 0.0033 lr: 0.02\n",
            "iteration: 242970 loss: 0.0037 lr: 0.02\n",
            "iteration: 242980 loss: 0.0030 lr: 0.02\n",
            "iteration: 242990 loss: 0.0027 lr: 0.02\n",
            "iteration: 243000 loss: 0.0034 lr: 0.02\n",
            "iteration: 243010 loss: 0.0033 lr: 0.02\n",
            "iteration: 243020 loss: 0.0030 lr: 0.02\n",
            "iteration: 243030 loss: 0.0030 lr: 0.02\n",
            "iteration: 243040 loss: 0.0031 lr: 0.02\n",
            "iteration: 243050 loss: 0.0029 lr: 0.02\n",
            "iteration: 243060 loss: 0.0031 lr: 0.02\n",
            "iteration: 243070 loss: 0.0034 lr: 0.02\n",
            "iteration: 243080 loss: 0.0034 lr: 0.02\n",
            "iteration: 243090 loss: 0.0031 lr: 0.02\n",
            "iteration: 243100 loss: 0.0026 lr: 0.02\n",
            "iteration: 243110 loss: 0.0029 lr: 0.02\n",
            "iteration: 243120 loss: 0.0033 lr: 0.02\n",
            "iteration: 243130 loss: 0.0033 lr: 0.02\n",
            "iteration: 243140 loss: 0.0029 lr: 0.02\n",
            "iteration: 243150 loss: 0.0042 lr: 0.02\n",
            "iteration: 243160 loss: 0.0037 lr: 0.02\n",
            "iteration: 243170 loss: 0.0034 lr: 0.02\n",
            "iteration: 243180 loss: 0.0032 lr: 0.02\n",
            "iteration: 243190 loss: 0.0044 lr: 0.02\n",
            "iteration: 243200 loss: 0.0037 lr: 0.02\n",
            "iteration: 243210 loss: 0.0034 lr: 0.02\n",
            "iteration: 243220 loss: 0.0030 lr: 0.02\n",
            "iteration: 243230 loss: 0.0027 lr: 0.02\n",
            "iteration: 243240 loss: 0.0037 lr: 0.02\n",
            "iteration: 243250 loss: 0.0033 lr: 0.02\n",
            "iteration: 243260 loss: 0.0030 lr: 0.02\n",
            "iteration: 243270 loss: 0.0029 lr: 0.02\n",
            "iteration: 243280 loss: 0.0029 lr: 0.02\n",
            "iteration: 243290 loss: 0.0031 lr: 0.02\n",
            "iteration: 243300 loss: 0.0037 lr: 0.02\n",
            "iteration: 243310 loss: 0.0037 lr: 0.02\n",
            "iteration: 243320 loss: 0.0030 lr: 0.02\n",
            "iteration: 243330 loss: 0.0025 lr: 0.02\n",
            "iteration: 243340 loss: 0.0032 lr: 0.02\n",
            "iteration: 243350 loss: 0.0040 lr: 0.02\n",
            "iteration: 243360 loss: 0.0031 lr: 0.02\n",
            "iteration: 243370 loss: 0.0034 lr: 0.02\n",
            "iteration: 243380 loss: 0.0034 lr: 0.02\n",
            "iteration: 243390 loss: 0.0034 lr: 0.02\n",
            "iteration: 243400 loss: 0.0029 lr: 0.02\n",
            "iteration: 243410 loss: 0.0038 lr: 0.02\n",
            "iteration: 243420 loss: 0.0035 lr: 0.02\n",
            "iteration: 243430 loss: 0.0028 lr: 0.02\n",
            "iteration: 243440 loss: 0.0037 lr: 0.02\n",
            "iteration: 243450 loss: 0.0034 lr: 0.02\n",
            "iteration: 243460 loss: 0.0030 lr: 0.02\n",
            "iteration: 243470 loss: 0.0031 lr: 0.02\n",
            "iteration: 243480 loss: 0.0039 lr: 0.02\n",
            "iteration: 243490 loss: 0.0035 lr: 0.02\n",
            "iteration: 243500 loss: 0.0026 lr: 0.02\n",
            "iteration: 243510 loss: 0.0034 lr: 0.02\n",
            "iteration: 243520 loss: 0.0038 lr: 0.02\n",
            "iteration: 243530 loss: 0.0027 lr: 0.02\n",
            "iteration: 243540 loss: 0.0039 lr: 0.02\n",
            "iteration: 243550 loss: 0.0029 lr: 0.02\n",
            "iteration: 243560 loss: 0.0029 lr: 0.02\n",
            "iteration: 243570 loss: 0.0029 lr: 0.02\n",
            "iteration: 243580 loss: 0.0032 lr: 0.02\n",
            "iteration: 243590 loss: 0.0040 lr: 0.02\n",
            "iteration: 243600 loss: 0.0039 lr: 0.02\n",
            "iteration: 243610 loss: 0.0035 lr: 0.02\n",
            "iteration: 243620 loss: 0.0025 lr: 0.02\n",
            "iteration: 243630 loss: 0.0030 lr: 0.02\n",
            "iteration: 243640 loss: 0.0030 lr: 0.02\n",
            "iteration: 243650 loss: 0.0035 lr: 0.02\n",
            "iteration: 243660 loss: 0.0025 lr: 0.02\n",
            "iteration: 243670 loss: 0.0034 lr: 0.02\n",
            "iteration: 243680 loss: 0.0029 lr: 0.02\n",
            "iteration: 243690 loss: 0.0040 lr: 0.02\n",
            "iteration: 243700 loss: 0.0035 lr: 0.02\n",
            "iteration: 243710 loss: 0.0029 lr: 0.02\n",
            "iteration: 243720 loss: 0.0027 lr: 0.02\n",
            "iteration: 243730 loss: 0.0038 lr: 0.02\n",
            "iteration: 243740 loss: 0.0033 lr: 0.02\n",
            "iteration: 243750 loss: 0.0032 lr: 0.02\n",
            "iteration: 243760 loss: 0.0049 lr: 0.02\n",
            "iteration: 243770 loss: 0.0037 lr: 0.02\n",
            "iteration: 243780 loss: 0.0029 lr: 0.02\n",
            "iteration: 243790 loss: 0.0028 lr: 0.02\n",
            "iteration: 243800 loss: 0.0033 lr: 0.02\n",
            "iteration: 243810 loss: 0.0031 lr: 0.02\n",
            "iteration: 243820 loss: 0.0039 lr: 0.02\n",
            "iteration: 243830 loss: 0.0032 lr: 0.02\n",
            "iteration: 243840 loss: 0.0025 lr: 0.02\n",
            "iteration: 243850 loss: 0.0034 lr: 0.02\n",
            "iteration: 243860 loss: 0.0026 lr: 0.02\n",
            "iteration: 243870 loss: 0.0045 lr: 0.02\n",
            "iteration: 243880 loss: 0.0035 lr: 0.02\n",
            "iteration: 243890 loss: 0.0048 lr: 0.02\n",
            "iteration: 243900 loss: 0.0034 lr: 0.02\n",
            "iteration: 243910 loss: 0.0037 lr: 0.02\n",
            "iteration: 243920 loss: 0.0031 lr: 0.02\n",
            "iteration: 243930 loss: 0.0029 lr: 0.02\n",
            "iteration: 243940 loss: 0.0039 lr: 0.02\n",
            "iteration: 243950 loss: 0.0031 lr: 0.02\n",
            "iteration: 243960 loss: 0.0037 lr: 0.02\n",
            "iteration: 243970 loss: 0.0032 lr: 0.02\n",
            "iteration: 243980 loss: 0.0032 lr: 0.02\n",
            "iteration: 243990 loss: 0.0038 lr: 0.02\n",
            "iteration: 244000 loss: 0.0037 lr: 0.02\n",
            "iteration: 244010 loss: 0.0030 lr: 0.02\n",
            "iteration: 244020 loss: 0.0037 lr: 0.02\n",
            "iteration: 244030 loss: 0.0031 lr: 0.02\n",
            "iteration: 244040 loss: 0.0032 lr: 0.02\n",
            "iteration: 244050 loss: 0.0031 lr: 0.02\n",
            "iteration: 244060 loss: 0.0035 lr: 0.02\n",
            "iteration: 244070 loss: 0.0035 lr: 0.02\n",
            "iteration: 244080 loss: 0.0025 lr: 0.02\n",
            "iteration: 244090 loss: 0.0033 lr: 0.02\n",
            "iteration: 244100 loss: 0.0030 lr: 0.02\n",
            "iteration: 244110 loss: 0.0026 lr: 0.02\n",
            "iteration: 244120 loss: 0.0027 lr: 0.02\n",
            "iteration: 244130 loss: 0.0029 lr: 0.02\n",
            "iteration: 244140 loss: 0.0031 lr: 0.02\n",
            "iteration: 244150 loss: 0.0035 lr: 0.02\n",
            "iteration: 244160 loss: 0.0031 lr: 0.02\n",
            "iteration: 244170 loss: 0.0038 lr: 0.02\n",
            "iteration: 244180 loss: 0.0031 lr: 0.02\n",
            "iteration: 244190 loss: 0.0031 lr: 0.02\n",
            "iteration: 244200 loss: 0.0029 lr: 0.02\n",
            "iteration: 244210 loss: 0.0037 lr: 0.02\n",
            "iteration: 244220 loss: 0.0032 lr: 0.02\n",
            "iteration: 244230 loss: 0.0025 lr: 0.02\n",
            "iteration: 244240 loss: 0.0034 lr: 0.02\n",
            "iteration: 244250 loss: 0.0031 lr: 0.02\n",
            "iteration: 244260 loss: 0.0031 lr: 0.02\n",
            "iteration: 244270 loss: 0.0032 lr: 0.02\n",
            "iteration: 244280 loss: 0.0035 lr: 0.02\n",
            "iteration: 244290 loss: 0.0025 lr: 0.02\n",
            "iteration: 244300 loss: 0.0037 lr: 0.02\n",
            "iteration: 244310 loss: 0.0027 lr: 0.02\n",
            "iteration: 244320 loss: 0.0030 lr: 0.02\n",
            "iteration: 244330 loss: 0.0039 lr: 0.02\n",
            "iteration: 244340 loss: 0.0031 lr: 0.02\n",
            "iteration: 244350 loss: 0.0034 lr: 0.02\n",
            "iteration: 244360 loss: 0.0035 lr: 0.02\n",
            "iteration: 244370 loss: 0.0037 lr: 0.02\n",
            "iteration: 244380 loss: 0.0029 lr: 0.02\n",
            "iteration: 244390 loss: 0.0031 lr: 0.02\n",
            "iteration: 244400 loss: 0.0034 lr: 0.02\n",
            "iteration: 244410 loss: 0.0038 lr: 0.02\n",
            "iteration: 244420 loss: 0.0024 lr: 0.02\n",
            "iteration: 244430 loss: 0.0037 lr: 0.02\n",
            "iteration: 244440 loss: 0.0027 lr: 0.02\n",
            "iteration: 244450 loss: 0.0033 lr: 0.02\n",
            "iteration: 244460 loss: 0.0035 lr: 0.02\n",
            "iteration: 244470 loss: 0.0033 lr: 0.02\n",
            "iteration: 244480 loss: 0.0031 lr: 0.02\n",
            "iteration: 244490 loss: 0.0033 lr: 0.02\n",
            "iteration: 244500 loss: 0.0032 lr: 0.02\n",
            "iteration: 244510 loss: 0.0032 lr: 0.02\n",
            "iteration: 244520 loss: 0.0033 lr: 0.02\n",
            "iteration: 244530 loss: 0.0033 lr: 0.02\n",
            "iteration: 244540 loss: 0.0037 lr: 0.02\n",
            "iteration: 244550 loss: 0.0035 lr: 0.02\n",
            "iteration: 244560 loss: 0.0037 lr: 0.02\n",
            "iteration: 244570 loss: 0.0039 lr: 0.02\n",
            "iteration: 244580 loss: 0.0037 lr: 0.02\n",
            "iteration: 244590 loss: 0.0023 lr: 0.02\n",
            "iteration: 244600 loss: 0.0034 lr: 0.02\n",
            "iteration: 244610 loss: 0.0033 lr: 0.02\n",
            "iteration: 244620 loss: 0.0031 lr: 0.02\n",
            "iteration: 244630 loss: 0.0029 lr: 0.02\n",
            "iteration: 244640 loss: 0.0029 lr: 0.02\n",
            "iteration: 244650 loss: 0.0030 lr: 0.02\n",
            "iteration: 244660 loss: 0.0036 lr: 0.02\n",
            "iteration: 244670 loss: 0.0025 lr: 0.02\n",
            "iteration: 244680 loss: 0.0032 lr: 0.02\n",
            "iteration: 244690 loss: 0.0036 lr: 0.02\n",
            "iteration: 244700 loss: 0.0033 lr: 0.02\n",
            "iteration: 244710 loss: 0.0030 lr: 0.02\n",
            "iteration: 244720 loss: 0.0026 lr: 0.02\n",
            "iteration: 244730 loss: 0.0026 lr: 0.02\n",
            "iteration: 244740 loss: 0.0026 lr: 0.02\n",
            "iteration: 244750 loss: 0.0025 lr: 0.02\n",
            "iteration: 244760 loss: 0.0023 lr: 0.02\n",
            "iteration: 244770 loss: 0.0035 lr: 0.02\n",
            "iteration: 244780 loss: 0.0030 lr: 0.02\n",
            "iteration: 244790 loss: 0.0041 lr: 0.02\n",
            "iteration: 244800 loss: 0.0036 lr: 0.02\n",
            "iteration: 244810 loss: 0.0026 lr: 0.02\n",
            "iteration: 244820 loss: 0.0033 lr: 0.02\n",
            "iteration: 244830 loss: 0.0038 lr: 0.02\n",
            "iteration: 244840 loss: 0.0034 lr: 0.02\n",
            "iteration: 244850 loss: 0.0038 lr: 0.02\n",
            "iteration: 244860 loss: 0.0031 lr: 0.02\n",
            "iteration: 244870 loss: 0.0025 lr: 0.02\n",
            "iteration: 244880 loss: 0.0033 lr: 0.02\n",
            "iteration: 244890 loss: 0.0037 lr: 0.02\n",
            "iteration: 244900 loss: 0.0041 lr: 0.02\n",
            "iteration: 244910 loss: 0.0035 lr: 0.02\n",
            "iteration: 244920 loss: 0.0037 lr: 0.02\n",
            "iteration: 244930 loss: 0.0045 lr: 0.02\n",
            "iteration: 244940 loss: 0.0029 lr: 0.02\n",
            "iteration: 244950 loss: 0.0033 lr: 0.02\n",
            "iteration: 244960 loss: 0.0025 lr: 0.02\n",
            "iteration: 244970 loss: 0.0040 lr: 0.02\n",
            "iteration: 244980 loss: 0.0032 lr: 0.02\n",
            "iteration: 244990 loss: 0.0027 lr: 0.02\n",
            "iteration: 245000 loss: 0.0039 lr: 0.02\n",
            "iteration: 245010 loss: 0.0026 lr: 0.02\n",
            "iteration: 245020 loss: 0.0036 lr: 0.02\n",
            "iteration: 245030 loss: 0.0028 lr: 0.02\n",
            "iteration: 245040 loss: 0.0046 lr: 0.02\n",
            "iteration: 245050 loss: 0.0038 lr: 0.02\n",
            "iteration: 245060 loss: 0.0034 lr: 0.02\n",
            "iteration: 245070 loss: 0.0037 lr: 0.02\n",
            "iteration: 245080 loss: 0.0026 lr: 0.02\n",
            "iteration: 245090 loss: 0.0038 lr: 0.02\n",
            "iteration: 245100 loss: 0.0031 lr: 0.02\n",
            "iteration: 245110 loss: 0.0037 lr: 0.02\n",
            "iteration: 245120 loss: 0.0037 lr: 0.02\n",
            "iteration: 245130 loss: 0.0051 lr: 0.02\n",
            "iteration: 245140 loss: 0.0040 lr: 0.02\n",
            "iteration: 245150 loss: 0.0028 lr: 0.02\n",
            "iteration: 245160 loss: 0.0037 lr: 0.02\n",
            "iteration: 245170 loss: 0.0038 lr: 0.02\n",
            "iteration: 245180 loss: 0.0031 lr: 0.02\n",
            "iteration: 245190 loss: 0.0029 lr: 0.02\n",
            "iteration: 245200 loss: 0.0031 lr: 0.02\n",
            "iteration: 245210 loss: 0.0029 lr: 0.02\n",
            "iteration: 245220 loss: 0.0029 lr: 0.02\n",
            "iteration: 245230 loss: 0.0032 lr: 0.02\n",
            "iteration: 245240 loss: 0.0034 lr: 0.02\n",
            "iteration: 245250 loss: 0.0049 lr: 0.02\n",
            "iteration: 245260 loss: 0.0026 lr: 0.02\n",
            "iteration: 245270 loss: 0.0037 lr: 0.02\n",
            "iteration: 245280 loss: 0.0029 lr: 0.02\n",
            "iteration: 245290 loss: 0.0027 lr: 0.02\n",
            "iteration: 245300 loss: 0.0030 lr: 0.02\n",
            "iteration: 245310 loss: 0.0030 lr: 0.02\n",
            "iteration: 245320 loss: 0.0035 lr: 0.02\n",
            "iteration: 245330 loss: 0.0034 lr: 0.02\n",
            "iteration: 245340 loss: 0.0041 lr: 0.02\n",
            "iteration: 245350 loss: 0.0033 lr: 0.02\n",
            "iteration: 245360 loss: 0.0040 lr: 0.02\n",
            "iteration: 245370 loss: 0.0029 lr: 0.02\n",
            "iteration: 245380 loss: 0.0029 lr: 0.02\n",
            "iteration: 245390 loss: 0.0026 lr: 0.02\n",
            "iteration: 245400 loss: 0.0027 lr: 0.02\n",
            "iteration: 245410 loss: 0.0041 lr: 0.02\n",
            "iteration: 245420 loss: 0.0027 lr: 0.02\n",
            "iteration: 245430 loss: 0.0032 lr: 0.02\n",
            "iteration: 245440 loss: 0.0036 lr: 0.02\n",
            "iteration: 245450 loss: 0.0034 lr: 0.02\n",
            "iteration: 245460 loss: 0.0044 lr: 0.02\n",
            "iteration: 245470 loss: 0.0034 lr: 0.02\n",
            "iteration: 245480 loss: 0.0031 lr: 0.02\n",
            "iteration: 245490 loss: 0.0030 lr: 0.02\n",
            "iteration: 245500 loss: 0.0038 lr: 0.02\n",
            "iteration: 245510 loss: 0.0025 lr: 0.02\n",
            "iteration: 245520 loss: 0.0039 lr: 0.02\n",
            "iteration: 245530 loss: 0.0033 lr: 0.02\n",
            "iteration: 245540 loss: 0.0028 lr: 0.02\n",
            "iteration: 245550 loss: 0.0033 lr: 0.02\n",
            "iteration: 245560 loss: 0.0033 lr: 0.02\n",
            "iteration: 245570 loss: 0.0029 lr: 0.02\n",
            "iteration: 245580 loss: 0.0033 lr: 0.02\n",
            "iteration: 245590 loss: 0.0035 lr: 0.02\n",
            "iteration: 245600 loss: 0.0037 lr: 0.02\n",
            "iteration: 245610 loss: 0.0037 lr: 0.02\n",
            "iteration: 245620 loss: 0.0026 lr: 0.02\n",
            "iteration: 245630 loss: 0.0025 lr: 0.02\n",
            "iteration: 245640 loss: 0.0022 lr: 0.02\n",
            "iteration: 245650 loss: 0.0034 lr: 0.02\n",
            "iteration: 245660 loss: 0.0033 lr: 0.02\n",
            "iteration: 245670 loss: 0.0026 lr: 0.02\n",
            "iteration: 245680 loss: 0.0035 lr: 0.02\n",
            "iteration: 245690 loss: 0.0040 lr: 0.02\n",
            "iteration: 245700 loss: 0.0037 lr: 0.02\n",
            "iteration: 245710 loss: 0.0030 lr: 0.02\n",
            "iteration: 245720 loss: 0.0029 lr: 0.02\n",
            "iteration: 245730 loss: 0.0028 lr: 0.02\n",
            "iteration: 245740 loss: 0.0038 lr: 0.02\n",
            "iteration: 245750 loss: 0.0032 lr: 0.02\n",
            "iteration: 245760 loss: 0.0030 lr: 0.02\n",
            "iteration: 245770 loss: 0.0027 lr: 0.02\n",
            "iteration: 245780 loss: 0.0041 lr: 0.02\n",
            "iteration: 245790 loss: 0.0035 lr: 0.02\n",
            "iteration: 245800 loss: 0.0036 lr: 0.02\n",
            "iteration: 245810 loss: 0.0038 lr: 0.02\n",
            "iteration: 245820 loss: 0.0028 lr: 0.02\n",
            "iteration: 245830 loss: 0.0041 lr: 0.02\n",
            "iteration: 245840 loss: 0.0030 lr: 0.02\n",
            "iteration: 245850 loss: 0.0032 lr: 0.02\n",
            "iteration: 245860 loss: 0.0028 lr: 0.02\n",
            "iteration: 245870 loss: 0.0036 lr: 0.02\n",
            "iteration: 245880 loss: 0.0035 lr: 0.02\n",
            "iteration: 245890 loss: 0.0036 lr: 0.02\n",
            "iteration: 245900 loss: 0.0029 lr: 0.02\n",
            "iteration: 245910 loss: 0.0040 lr: 0.02\n",
            "iteration: 245920 loss: 0.0034 lr: 0.02\n",
            "iteration: 245930 loss: 0.0039 lr: 0.02\n",
            "iteration: 245940 loss: 0.0037 lr: 0.02\n",
            "iteration: 245950 loss: 0.0032 lr: 0.02\n",
            "iteration: 245960 loss: 0.0032 lr: 0.02\n",
            "iteration: 245970 loss: 0.0030 lr: 0.02\n",
            "iteration: 245980 loss: 0.0034 lr: 0.02\n",
            "iteration: 245990 loss: 0.0037 lr: 0.02\n",
            "iteration: 246000 loss: 0.0035 lr: 0.02\n",
            "iteration: 246010 loss: 0.0040 lr: 0.02\n",
            "iteration: 246020 loss: 0.0037 lr: 0.02\n",
            "iteration: 246030 loss: 0.0030 lr: 0.02\n",
            "iteration: 246040 loss: 0.0035 lr: 0.02\n",
            "iteration: 246050 loss: 0.0035 lr: 0.02\n",
            "iteration: 246060 loss: 0.0028 lr: 0.02\n",
            "iteration: 246070 loss: 0.0031 lr: 0.02\n",
            "iteration: 246080 loss: 0.0030 lr: 0.02\n",
            "iteration: 246090 loss: 0.0038 lr: 0.02\n",
            "iteration: 246100 loss: 0.0041 lr: 0.02\n",
            "iteration: 246110 loss: 0.0030 lr: 0.02\n",
            "iteration: 246120 loss: 0.0028 lr: 0.02\n",
            "iteration: 246130 loss: 0.0037 lr: 0.02\n",
            "iteration: 246140 loss: 0.0031 lr: 0.02\n",
            "iteration: 246150 loss: 0.0027 lr: 0.02\n",
            "iteration: 246160 loss: 0.0035 lr: 0.02\n",
            "iteration: 246170 loss: 0.0036 lr: 0.02\n",
            "iteration: 246180 loss: 0.0032 lr: 0.02\n",
            "iteration: 246190 loss: 0.0032 lr: 0.02\n",
            "iteration: 246200 loss: 0.0026 lr: 0.02\n",
            "iteration: 246210 loss: 0.0038 lr: 0.02\n",
            "iteration: 246220 loss: 0.0039 lr: 0.02\n",
            "iteration: 246230 loss: 0.0036 lr: 0.02\n",
            "iteration: 246240 loss: 0.0031 lr: 0.02\n",
            "iteration: 246250 loss: 0.0030 lr: 0.02\n",
            "iteration: 246260 loss: 0.0034 lr: 0.02\n",
            "iteration: 246270 loss: 0.0037 lr: 0.02\n",
            "iteration: 246280 loss: 0.0032 lr: 0.02\n",
            "iteration: 246290 loss: 0.0029 lr: 0.02\n",
            "iteration: 246300 loss: 0.0030 lr: 0.02\n",
            "iteration: 246310 loss: 0.0035 lr: 0.02\n",
            "iteration: 246320 loss: 0.0034 lr: 0.02\n",
            "iteration: 246330 loss: 0.0044 lr: 0.02\n",
            "iteration: 246340 loss: 0.0039 lr: 0.02\n",
            "iteration: 246350 loss: 0.0037 lr: 0.02\n",
            "iteration: 246360 loss: 0.0030 lr: 0.02\n",
            "iteration: 246370 loss: 0.0034 lr: 0.02\n",
            "iteration: 246380 loss: 0.0038 lr: 0.02\n",
            "iteration: 246390 loss: 0.0033 lr: 0.02\n",
            "iteration: 246400 loss: 0.0026 lr: 0.02\n",
            "iteration: 246410 loss: 0.0026 lr: 0.02\n",
            "iteration: 246420 loss: 0.0036 lr: 0.02\n",
            "iteration: 246430 loss: 0.0037 lr: 0.02\n",
            "iteration: 246440 loss: 0.0030 lr: 0.02\n",
            "iteration: 246450 loss: 0.0023 lr: 0.02\n",
            "iteration: 246460 loss: 0.0028 lr: 0.02\n",
            "iteration: 246470 loss: 0.0031 lr: 0.02\n",
            "iteration: 246480 loss: 0.0033 lr: 0.02\n",
            "iteration: 246490 loss: 0.0025 lr: 0.02\n",
            "iteration: 246500 loss: 0.0026 lr: 0.02\n",
            "iteration: 246510 loss: 0.0029 lr: 0.02\n",
            "iteration: 246520 loss: 0.0029 lr: 0.02\n",
            "iteration: 246530 loss: 0.0036 lr: 0.02\n",
            "iteration: 246540 loss: 0.0030 lr: 0.02\n",
            "iteration: 246550 loss: 0.0034 lr: 0.02\n",
            "iteration: 246560 loss: 0.0030 lr: 0.02\n",
            "iteration: 246570 loss: 0.0041 lr: 0.02\n",
            "iteration: 246580 loss: 0.0032 lr: 0.02\n",
            "iteration: 246590 loss: 0.0026 lr: 0.02\n",
            "iteration: 246600 loss: 0.0035 lr: 0.02\n",
            "iteration: 246610 loss: 0.0034 lr: 0.02\n",
            "iteration: 246620 loss: 0.0030 lr: 0.02\n",
            "iteration: 246630 loss: 0.0034 lr: 0.02\n",
            "iteration: 246640 loss: 0.0040 lr: 0.02\n",
            "iteration: 246650 loss: 0.0036 lr: 0.02\n",
            "iteration: 246660 loss: 0.0041 lr: 0.02\n",
            "iteration: 246670 loss: 0.0026 lr: 0.02\n",
            "iteration: 246680 loss: 0.0032 lr: 0.02\n",
            "iteration: 246690 loss: 0.0030 lr: 0.02\n",
            "iteration: 246700 loss: 0.0031 lr: 0.02\n",
            "iteration: 246710 loss: 0.0026 lr: 0.02\n",
            "iteration: 246720 loss: 0.0032 lr: 0.02\n",
            "iteration: 246730 loss: 0.0035 lr: 0.02\n",
            "iteration: 246740 loss: 0.0032 lr: 0.02\n",
            "iteration: 246750 loss: 0.0038 lr: 0.02\n",
            "iteration: 246760 loss: 0.0033 lr: 0.02\n",
            "iteration: 246770 loss: 0.0028 lr: 0.02\n",
            "iteration: 246780 loss: 0.0031 lr: 0.02\n",
            "iteration: 246790 loss: 0.0031 lr: 0.02\n",
            "iteration: 246800 loss: 0.0031 lr: 0.02\n",
            "iteration: 246810 loss: 0.0041 lr: 0.02\n",
            "iteration: 246820 loss: 0.0029 lr: 0.02\n",
            "iteration: 246830 loss: 0.0036 lr: 0.02\n",
            "iteration: 246840 loss: 0.0029 lr: 0.02\n",
            "iteration: 246850 loss: 0.0034 lr: 0.02\n",
            "iteration: 246860 loss: 0.0035 lr: 0.02\n",
            "iteration: 246870 loss: 0.0036 lr: 0.02\n",
            "iteration: 246880 loss: 0.0037 lr: 0.02\n",
            "iteration: 246890 loss: 0.0029 lr: 0.02\n",
            "iteration: 246900 loss: 0.0036 lr: 0.02\n",
            "iteration: 246910 loss: 0.0035 lr: 0.02\n",
            "iteration: 246920 loss: 0.0027 lr: 0.02\n",
            "iteration: 246930 loss: 0.0040 lr: 0.02\n",
            "iteration: 246940 loss: 0.0033 lr: 0.02\n",
            "iteration: 246950 loss: 0.0038 lr: 0.02\n",
            "iteration: 246960 loss: 0.0026 lr: 0.02\n",
            "iteration: 246970 loss: 0.0032 lr: 0.02\n",
            "iteration: 246980 loss: 0.0036 lr: 0.02\n",
            "iteration: 246990 loss: 0.0033 lr: 0.02\n",
            "iteration: 247000 loss: 0.0031 lr: 0.02\n",
            "iteration: 247010 loss: 0.0034 lr: 0.02\n",
            "iteration: 247020 loss: 0.0032 lr: 0.02\n",
            "iteration: 247030 loss: 0.0032 lr: 0.02\n",
            "iteration: 247040 loss: 0.0033 lr: 0.02\n",
            "iteration: 247050 loss: 0.0036 lr: 0.02\n",
            "iteration: 247060 loss: 0.0032 lr: 0.02\n",
            "iteration: 247070 loss: 0.0026 lr: 0.02\n",
            "iteration: 247080 loss: 0.0026 lr: 0.02\n",
            "iteration: 247090 loss: 0.0048 lr: 0.02\n",
            "iteration: 247100 loss: 0.0030 lr: 0.02\n",
            "iteration: 247110 loss: 0.0033 lr: 0.02\n",
            "iteration: 247120 loss: 0.0027 lr: 0.02\n",
            "iteration: 247130 loss: 0.0041 lr: 0.02\n",
            "iteration: 247140 loss: 0.0037 lr: 0.02\n",
            "iteration: 247150 loss: 0.0027 lr: 0.02\n",
            "iteration: 247160 loss: 0.0033 lr: 0.02\n",
            "iteration: 247170 loss: 0.0028 lr: 0.02\n",
            "iteration: 247180 loss: 0.0022 lr: 0.02\n",
            "iteration: 247190 loss: 0.0037 lr: 0.02\n",
            "iteration: 247200 loss: 0.0039 lr: 0.02\n",
            "iteration: 247210 loss: 0.0032 lr: 0.02\n",
            "iteration: 247220 loss: 0.0024 lr: 0.02\n",
            "iteration: 247230 loss: 0.0030 lr: 0.02\n",
            "iteration: 247240 loss: 0.0030 lr: 0.02\n",
            "iteration: 247250 loss: 0.0030 lr: 0.02\n",
            "iteration: 247260 loss: 0.0039 lr: 0.02\n",
            "iteration: 247270 loss: 0.0035 lr: 0.02\n",
            "iteration: 247280 loss: 0.0037 lr: 0.02\n",
            "iteration: 247290 loss: 0.0027 lr: 0.02\n",
            "iteration: 247300 loss: 0.0037 lr: 0.02\n",
            "iteration: 247310 loss: 0.0028 lr: 0.02\n",
            "iteration: 247320 loss: 0.0036 lr: 0.02\n",
            "iteration: 247330 loss: 0.0029 lr: 0.02\n",
            "iteration: 247340 loss: 0.0035 lr: 0.02\n",
            "iteration: 247350 loss: 0.0035 lr: 0.02\n",
            "iteration: 247360 loss: 0.0028 lr: 0.02\n",
            "iteration: 247370 loss: 0.0036 lr: 0.02\n",
            "iteration: 247380 loss: 0.0032 lr: 0.02\n",
            "iteration: 247390 loss: 0.0048 lr: 0.02\n",
            "iteration: 247400 loss: 0.0032 lr: 0.02\n",
            "iteration: 247410 loss: 0.0038 lr: 0.02\n",
            "iteration: 247420 loss: 0.0032 lr: 0.02\n",
            "iteration: 247430 loss: 0.0037 lr: 0.02\n",
            "iteration: 247440 loss: 0.0034 lr: 0.02\n",
            "iteration: 247450 loss: 0.0043 lr: 0.02\n",
            "iteration: 247460 loss: 0.0031 lr: 0.02\n",
            "iteration: 247470 loss: 0.0034 lr: 0.02\n",
            "iteration: 247480 loss: 0.0021 lr: 0.02\n",
            "iteration: 247490 loss: 0.0034 lr: 0.02\n",
            "iteration: 247500 loss: 0.0032 lr: 0.02\n",
            "iteration: 247510 loss: 0.0024 lr: 0.02\n",
            "iteration: 247520 loss: 0.0038 lr: 0.02\n",
            "iteration: 247530 loss: 0.0029 lr: 0.02\n",
            "iteration: 247540 loss: 0.0040 lr: 0.02\n",
            "iteration: 247550 loss: 0.0027 lr: 0.02\n",
            "iteration: 247560 loss: 0.0029 lr: 0.02\n",
            "iteration: 247570 loss: 0.0032 lr: 0.02\n",
            "iteration: 247580 loss: 0.0031 lr: 0.02\n",
            "iteration: 247590 loss: 0.0033 lr: 0.02\n",
            "iteration: 247600 loss: 0.0027 lr: 0.02\n",
            "iteration: 247610 loss: 0.0027 lr: 0.02\n",
            "iteration: 247620 loss: 0.0030 lr: 0.02\n",
            "iteration: 247630 loss: 0.0031 lr: 0.02\n",
            "iteration: 247640 loss: 0.0026 lr: 0.02\n",
            "iteration: 247650 loss: 0.0040 lr: 0.02\n",
            "iteration: 247660 loss: 0.0034 lr: 0.02\n",
            "iteration: 247670 loss: 0.0038 lr: 0.02\n",
            "iteration: 247680 loss: 0.0033 lr: 0.02\n",
            "iteration: 247690 loss: 0.0039 lr: 0.02\n",
            "iteration: 247700 loss: 0.0030 lr: 0.02\n",
            "iteration: 247710 loss: 0.0029 lr: 0.02\n",
            "iteration: 247720 loss: 0.0027 lr: 0.02\n",
            "iteration: 247730 loss: 0.0037 lr: 0.02\n",
            "iteration: 247740 loss: 0.0031 lr: 0.02\n",
            "iteration: 247750 loss: 0.0031 lr: 0.02\n",
            "iteration: 247760 loss: 0.0035 lr: 0.02\n",
            "iteration: 247770 loss: 0.0031 lr: 0.02\n",
            "iteration: 247780 loss: 0.0028 lr: 0.02\n",
            "iteration: 247790 loss: 0.0022 lr: 0.02\n",
            "iteration: 247800 loss: 0.0032 lr: 0.02\n",
            "iteration: 247810 loss: 0.0034 lr: 0.02\n",
            "iteration: 247820 loss: 0.0031 lr: 0.02\n",
            "iteration: 247830 loss: 0.0027 lr: 0.02\n",
            "iteration: 247840 loss: 0.0039 lr: 0.02\n",
            "iteration: 247850 loss: 0.0042 lr: 0.02\n",
            "iteration: 247860 loss: 0.0037 lr: 0.02\n",
            "iteration: 247870 loss: 0.0042 lr: 0.02\n",
            "iteration: 247880 loss: 0.0031 lr: 0.02\n",
            "iteration: 247890 loss: 0.0047 lr: 0.02\n",
            "iteration: 247900 loss: 0.0031 lr: 0.02\n",
            "iteration: 247910 loss: 0.0040 lr: 0.02\n",
            "iteration: 247920 loss: 0.0028 lr: 0.02\n",
            "iteration: 247930 loss: 0.0033 lr: 0.02\n",
            "iteration: 247940 loss: 0.0037 lr: 0.02\n",
            "iteration: 247950 loss: 0.0026 lr: 0.02\n",
            "iteration: 247960 loss: 0.0035 lr: 0.02\n",
            "iteration: 247970 loss: 0.0035 lr: 0.02\n",
            "iteration: 247980 loss: 0.0033 lr: 0.02\n",
            "iteration: 247990 loss: 0.0037 lr: 0.02\n",
            "iteration: 248000 loss: 0.0031 lr: 0.02\n",
            "iteration: 248010 loss: 0.0027 lr: 0.02\n",
            "iteration: 248020 loss: 0.0031 lr: 0.02\n",
            "iteration: 248030 loss: 0.0027 lr: 0.02\n",
            "iteration: 248040 loss: 0.0033 lr: 0.02\n",
            "iteration: 248050 loss: 0.0032 lr: 0.02\n",
            "iteration: 248060 loss: 0.0028 lr: 0.02\n",
            "iteration: 248070 loss: 0.0030 lr: 0.02\n",
            "iteration: 248080 loss: 0.0028 lr: 0.02\n",
            "iteration: 248090 loss: 0.0039 lr: 0.02\n",
            "iteration: 248100 loss: 0.0023 lr: 0.02\n",
            "iteration: 248110 loss: 0.0027 lr: 0.02\n",
            "iteration: 248120 loss: 0.0031 lr: 0.02\n",
            "iteration: 248130 loss: 0.0032 lr: 0.02\n",
            "iteration: 248140 loss: 0.0045 lr: 0.02\n",
            "iteration: 248150 loss: 0.0029 lr: 0.02\n",
            "iteration: 248160 loss: 0.0034 lr: 0.02\n",
            "iteration: 248170 loss: 0.0036 lr: 0.02\n",
            "iteration: 248180 loss: 0.0042 lr: 0.02\n",
            "iteration: 248190 loss: 0.0034 lr: 0.02\n",
            "iteration: 248200 loss: 0.0038 lr: 0.02\n",
            "iteration: 248210 loss: 0.0035 lr: 0.02\n",
            "iteration: 248220 loss: 0.0033 lr: 0.02\n",
            "iteration: 248230 loss: 0.0028 lr: 0.02\n",
            "iteration: 248240 loss: 0.0033 lr: 0.02\n",
            "iteration: 248250 loss: 0.0035 lr: 0.02\n",
            "iteration: 248260 loss: 0.0028 lr: 0.02\n",
            "iteration: 248270 loss: 0.0032 lr: 0.02\n",
            "iteration: 248280 loss: 0.0032 lr: 0.02\n",
            "iteration: 248290 loss: 0.0036 lr: 0.02\n",
            "iteration: 248300 loss: 0.0023 lr: 0.02\n",
            "iteration: 248310 loss: 0.0034 lr: 0.02\n",
            "iteration: 248320 loss: 0.0036 lr: 0.02\n",
            "iteration: 248330 loss: 0.0036 lr: 0.02\n",
            "iteration: 248340 loss: 0.0032 lr: 0.02\n",
            "iteration: 248350 loss: 0.0029 lr: 0.02\n",
            "iteration: 248360 loss: 0.0029 lr: 0.02\n",
            "iteration: 248370 loss: 0.0034 lr: 0.02\n",
            "iteration: 248380 loss: 0.0036 lr: 0.02\n",
            "iteration: 248390 loss: 0.0036 lr: 0.02\n",
            "iteration: 248400 loss: 0.0028 lr: 0.02\n",
            "iteration: 248410 loss: 0.0033 lr: 0.02\n",
            "iteration: 248420 loss: 0.0035 lr: 0.02\n",
            "iteration: 248430 loss: 0.0029 lr: 0.02\n",
            "iteration: 248440 loss: 0.0031 lr: 0.02\n",
            "iteration: 248450 loss: 0.0026 lr: 0.02\n",
            "iteration: 248460 loss: 0.0038 lr: 0.02\n",
            "iteration: 248470 loss: 0.0028 lr: 0.02\n",
            "iteration: 248480 loss: 0.0029 lr: 0.02\n",
            "iteration: 248490 loss: 0.0039 lr: 0.02\n",
            "iteration: 248500 loss: 0.0041 lr: 0.02\n",
            "iteration: 248510 loss: 0.0031 lr: 0.02\n",
            "iteration: 248520 loss: 0.0039 lr: 0.02\n",
            "iteration: 248530 loss: 0.0038 lr: 0.02\n",
            "iteration: 248540 loss: 0.0027 lr: 0.02\n",
            "iteration: 248550 loss: 0.0031 lr: 0.02\n",
            "iteration: 248560 loss: 0.0032 lr: 0.02\n",
            "iteration: 248570 loss: 0.0040 lr: 0.02\n",
            "iteration: 248580 loss: 0.0044 lr: 0.02\n",
            "iteration: 248590 loss: 0.0040 lr: 0.02\n",
            "iteration: 248600 loss: 0.0031 lr: 0.02\n",
            "iteration: 248610 loss: 0.0033 lr: 0.02\n",
            "iteration: 248620 loss: 0.0034 lr: 0.02\n",
            "iteration: 248630 loss: 0.0037 lr: 0.02\n",
            "iteration: 248640 loss: 0.0043 lr: 0.02\n",
            "iteration: 248650 loss: 0.0033 lr: 0.02\n",
            "iteration: 248660 loss: 0.0032 lr: 0.02\n",
            "iteration: 248670 loss: 0.0031 lr: 0.02\n",
            "iteration: 248680 loss: 0.0037 lr: 0.02\n",
            "iteration: 248690 loss: 0.0035 lr: 0.02\n",
            "iteration: 248700 loss: 0.0031 lr: 0.02\n",
            "iteration: 248710 loss: 0.0032 lr: 0.02\n",
            "iteration: 248720 loss: 0.0036 lr: 0.02\n",
            "iteration: 248730 loss: 0.0041 lr: 0.02\n",
            "iteration: 248740 loss: 0.0029 lr: 0.02\n",
            "iteration: 248750 loss: 0.0032 lr: 0.02\n",
            "iteration: 248760 loss: 0.0026 lr: 0.02\n",
            "iteration: 248770 loss: 0.0034 lr: 0.02\n",
            "iteration: 248780 loss: 0.0032 lr: 0.02\n",
            "iteration: 248790 loss: 0.0034 lr: 0.02\n",
            "iteration: 248800 loss: 0.0044 lr: 0.02\n",
            "iteration: 248810 loss: 0.0028 lr: 0.02\n",
            "iteration: 248820 loss: 0.0036 lr: 0.02\n",
            "iteration: 248830 loss: 0.0035 lr: 0.02\n",
            "iteration: 248840 loss: 0.0031 lr: 0.02\n",
            "iteration: 248850 loss: 0.0033 lr: 0.02\n",
            "iteration: 248860 loss: 0.0027 lr: 0.02\n",
            "iteration: 248870 loss: 0.0028 lr: 0.02\n",
            "iteration: 248880 loss: 0.0031 lr: 0.02\n",
            "iteration: 248890 loss: 0.0027 lr: 0.02\n",
            "iteration: 248900 loss: 0.0033 lr: 0.02\n",
            "iteration: 248910 loss: 0.0043 lr: 0.02\n",
            "iteration: 248920 loss: 0.0029 lr: 0.02\n",
            "iteration: 248930 loss: 0.0033 lr: 0.02\n",
            "iteration: 248940 loss: 0.0029 lr: 0.02\n",
            "iteration: 248950 loss: 0.0024 lr: 0.02\n",
            "iteration: 248960 loss: 0.0020 lr: 0.02\n",
            "iteration: 248970 loss: 0.0028 lr: 0.02\n",
            "iteration: 248980 loss: 0.0031 lr: 0.02\n",
            "iteration: 248990 loss: 0.0036 lr: 0.02\n",
            "iteration: 249000 loss: 0.0030 lr: 0.02\n",
            "iteration: 249010 loss: 0.0038 lr: 0.02\n",
            "iteration: 249020 loss: 0.0030 lr: 0.02\n",
            "iteration: 249030 loss: 0.0025 lr: 0.02\n",
            "iteration: 249040 loss: 0.0032 lr: 0.02\n",
            "iteration: 249050 loss: 0.0037 lr: 0.02\n",
            "iteration: 249060 loss: 0.0028 lr: 0.02\n",
            "iteration: 249070 loss: 0.0032 lr: 0.02\n",
            "iteration: 249080 loss: 0.0043 lr: 0.02\n",
            "iteration: 249090 loss: 0.0037 lr: 0.02\n",
            "iteration: 249100 loss: 0.0037 lr: 0.02\n",
            "iteration: 249110 loss: 0.0029 lr: 0.02\n",
            "iteration: 249120 loss: 0.0041 lr: 0.02\n",
            "iteration: 249130 loss: 0.0038 lr: 0.02\n",
            "iteration: 249140 loss: 0.0029 lr: 0.02\n",
            "iteration: 249150 loss: 0.0030 lr: 0.02\n",
            "iteration: 249160 loss: 0.0028 lr: 0.02\n",
            "iteration: 249170 loss: 0.0035 lr: 0.02\n",
            "iteration: 249180 loss: 0.0031 lr: 0.02\n",
            "iteration: 249190 loss: 0.0027 lr: 0.02\n",
            "iteration: 249200 loss: 0.0030 lr: 0.02\n",
            "iteration: 249210 loss: 0.0030 lr: 0.02\n",
            "iteration: 249220 loss: 0.0031 lr: 0.02\n",
            "iteration: 249230 loss: 0.0032 lr: 0.02\n",
            "iteration: 249240 loss: 0.0033 lr: 0.02\n",
            "iteration: 249250 loss: 0.0031 lr: 0.02\n",
            "iteration: 249260 loss: 0.0043 lr: 0.02\n",
            "iteration: 249270 loss: 0.0048 lr: 0.02\n",
            "iteration: 249280 loss: 0.0038 lr: 0.02\n",
            "iteration: 249290 loss: 0.0029 lr: 0.02\n",
            "iteration: 249300 loss: 0.0036 lr: 0.02\n",
            "iteration: 249310 loss: 0.0042 lr: 0.02\n",
            "iteration: 249320 loss: 0.0026 lr: 0.02\n",
            "iteration: 249330 loss: 0.0042 lr: 0.02\n",
            "iteration: 249340 loss: 0.0030 lr: 0.02\n",
            "iteration: 249350 loss: 0.0043 lr: 0.02\n",
            "iteration: 249360 loss: 0.0038 lr: 0.02\n",
            "iteration: 249370 loss: 0.0026 lr: 0.02\n",
            "iteration: 249380 loss: 0.0034 lr: 0.02\n",
            "iteration: 249390 loss: 0.0040 lr: 0.02\n",
            "iteration: 249400 loss: 0.0037 lr: 0.02\n",
            "iteration: 249410 loss: 0.0036 lr: 0.02\n",
            "iteration: 249420 loss: 0.0026 lr: 0.02\n",
            "iteration: 249430 loss: 0.0034 lr: 0.02\n",
            "iteration: 249440 loss: 0.0030 lr: 0.02\n",
            "iteration: 249450 loss: 0.0028 lr: 0.02\n",
            "iteration: 249460 loss: 0.0029 lr: 0.02\n",
            "iteration: 249470 loss: 0.0035 lr: 0.02\n",
            "iteration: 249480 loss: 0.0033 lr: 0.02\n",
            "iteration: 249490 loss: 0.0025 lr: 0.02\n",
            "iteration: 249500 loss: 0.0032 lr: 0.02\n",
            "iteration: 249510 loss: 0.0032 lr: 0.02\n",
            "iteration: 249520 loss: 0.0027 lr: 0.02\n",
            "iteration: 249530 loss: 0.0026 lr: 0.02\n",
            "iteration: 249540 loss: 0.0029 lr: 0.02\n",
            "iteration: 249550 loss: 0.0026 lr: 0.02\n",
            "iteration: 249560 loss: 0.0029 lr: 0.02\n",
            "iteration: 249570 loss: 0.0024 lr: 0.02\n",
            "iteration: 249580 loss: 0.0027 lr: 0.02\n",
            "iteration: 249590 loss: 0.0039 lr: 0.02\n",
            "iteration: 249600 loss: 0.0027 lr: 0.02\n",
            "iteration: 249610 loss: 0.0028 lr: 0.02\n",
            "iteration: 249620 loss: 0.0035 lr: 0.02\n",
            "iteration: 249630 loss: 0.0027 lr: 0.02\n",
            "iteration: 249640 loss: 0.0035 lr: 0.02\n",
            "iteration: 249650 loss: 0.0035 lr: 0.02\n",
            "iteration: 249660 loss: 0.0027 lr: 0.02\n",
            "iteration: 249670 loss: 0.0031 lr: 0.02\n",
            "iteration: 249680 loss: 0.0033 lr: 0.02\n",
            "iteration: 249690 loss: 0.0035 lr: 0.02\n",
            "iteration: 249700 loss: 0.0026 lr: 0.02\n",
            "iteration: 249710 loss: 0.0029 lr: 0.02\n",
            "iteration: 249720 loss: 0.0033 lr: 0.02\n",
            "iteration: 249730 loss: 0.0028 lr: 0.02\n",
            "iteration: 249740 loss: 0.0032 lr: 0.02\n",
            "iteration: 249750 loss: 0.0046 lr: 0.02\n",
            "iteration: 249760 loss: 0.0034 lr: 0.02\n",
            "iteration: 249770 loss: 0.0039 lr: 0.02\n",
            "iteration: 249780 loss: 0.0023 lr: 0.02\n",
            "iteration: 249790 loss: 0.0040 lr: 0.02\n",
            "iteration: 249800 loss: 0.0043 lr: 0.02\n",
            "iteration: 249810 loss: 0.0044 lr: 0.02\n",
            "iteration: 249820 loss: 0.0032 lr: 0.02\n",
            "iteration: 249830 loss: 0.0032 lr: 0.02\n",
            "iteration: 249840 loss: 0.0030 lr: 0.02\n",
            "iteration: 249850 loss: 0.0042 lr: 0.02\n",
            "iteration: 249860 loss: 0.0038 lr: 0.02\n",
            "iteration: 249870 loss: 0.0028 lr: 0.02\n",
            "iteration: 249880 loss: 0.0032 lr: 0.02\n",
            "iteration: 249890 loss: 0.0034 lr: 0.02\n",
            "iteration: 249900 loss: 0.0034 lr: 0.02\n",
            "iteration: 249910 loss: 0.0036 lr: 0.02\n",
            "iteration: 249920 loss: 0.0032 lr: 0.02\n",
            "iteration: 249930 loss: 0.0033 lr: 0.02\n",
            "iteration: 249940 loss: 0.0038 lr: 0.02\n",
            "iteration: 249950 loss: 0.0034 lr: 0.02\n",
            "iteration: 249960 loss: 0.0026 lr: 0.02\n",
            "iteration: 249970 loss: 0.0032 lr: 0.02\n",
            "iteration: 249980 loss: 0.0028 lr: 0.02\n",
            "iteration: 249990 loss: 0.0031 lr: 0.02\n",
            "iteration: 250000 loss: 0.0029 lr: 0.02\n",
            "iteration: 250010 loss: 0.0035 lr: 0.02\n",
            "iteration: 250020 loss: 0.0025 lr: 0.02\n",
            "iteration: 250030 loss: 0.0040 lr: 0.02\n",
            "iteration: 250040 loss: 0.0045 lr: 0.02\n",
            "iteration: 250050 loss: 0.0029 lr: 0.02\n",
            "iteration: 250060 loss: 0.0031 lr: 0.02\n",
            "iteration: 250070 loss: 0.0040 lr: 0.02\n",
            "iteration: 250080 loss: 0.0027 lr: 0.02\n",
            "iteration: 250090 loss: 0.0036 lr: 0.02\n",
            "iteration: 250100 loss: 0.0039 lr: 0.02\n",
            "iteration: 250110 loss: 0.0030 lr: 0.02\n",
            "iteration: 250120 loss: 0.0036 lr: 0.02\n",
            "iteration: 250130 loss: 0.0031 lr: 0.02\n",
            "iteration: 250140 loss: 0.0029 lr: 0.02\n",
            "iteration: 250150 loss: 0.0036 lr: 0.02\n",
            "iteration: 250160 loss: 0.0026 lr: 0.02\n",
            "iteration: 250170 loss: 0.0035 lr: 0.02\n",
            "iteration: 250180 loss: 0.0030 lr: 0.02\n",
            "iteration: 250190 loss: 0.0032 lr: 0.02\n",
            "iteration: 250200 loss: 0.0030 lr: 0.02\n",
            "iteration: 250210 loss: 0.0033 lr: 0.02\n",
            "iteration: 250220 loss: 0.0025 lr: 0.02\n",
            "iteration: 250230 loss: 0.0032 lr: 0.02\n",
            "iteration: 250240 loss: 0.0034 lr: 0.02\n",
            "iteration: 250250 loss: 0.0036 lr: 0.02\n",
            "iteration: 250260 loss: 0.0026 lr: 0.02\n",
            "iteration: 250270 loss: 0.0032 lr: 0.02\n",
            "iteration: 250280 loss: 0.0031 lr: 0.02\n",
            "iteration: 250290 loss: 0.0034 lr: 0.02\n",
            "iteration: 250300 loss: 0.0035 lr: 0.02\n",
            "iteration: 250310 loss: 0.0042 lr: 0.02\n",
            "iteration: 250320 loss: 0.0039 lr: 0.02\n",
            "iteration: 250330 loss: 0.0033 lr: 0.02\n",
            "iteration: 250340 loss: 0.0031 lr: 0.02\n",
            "iteration: 250350 loss: 0.0039 lr: 0.02\n",
            "iteration: 250360 loss: 0.0029 lr: 0.02\n",
            "iteration: 250370 loss: 0.0034 lr: 0.02\n",
            "iteration: 250380 loss: 0.0031 lr: 0.02\n",
            "iteration: 250390 loss: 0.0033 lr: 0.02\n",
            "iteration: 250400 loss: 0.0035 lr: 0.02\n",
            "iteration: 250410 loss: 0.0026 lr: 0.02\n",
            "iteration: 250420 loss: 0.0028 lr: 0.02\n",
            "iteration: 250430 loss: 0.0035 lr: 0.02\n",
            "iteration: 250440 loss: 0.0024 lr: 0.02\n",
            "iteration: 250450 loss: 0.0034 lr: 0.02\n",
            "iteration: 250460 loss: 0.0033 lr: 0.02\n",
            "iteration: 250470 loss: 0.0033 lr: 0.02\n",
            "iteration: 250480 loss: 0.0027 lr: 0.02\n",
            "iteration: 250490 loss: 0.0034 lr: 0.02\n",
            "iteration: 250500 loss: 0.0033 lr: 0.02\n",
            "iteration: 250510 loss: 0.0037 lr: 0.02\n",
            "iteration: 250520 loss: 0.0027 lr: 0.02\n",
            "iteration: 250530 loss: 0.0023 lr: 0.02\n",
            "iteration: 250540 loss: 0.0027 lr: 0.02\n",
            "iteration: 250550 loss: 0.0034 lr: 0.02\n",
            "iteration: 250560 loss: 0.0033 lr: 0.02\n",
            "iteration: 250570 loss: 0.0031 lr: 0.02\n",
            "iteration: 250580 loss: 0.0034 lr: 0.02\n",
            "iteration: 250590 loss: 0.0036 lr: 0.02\n",
            "iteration: 250600 loss: 0.0026 lr: 0.02\n",
            "iteration: 250610 loss: 0.0027 lr: 0.02\n",
            "iteration: 250620 loss: 0.0041 lr: 0.02\n",
            "iteration: 250630 loss: 0.0042 lr: 0.02\n",
            "iteration: 250640 loss: 0.0035 lr: 0.02\n",
            "iteration: 250650 loss: 0.0045 lr: 0.02\n",
            "iteration: 250660 loss: 0.0035 lr: 0.02\n",
            "iteration: 250670 loss: 0.0041 lr: 0.02\n",
            "iteration: 250680 loss: 0.0036 lr: 0.02\n",
            "iteration: 250690 loss: 0.0035 lr: 0.02\n",
            "iteration: 250700 loss: 0.0031 lr: 0.02\n",
            "iteration: 250710 loss: 0.0025 lr: 0.02\n",
            "iteration: 250720 loss: 0.0038 lr: 0.02\n",
            "iteration: 250730 loss: 0.0039 lr: 0.02\n",
            "iteration: 250740 loss: 0.0031 lr: 0.02\n",
            "iteration: 250750 loss: 0.0037 lr: 0.02\n",
            "iteration: 250760 loss: 0.0028 lr: 0.02\n",
            "iteration: 250770 loss: 0.0034 lr: 0.02\n",
            "iteration: 250780 loss: 0.0026 lr: 0.02\n",
            "iteration: 250790 loss: 0.0034 lr: 0.02\n",
            "iteration: 250800 loss: 0.0033 lr: 0.02\n",
            "iteration: 250810 loss: 0.0037 lr: 0.02\n",
            "iteration: 250820 loss: 0.0034 lr: 0.02\n",
            "iteration: 250830 loss: 0.0038 lr: 0.02\n",
            "iteration: 250840 loss: 0.0034 lr: 0.02\n",
            "iteration: 250850 loss: 0.0028 lr: 0.02\n",
            "iteration: 250860 loss: 0.0028 lr: 0.02\n",
            "iteration: 250870 loss: 0.0032 lr: 0.02\n",
            "iteration: 250880 loss: 0.0033 lr: 0.02\n",
            "iteration: 250890 loss: 0.0028 lr: 0.02\n",
            "iteration: 250900 loss: 0.0030 lr: 0.02\n",
            "iteration: 250910 loss: 0.0039 lr: 0.02\n",
            "iteration: 250920 loss: 0.0029 lr: 0.02\n",
            "iteration: 250930 loss: 0.0030 lr: 0.02\n",
            "iteration: 250940 loss: 0.0031 lr: 0.02\n",
            "iteration: 250950 loss: 0.0036 lr: 0.02\n",
            "iteration: 250960 loss: 0.0025 lr: 0.02\n",
            "iteration: 250970 loss: 0.0023 lr: 0.02\n",
            "iteration: 250980 loss: 0.0028 lr: 0.02\n",
            "iteration: 250990 loss: 0.0032 lr: 0.02\n",
            "iteration: 251000 loss: 0.0036 lr: 0.02\n",
            "iteration: 251010 loss: 0.0037 lr: 0.02\n",
            "iteration: 251020 loss: 0.0043 lr: 0.02\n",
            "iteration: 251030 loss: 0.0037 lr: 0.02\n",
            "iteration: 251040 loss: 0.0038 lr: 0.02\n",
            "iteration: 251050 loss: 0.0040 lr: 0.02\n",
            "iteration: 251060 loss: 0.0034 lr: 0.02\n",
            "iteration: 251070 loss: 0.0031 lr: 0.02\n",
            "iteration: 251080 loss: 0.0029 lr: 0.02\n",
            "iteration: 251090 loss: 0.0030 lr: 0.02\n",
            "iteration: 251100 loss: 0.0028 lr: 0.02\n",
            "iteration: 251110 loss: 0.0029 lr: 0.02\n",
            "iteration: 251120 loss: 0.0038 lr: 0.02\n",
            "iteration: 251130 loss: 0.0029 lr: 0.02\n",
            "iteration: 251140 loss: 0.0027 lr: 0.02\n",
            "iteration: 251150 loss: 0.0035 lr: 0.02\n",
            "iteration: 251160 loss: 0.0033 lr: 0.02\n",
            "iteration: 251170 loss: 0.0035 lr: 0.02\n",
            "iteration: 251180 loss: 0.0035 lr: 0.02\n",
            "iteration: 251190 loss: 0.0033 lr: 0.02\n",
            "iteration: 251200 loss: 0.0034 lr: 0.02\n",
            "iteration: 251210 loss: 0.0035 lr: 0.02\n",
            "iteration: 251220 loss: 0.0032 lr: 0.02\n",
            "iteration: 251230 loss: 0.0034 lr: 0.02\n",
            "iteration: 251240 loss: 0.0039 lr: 0.02\n",
            "iteration: 251250 loss: 0.0030 lr: 0.02\n",
            "iteration: 251260 loss: 0.0030 lr: 0.02\n",
            "iteration: 251270 loss: 0.0035 lr: 0.02\n",
            "iteration: 251280 loss: 0.0039 lr: 0.02\n",
            "iteration: 251290 loss: 0.0046 lr: 0.02\n",
            "iteration: 251300 loss: 0.0032 lr: 0.02\n",
            "iteration: 251310 loss: 0.0025 lr: 0.02\n",
            "iteration: 251320 loss: 0.0032 lr: 0.02\n",
            "iteration: 251330 loss: 0.0041 lr: 0.02\n",
            "iteration: 251340 loss: 0.0030 lr: 0.02\n",
            "iteration: 251350 loss: 0.0044 lr: 0.02\n",
            "iteration: 251360 loss: 0.0033 lr: 0.02\n",
            "iteration: 251370 loss: 0.0040 lr: 0.02\n",
            "iteration: 251380 loss: 0.0035 lr: 0.02\n",
            "iteration: 251390 loss: 0.0031 lr: 0.02\n",
            "iteration: 251400 loss: 0.0035 lr: 0.02\n",
            "iteration: 251410 loss: 0.0030 lr: 0.02\n",
            "iteration: 251420 loss: 0.0030 lr: 0.02\n",
            "iteration: 251430 loss: 0.0033 lr: 0.02\n",
            "iteration: 251440 loss: 0.0024 lr: 0.02\n",
            "iteration: 251450 loss: 0.0030 lr: 0.02\n",
            "iteration: 251460 loss: 0.0031 lr: 0.02\n",
            "iteration: 251470 loss: 0.0033 lr: 0.02\n",
            "iteration: 251480 loss: 0.0040 lr: 0.02\n",
            "iteration: 251490 loss: 0.0023 lr: 0.02\n",
            "iteration: 251500 loss: 0.0031 lr: 0.02\n",
            "iteration: 251510 loss: 0.0032 lr: 0.02\n",
            "iteration: 251520 loss: 0.0038 lr: 0.02\n",
            "iteration: 251530 loss: 0.0032 lr: 0.02\n",
            "iteration: 251540 loss: 0.0029 lr: 0.02\n",
            "iteration: 251550 loss: 0.0033 lr: 0.02\n",
            "iteration: 251560 loss: 0.0033 lr: 0.02\n",
            "iteration: 251570 loss: 0.0034 lr: 0.02\n",
            "iteration: 251580 loss: 0.0037 lr: 0.02\n",
            "iteration: 251590 loss: 0.0032 lr: 0.02\n",
            "iteration: 251600 loss: 0.0031 lr: 0.02\n",
            "iteration: 251610 loss: 0.0038 lr: 0.02\n",
            "iteration: 251620 loss: 0.0037 lr: 0.02\n",
            "iteration: 251630 loss: 0.0049 lr: 0.02\n",
            "iteration: 251640 loss: 0.0037 lr: 0.02\n",
            "iteration: 251650 loss: 0.0041 lr: 0.02\n",
            "iteration: 251660 loss: 0.0032 lr: 0.02\n",
            "iteration: 251670 loss: 0.0034 lr: 0.02\n",
            "iteration: 251680 loss: 0.0033 lr: 0.02\n",
            "iteration: 251690 loss: 0.0037 lr: 0.02\n",
            "iteration: 251700 loss: 0.0026 lr: 0.02\n",
            "iteration: 251710 loss: 0.0026 lr: 0.02\n",
            "iteration: 251720 loss: 0.0028 lr: 0.02\n",
            "iteration: 251730 loss: 0.0034 lr: 0.02\n",
            "iteration: 251740 loss: 0.0025 lr: 0.02\n",
            "iteration: 251750 loss: 0.0026 lr: 0.02\n",
            "iteration: 251760 loss: 0.0029 lr: 0.02\n",
            "iteration: 251770 loss: 0.0038 lr: 0.02\n",
            "iteration: 251780 loss: 0.0035 lr: 0.02\n",
            "iteration: 251790 loss: 0.0028 lr: 0.02\n",
            "iteration: 251800 loss: 0.0037 lr: 0.02\n",
            "iteration: 251810 loss: 0.0037 lr: 0.02\n",
            "iteration: 251820 loss: 0.0028 lr: 0.02\n",
            "iteration: 251830 loss: 0.0040 lr: 0.02\n",
            "iteration: 251840 loss: 0.0035 lr: 0.02\n",
            "iteration: 251850 loss: 0.0043 lr: 0.02\n",
            "iteration: 251860 loss: 0.0047 lr: 0.02\n",
            "iteration: 251870 loss: 0.0035 lr: 0.02\n",
            "iteration: 251880 loss: 0.0032 lr: 0.02\n",
            "iteration: 251890 loss: 0.0029 lr: 0.02\n",
            "iteration: 251900 loss: 0.0037 lr: 0.02\n",
            "iteration: 251910 loss: 0.0041 lr: 0.02\n",
            "iteration: 251920 loss: 0.0023 lr: 0.02\n",
            "iteration: 251930 loss: 0.0044 lr: 0.02\n",
            "iteration: 251940 loss: 0.0030 lr: 0.02\n",
            "iteration: 251950 loss: 0.0041 lr: 0.02\n",
            "iteration: 251960 loss: 0.0027 lr: 0.02\n",
            "iteration: 251970 loss: 0.0039 lr: 0.02\n",
            "iteration: 251980 loss: 0.0030 lr: 0.02\n",
            "iteration: 251990 loss: 0.0035 lr: 0.02\n",
            "iteration: 252000 loss: 0.0028 lr: 0.02\n",
            "iteration: 252010 loss: 0.0034 lr: 0.02\n",
            "iteration: 252020 loss: 0.0050 lr: 0.02\n",
            "iteration: 252030 loss: 0.0033 lr: 0.02\n",
            "iteration: 252040 loss: 0.0035 lr: 0.02\n",
            "iteration: 252050 loss: 0.0045 lr: 0.02\n",
            "iteration: 252060 loss: 0.0034 lr: 0.02\n",
            "iteration: 252070 loss: 0.0033 lr: 0.02\n",
            "iteration: 252080 loss: 0.0029 lr: 0.02\n",
            "iteration: 252090 loss: 0.0029 lr: 0.02\n",
            "iteration: 252100 loss: 0.0039 lr: 0.02\n",
            "iteration: 252110 loss: 0.0022 lr: 0.02\n",
            "iteration: 252120 loss: 0.0045 lr: 0.02\n",
            "iteration: 252130 loss: 0.0037 lr: 0.02\n",
            "iteration: 252140 loss: 0.0035 lr: 0.02\n",
            "iteration: 252150 loss: 0.0037 lr: 0.02\n",
            "iteration: 252160 loss: 0.0037 lr: 0.02\n",
            "iteration: 252170 loss: 0.0034 lr: 0.02\n",
            "iteration: 252180 loss: 0.0032 lr: 0.02\n",
            "iteration: 252190 loss: 0.0033 lr: 0.02\n",
            "iteration: 252200 loss: 0.0033 lr: 0.02\n",
            "iteration: 252210 loss: 0.0029 lr: 0.02\n",
            "iteration: 252220 loss: 0.0032 lr: 0.02\n",
            "iteration: 252230 loss: 0.0028 lr: 0.02\n",
            "iteration: 252240 loss: 0.0033 lr: 0.02\n",
            "iteration: 252250 loss: 0.0034 lr: 0.02\n",
            "iteration: 252260 loss: 0.0024 lr: 0.02\n",
            "iteration: 252270 loss: 0.0031 lr: 0.02\n",
            "iteration: 252280 loss: 0.0034 lr: 0.02\n",
            "iteration: 252290 loss: 0.0041 lr: 0.02\n",
            "iteration: 252300 loss: 0.0030 lr: 0.02\n",
            "iteration: 252310 loss: 0.0031 lr: 0.02\n",
            "iteration: 252320 loss: 0.0035 lr: 0.02\n",
            "iteration: 252330 loss: 0.0040 lr: 0.02\n",
            "iteration: 252340 loss: 0.0031 lr: 0.02\n",
            "iteration: 252350 loss: 0.0028 lr: 0.02\n",
            "iteration: 252360 loss: 0.0029 lr: 0.02\n",
            "iteration: 252370 loss: 0.0041 lr: 0.02\n",
            "iteration: 252380 loss: 0.0035 lr: 0.02\n",
            "iteration: 252390 loss: 0.0033 lr: 0.02\n",
            "iteration: 252400 loss: 0.0023 lr: 0.02\n",
            "iteration: 252410 loss: 0.0030 lr: 0.02\n",
            "iteration: 252420 loss: 0.0036 lr: 0.02\n",
            "iteration: 252430 loss: 0.0049 lr: 0.02\n",
            "iteration: 252440 loss: 0.0031 lr: 0.02\n",
            "iteration: 252450 loss: 0.0037 lr: 0.02\n",
            "iteration: 252460 loss: 0.0034 lr: 0.02\n",
            "iteration: 252470 loss: 0.0036 lr: 0.02\n",
            "iteration: 252480 loss: 0.0038 lr: 0.02\n",
            "iteration: 252490 loss: 0.0034 lr: 0.02\n",
            "iteration: 252500 loss: 0.0037 lr: 0.02\n",
            "iteration: 252510 loss: 0.0027 lr: 0.02\n",
            "iteration: 252520 loss: 0.0039 lr: 0.02\n",
            "iteration: 252530 loss: 0.0035 lr: 0.02\n",
            "iteration: 252540 loss: 0.0028 lr: 0.02\n",
            "iteration: 252550 loss: 0.0035 lr: 0.02\n",
            "iteration: 252560 loss: 0.0032 lr: 0.02\n",
            "iteration: 252570 loss: 0.0031 lr: 0.02\n",
            "iteration: 252580 loss: 0.0031 lr: 0.02\n",
            "iteration: 252590 loss: 0.0035 lr: 0.02\n",
            "iteration: 252600 loss: 0.0042 lr: 0.02\n",
            "iteration: 252610 loss: 0.0032 lr: 0.02\n",
            "iteration: 252620 loss: 0.0031 lr: 0.02\n",
            "iteration: 252630 loss: 0.0042 lr: 0.02\n",
            "iteration: 252640 loss: 0.0032 lr: 0.02\n",
            "iteration: 252650 loss: 0.0032 lr: 0.02\n",
            "iteration: 252660 loss: 0.0039 lr: 0.02\n",
            "iteration: 252670 loss: 0.0038 lr: 0.02\n",
            "iteration: 252680 loss: 0.0033 lr: 0.02\n",
            "iteration: 252690 loss: 0.0032 lr: 0.02\n",
            "iteration: 252700 loss: 0.0031 lr: 0.02\n",
            "iteration: 252710 loss: 0.0037 lr: 0.02\n",
            "iteration: 252720 loss: 0.0032 lr: 0.02\n",
            "iteration: 252730 loss: 0.0030 lr: 0.02\n",
            "iteration: 252740 loss: 0.0038 lr: 0.02\n",
            "iteration: 252750 loss: 0.0030 lr: 0.02\n",
            "iteration: 252760 loss: 0.0030 lr: 0.02\n",
            "iteration: 252770 loss: 0.0039 lr: 0.02\n",
            "iteration: 252780 loss: 0.0033 lr: 0.02\n",
            "iteration: 252790 loss: 0.0031 lr: 0.02\n",
            "iteration: 252800 loss: 0.0031 lr: 0.02\n",
            "iteration: 252810 loss: 0.0035 lr: 0.02\n",
            "iteration: 252820 loss: 0.0035 lr: 0.02\n",
            "iteration: 252830 loss: 0.0043 lr: 0.02\n",
            "iteration: 252840 loss: 0.0023 lr: 0.02\n",
            "iteration: 252850 loss: 0.0032 lr: 0.02\n",
            "iteration: 252860 loss: 0.0028 lr: 0.02\n",
            "iteration: 252870 loss: 0.0034 lr: 0.02\n",
            "iteration: 252880 loss: 0.0035 lr: 0.02\n",
            "iteration: 252890 loss: 0.0041 lr: 0.02\n",
            "iteration: 252900 loss: 0.0034 lr: 0.02\n",
            "iteration: 252910 loss: 0.0031 lr: 0.02\n",
            "iteration: 252920 loss: 0.0025 lr: 0.02\n",
            "iteration: 252930 loss: 0.0037 lr: 0.02\n",
            "iteration: 252940 loss: 0.0029 lr: 0.02\n",
            "iteration: 252950 loss: 0.0025 lr: 0.02\n",
            "iteration: 252960 loss: 0.0038 lr: 0.02\n",
            "iteration: 252970 loss: 0.0041 lr: 0.02\n",
            "iteration: 252980 loss: 0.0040 lr: 0.02\n",
            "iteration: 252990 loss: 0.0035 lr: 0.02\n",
            "iteration: 253000 loss: 0.0027 lr: 0.02\n",
            "iteration: 253010 loss: 0.0025 lr: 0.02\n",
            "iteration: 253020 loss: 0.0036 lr: 0.02\n",
            "iteration: 253030 loss: 0.0030 lr: 0.02\n",
            "iteration: 253040 loss: 0.0035 lr: 0.02\n",
            "iteration: 253050 loss: 0.0036 lr: 0.02\n",
            "iteration: 253060 loss: 0.0035 lr: 0.02\n",
            "iteration: 253070 loss: 0.0030 lr: 0.02\n",
            "iteration: 253080 loss: 0.0028 lr: 0.02\n",
            "iteration: 253090 loss: 0.0032 lr: 0.02\n",
            "iteration: 253100 loss: 0.0033 lr: 0.02\n",
            "iteration: 253110 loss: 0.0036 lr: 0.02\n",
            "iteration: 253120 loss: 0.0041 lr: 0.02\n",
            "iteration: 253130 loss: 0.0027 lr: 0.02\n",
            "iteration: 253140 loss: 0.0040 lr: 0.02\n",
            "iteration: 253150 loss: 0.0025 lr: 0.02\n",
            "iteration: 253160 loss: 0.0035 lr: 0.02\n",
            "iteration: 253170 loss: 0.0040 lr: 0.02\n",
            "iteration: 253180 loss: 0.0026 lr: 0.02\n",
            "iteration: 253190 loss: 0.0035 lr: 0.02\n",
            "iteration: 253200 loss: 0.0034 lr: 0.02\n",
            "iteration: 253210 loss: 0.0028 lr: 0.02\n",
            "iteration: 253220 loss: 0.0033 lr: 0.02\n",
            "iteration: 253230 loss: 0.0043 lr: 0.02\n",
            "iteration: 253240 loss: 0.0023 lr: 0.02\n",
            "iteration: 253250 loss: 0.0030 lr: 0.02\n",
            "iteration: 253260 loss: 0.0032 lr: 0.02\n",
            "iteration: 253270 loss: 0.0022 lr: 0.02\n",
            "iteration: 253280 loss: 0.0040 lr: 0.02\n",
            "iteration: 253290 loss: 0.0043 lr: 0.02\n",
            "iteration: 253300 loss: 0.0030 lr: 0.02\n",
            "iteration: 253310 loss: 0.0029 lr: 0.02\n",
            "iteration: 253320 loss: 0.0028 lr: 0.02\n",
            "iteration: 253330 loss: 0.0038 lr: 0.02\n",
            "iteration: 253340 loss: 0.0035 lr: 0.02\n",
            "iteration: 253350 loss: 0.0039 lr: 0.02\n",
            "iteration: 253360 loss: 0.0033 lr: 0.02\n",
            "iteration: 253370 loss: 0.0036 lr: 0.02\n",
            "iteration: 253380 loss: 0.0041 lr: 0.02\n",
            "iteration: 253390 loss: 0.0028 lr: 0.02\n",
            "iteration: 253400 loss: 0.0034 lr: 0.02\n",
            "iteration: 253410 loss: 0.0033 lr: 0.02\n",
            "iteration: 253420 loss: 0.0033 lr: 0.02\n",
            "iteration: 253430 loss: 0.0035 lr: 0.02\n",
            "iteration: 253440 loss: 0.0029 lr: 0.02\n",
            "iteration: 253450 loss: 0.0034 lr: 0.02\n",
            "iteration: 253460 loss: 0.0028 lr: 0.02\n",
            "iteration: 253470 loss: 0.0033 lr: 0.02\n",
            "iteration: 253480 loss: 0.0041 lr: 0.02\n",
            "iteration: 253490 loss: 0.0033 lr: 0.02\n",
            "iteration: 253500 loss: 0.0025 lr: 0.02\n",
            "iteration: 253510 loss: 0.0036 lr: 0.02\n",
            "iteration: 253520 loss: 0.0025 lr: 0.02\n",
            "iteration: 253530 loss: 0.0031 lr: 0.02\n",
            "iteration: 253540 loss: 0.0040 lr: 0.02\n",
            "iteration: 253550 loss: 0.0031 lr: 0.02\n",
            "iteration: 253560 loss: 0.0033 lr: 0.02\n",
            "iteration: 253570 loss: 0.0033 lr: 0.02\n",
            "iteration: 253580 loss: 0.0037 lr: 0.02\n",
            "iteration: 253590 loss: 0.0032 lr: 0.02\n",
            "iteration: 253600 loss: 0.0027 lr: 0.02\n",
            "iteration: 253610 loss: 0.0042 lr: 0.02\n",
            "iteration: 253620 loss: 0.0033 lr: 0.02\n",
            "iteration: 253630 loss: 0.0026 lr: 0.02\n",
            "iteration: 253640 loss: 0.0041 lr: 0.02\n",
            "iteration: 253650 loss: 0.0033 lr: 0.02\n",
            "iteration: 253660 loss: 0.0029 lr: 0.02\n",
            "iteration: 253670 loss: 0.0039 lr: 0.02\n",
            "iteration: 253680 loss: 0.0039 lr: 0.02\n",
            "iteration: 253690 loss: 0.0033 lr: 0.02\n",
            "iteration: 253700 loss: 0.0036 lr: 0.02\n",
            "iteration: 253710 loss: 0.0024 lr: 0.02\n",
            "iteration: 253720 loss: 0.0041 lr: 0.02\n",
            "iteration: 253730 loss: 0.0037 lr: 0.02\n",
            "iteration: 253740 loss: 0.0031 lr: 0.02\n",
            "iteration: 253750 loss: 0.0029 lr: 0.02\n",
            "iteration: 253760 loss: 0.0027 lr: 0.02\n",
            "iteration: 253770 loss: 0.0031 lr: 0.02\n",
            "iteration: 253780 loss: 0.0040 lr: 0.02\n",
            "iteration: 253790 loss: 0.0037 lr: 0.02\n",
            "iteration: 253800 loss: 0.0033 lr: 0.02\n",
            "iteration: 253810 loss: 0.0032 lr: 0.02\n",
            "iteration: 253820 loss: 0.0034 lr: 0.02\n",
            "iteration: 253830 loss: 0.0023 lr: 0.02\n",
            "iteration: 253840 loss: 0.0029 lr: 0.02\n",
            "iteration: 253850 loss: 0.0036 lr: 0.02\n",
            "iteration: 253860 loss: 0.0039 lr: 0.02\n",
            "iteration: 253870 loss: 0.0034 lr: 0.02\n",
            "iteration: 253880 loss: 0.0034 lr: 0.02\n",
            "iteration: 253890 loss: 0.0028 lr: 0.02\n",
            "iteration: 253900 loss: 0.0033 lr: 0.02\n",
            "iteration: 253910 loss: 0.0041 lr: 0.02\n",
            "iteration: 253920 loss: 0.0042 lr: 0.02\n",
            "iteration: 253930 loss: 0.0033 lr: 0.02\n",
            "iteration: 253940 loss: 0.0032 lr: 0.02\n",
            "iteration: 253950 loss: 0.0032 lr: 0.02\n",
            "iteration: 253960 loss: 0.0032 lr: 0.02\n",
            "iteration: 253970 loss: 0.0037 lr: 0.02\n",
            "iteration: 253980 loss: 0.0027 lr: 0.02\n",
            "iteration: 253990 loss: 0.0035 lr: 0.02\n",
            "iteration: 254000 loss: 0.0046 lr: 0.02\n",
            "iteration: 254010 loss: 0.0039 lr: 0.02\n",
            "iteration: 254020 loss: 0.0029 lr: 0.02\n",
            "iteration: 254030 loss: 0.0027 lr: 0.02\n",
            "iteration: 254040 loss: 0.0050 lr: 0.02\n",
            "iteration: 254050 loss: 0.0028 lr: 0.02\n",
            "iteration: 254060 loss: 0.0030 lr: 0.02\n",
            "iteration: 254070 loss: 0.0030 lr: 0.02\n",
            "iteration: 254080 loss: 0.0028 lr: 0.02\n",
            "iteration: 254090 loss: 0.0031 lr: 0.02\n",
            "iteration: 254100 loss: 0.0029 lr: 0.02\n",
            "iteration: 254110 loss: 0.0038 lr: 0.02\n",
            "iteration: 254120 loss: 0.0028 lr: 0.02\n",
            "iteration: 254130 loss: 0.0031 lr: 0.02\n",
            "iteration: 254140 loss: 0.0033 lr: 0.02\n",
            "iteration: 254150 loss: 0.0035 lr: 0.02\n",
            "iteration: 254160 loss: 0.0030 lr: 0.02\n",
            "iteration: 254170 loss: 0.0035 lr: 0.02\n",
            "iteration: 254180 loss: 0.0033 lr: 0.02\n",
            "iteration: 254190 loss: 0.0030 lr: 0.02\n",
            "iteration: 254200 loss: 0.0032 lr: 0.02\n",
            "iteration: 254210 loss: 0.0027 lr: 0.02\n",
            "iteration: 254220 loss: 0.0032 lr: 0.02\n",
            "iteration: 254230 loss: 0.0029 lr: 0.02\n",
            "iteration: 254240 loss: 0.0029 lr: 0.02\n",
            "iteration: 254250 loss: 0.0032 lr: 0.02\n",
            "iteration: 254260 loss: 0.0032 lr: 0.02\n",
            "iteration: 254270 loss: 0.0036 lr: 0.02\n",
            "iteration: 254280 loss: 0.0036 lr: 0.02\n",
            "iteration: 254290 loss: 0.0037 lr: 0.02\n",
            "iteration: 254300 loss: 0.0030 lr: 0.02\n",
            "iteration: 254310 loss: 0.0038 lr: 0.02\n",
            "iteration: 254320 loss: 0.0029 lr: 0.02\n",
            "iteration: 254330 loss: 0.0041 lr: 0.02\n",
            "iteration: 254340 loss: 0.0035 lr: 0.02\n",
            "iteration: 254350 loss: 0.0027 lr: 0.02\n",
            "iteration: 254360 loss: 0.0034 lr: 0.02\n",
            "iteration: 254370 loss: 0.0029 lr: 0.02\n",
            "iteration: 254380 loss: 0.0029 lr: 0.02\n",
            "iteration: 254390 loss: 0.0031 lr: 0.02\n",
            "iteration: 254400 loss: 0.0044 lr: 0.02\n",
            "iteration: 254410 loss: 0.0032 lr: 0.02\n",
            "iteration: 254420 loss: 0.0035 lr: 0.02\n",
            "iteration: 254430 loss: 0.0032 lr: 0.02\n",
            "iteration: 254440 loss: 0.0036 lr: 0.02\n",
            "iteration: 254450 loss: 0.0041 lr: 0.02\n",
            "iteration: 254460 loss: 0.0032 lr: 0.02\n",
            "iteration: 254470 loss: 0.0034 lr: 0.02\n",
            "iteration: 254480 loss: 0.0028 lr: 0.02\n",
            "iteration: 254490 loss: 0.0030 lr: 0.02\n",
            "iteration: 254500 loss: 0.0031 lr: 0.02\n",
            "iteration: 254510 loss: 0.0034 lr: 0.02\n",
            "iteration: 254520 loss: 0.0043 lr: 0.02\n",
            "iteration: 254530 loss: 0.0031 lr: 0.02\n",
            "iteration: 254540 loss: 0.0035 lr: 0.02\n",
            "iteration: 254550 loss: 0.0033 lr: 0.02\n",
            "iteration: 254560 loss: 0.0035 lr: 0.02\n",
            "iteration: 254570 loss: 0.0029 lr: 0.02\n",
            "iteration: 254580 loss: 0.0028 lr: 0.02\n",
            "iteration: 254590 loss: 0.0030 lr: 0.02\n",
            "iteration: 254600 loss: 0.0032 lr: 0.02\n",
            "iteration: 254610 loss: 0.0033 lr: 0.02\n",
            "iteration: 254620 loss: 0.0028 lr: 0.02\n",
            "iteration: 254630 loss: 0.0028 lr: 0.02\n",
            "iteration: 254640 loss: 0.0032 lr: 0.02\n",
            "iteration: 254650 loss: 0.0028 lr: 0.02\n",
            "iteration: 254660 loss: 0.0037 lr: 0.02\n",
            "iteration: 254670 loss: 0.0028 lr: 0.02\n",
            "iteration: 254680 loss: 0.0027 lr: 0.02\n",
            "iteration: 254690 loss: 0.0022 lr: 0.02\n",
            "iteration: 254700 loss: 0.0031 lr: 0.02\n",
            "iteration: 254710 loss: 0.0036 lr: 0.02\n",
            "iteration: 254720 loss: 0.0023 lr: 0.02\n",
            "iteration: 254730 loss: 0.0026 lr: 0.02\n",
            "iteration: 254740 loss: 0.0030 lr: 0.02\n",
            "iteration: 254750 loss: 0.0034 lr: 0.02\n",
            "iteration: 254760 loss: 0.0028 lr: 0.02\n",
            "iteration: 254770 loss: 0.0039 lr: 0.02\n",
            "iteration: 254780 loss: 0.0033 lr: 0.02\n",
            "iteration: 254790 loss: 0.0036 lr: 0.02\n",
            "iteration: 254800 loss: 0.0033 lr: 0.02\n",
            "iteration: 254810 loss: 0.0032 lr: 0.02\n",
            "iteration: 254820 loss: 0.0036 lr: 0.02\n",
            "iteration: 254830 loss: 0.0042 lr: 0.02\n",
            "iteration: 254840 loss: 0.0029 lr: 0.02\n",
            "iteration: 254850 loss: 0.0037 lr: 0.02\n",
            "iteration: 254860 loss: 0.0034 lr: 0.02\n",
            "iteration: 254870 loss: 0.0034 lr: 0.02\n",
            "iteration: 254880 loss: 0.0035 lr: 0.02\n",
            "iteration: 254890 loss: 0.0030 lr: 0.02\n",
            "iteration: 254900 loss: 0.0028 lr: 0.02\n",
            "iteration: 254910 loss: 0.0046 lr: 0.02\n",
            "iteration: 254920 loss: 0.0036 lr: 0.02\n",
            "iteration: 254930 loss: 0.0044 lr: 0.02\n",
            "iteration: 254940 loss: 0.0032 lr: 0.02\n",
            "iteration: 254950 loss: 0.0035 lr: 0.02\n",
            "iteration: 254960 loss: 0.0036 lr: 0.02\n",
            "iteration: 254970 loss: 0.0031 lr: 0.02\n",
            "iteration: 254980 loss: 0.0036 lr: 0.02\n",
            "iteration: 254990 loss: 0.0038 lr: 0.02\n",
            "iteration: 255000 loss: 0.0035 lr: 0.02\n",
            "iteration: 255010 loss: 0.0036 lr: 0.02\n",
            "iteration: 255020 loss: 0.0035 lr: 0.02\n",
            "iteration: 255030 loss: 0.0036 lr: 0.02\n",
            "iteration: 255040 loss: 0.0040 lr: 0.02\n",
            "iteration: 255050 loss: 0.0034 lr: 0.02\n",
            "iteration: 255060 loss: 0.0031 lr: 0.02\n",
            "iteration: 255070 loss: 0.0030 lr: 0.02\n",
            "iteration: 255080 loss: 0.0041 lr: 0.02\n",
            "iteration: 255090 loss: 0.0035 lr: 0.02\n",
            "iteration: 255100 loss: 0.0030 lr: 0.02\n",
            "iteration: 255110 loss: 0.0030 lr: 0.02\n",
            "iteration: 255120 loss: 0.0026 lr: 0.02\n",
            "iteration: 255130 loss: 0.0029 lr: 0.02\n",
            "iteration: 255140 loss: 0.0037 lr: 0.02\n",
            "iteration: 255150 loss: 0.0027 lr: 0.02\n",
            "iteration: 255160 loss: 0.0031 lr: 0.02\n",
            "iteration: 255170 loss: 0.0043 lr: 0.02\n",
            "iteration: 255180 loss: 0.0031 lr: 0.02\n",
            "iteration: 255190 loss: 0.0032 lr: 0.02\n",
            "iteration: 255200 loss: 0.0032 lr: 0.02\n",
            "iteration: 255210 loss: 0.0039 lr: 0.02\n",
            "iteration: 255220 loss: 0.0029 lr: 0.02\n",
            "iteration: 255230 loss: 0.0028 lr: 0.02\n",
            "iteration: 255240 loss: 0.0030 lr: 0.02\n",
            "iteration: 255250 loss: 0.0041 lr: 0.02\n",
            "iteration: 255260 loss: 0.0030 lr: 0.02\n",
            "iteration: 255270 loss: 0.0028 lr: 0.02\n",
            "iteration: 255280 loss: 0.0027 lr: 0.02\n",
            "iteration: 255290 loss: 0.0040 lr: 0.02\n",
            "iteration: 255300 loss: 0.0033 lr: 0.02\n",
            "iteration: 255310 loss: 0.0036 lr: 0.02\n",
            "iteration: 255320 loss: 0.0028 lr: 0.02\n",
            "iteration: 255330 loss: 0.0026 lr: 0.02\n",
            "iteration: 255340 loss: 0.0034 lr: 0.02\n",
            "iteration: 255350 loss: 0.0035 lr: 0.02\n",
            "iteration: 255360 loss: 0.0032 lr: 0.02\n",
            "iteration: 255370 loss: 0.0036 lr: 0.02\n",
            "iteration: 255380 loss: 0.0027 lr: 0.02\n",
            "iteration: 255390 loss: 0.0023 lr: 0.02\n",
            "iteration: 255400 loss: 0.0030 lr: 0.02\n",
            "iteration: 255410 loss: 0.0045 lr: 0.02\n",
            "iteration: 255420 loss: 0.0029 lr: 0.02\n",
            "iteration: 255430 loss: 0.0034 lr: 0.02\n",
            "iteration: 255440 loss: 0.0028 lr: 0.02\n",
            "iteration: 255450 loss: 0.0030 lr: 0.02\n",
            "iteration: 255460 loss: 0.0033 lr: 0.02\n",
            "iteration: 255470 loss: 0.0031 lr: 0.02\n",
            "iteration: 255480 loss: 0.0030 lr: 0.02\n",
            "iteration: 255490 loss: 0.0026 lr: 0.02\n",
            "iteration: 255500 loss: 0.0035 lr: 0.02\n",
            "iteration: 255510 loss: 0.0024 lr: 0.02\n",
            "iteration: 255520 loss: 0.0032 lr: 0.02\n",
            "iteration: 255530 loss: 0.0031 lr: 0.02\n",
            "iteration: 255540 loss: 0.0028 lr: 0.02\n",
            "iteration: 255550 loss: 0.0039 lr: 0.02\n",
            "iteration: 255560 loss: 0.0037 lr: 0.02\n",
            "iteration: 255570 loss: 0.0031 lr: 0.02\n",
            "iteration: 255580 loss: 0.0029 lr: 0.02\n",
            "iteration: 255590 loss: 0.0036 lr: 0.02\n",
            "iteration: 255600 loss: 0.0038 lr: 0.02\n",
            "iteration: 255610 loss: 0.0039 lr: 0.02\n",
            "iteration: 255620 loss: 0.0022 lr: 0.02\n",
            "iteration: 255630 loss: 0.0034 lr: 0.02\n",
            "iteration: 255640 loss: 0.0033 lr: 0.02\n",
            "iteration: 255650 loss: 0.0028 lr: 0.02\n",
            "iteration: 255660 loss: 0.0028 lr: 0.02\n",
            "iteration: 255670 loss: 0.0024 lr: 0.02\n",
            "iteration: 255680 loss: 0.0043 lr: 0.02\n",
            "iteration: 255690 loss: 0.0027 lr: 0.02\n",
            "iteration: 255700 loss: 0.0026 lr: 0.02\n",
            "iteration: 255710 loss: 0.0039 lr: 0.02\n",
            "iteration: 255720 loss: 0.0039 lr: 0.02\n",
            "iteration: 255730 loss: 0.0036 lr: 0.02\n",
            "iteration: 255740 loss: 0.0038 lr: 0.02\n",
            "iteration: 255750 loss: 0.0032 lr: 0.02\n",
            "iteration: 255760 loss: 0.0038 lr: 0.02\n",
            "iteration: 255770 loss: 0.0040 lr: 0.02\n",
            "iteration: 255780 loss: 0.0035 lr: 0.02\n",
            "iteration: 255790 loss: 0.0031 lr: 0.02\n",
            "iteration: 255800 loss: 0.0034 lr: 0.02\n",
            "iteration: 255810 loss: 0.0033 lr: 0.02\n",
            "iteration: 255820 loss: 0.0034 lr: 0.02\n",
            "iteration: 255830 loss: 0.0027 lr: 0.02\n",
            "iteration: 255840 loss: 0.0038 lr: 0.02\n",
            "iteration: 255850 loss: 0.0034 lr: 0.02\n",
            "iteration: 255860 loss: 0.0023 lr: 0.02\n",
            "iteration: 255870 loss: 0.0031 lr: 0.02\n",
            "iteration: 255880 loss: 0.0025 lr: 0.02\n",
            "iteration: 255890 loss: 0.0028 lr: 0.02\n",
            "iteration: 255900 loss: 0.0024 lr: 0.02\n",
            "iteration: 255910 loss: 0.0029 lr: 0.02\n",
            "iteration: 255920 loss: 0.0033 lr: 0.02\n",
            "iteration: 255930 loss: 0.0034 lr: 0.02\n",
            "iteration: 255940 loss: 0.0031 lr: 0.02\n",
            "iteration: 255950 loss: 0.0032 lr: 0.02\n",
            "iteration: 255960 loss: 0.0035 lr: 0.02\n",
            "iteration: 255970 loss: 0.0037 lr: 0.02\n",
            "iteration: 255980 loss: 0.0031 lr: 0.02\n",
            "iteration: 255990 loss: 0.0034 lr: 0.02\n",
            "iteration: 256000 loss: 0.0036 lr: 0.02\n",
            "iteration: 256010 loss: 0.0024 lr: 0.02\n",
            "iteration: 256020 loss: 0.0026 lr: 0.02\n",
            "iteration: 256030 loss: 0.0032 lr: 0.02\n",
            "iteration: 256040 loss: 0.0035 lr: 0.02\n",
            "iteration: 256050 loss: 0.0033 lr: 0.02\n",
            "iteration: 256060 loss: 0.0043 lr: 0.02\n",
            "iteration: 256070 loss: 0.0034 lr: 0.02\n",
            "iteration: 256080 loss: 0.0030 lr: 0.02\n",
            "iteration: 256090 loss: 0.0026 lr: 0.02\n",
            "iteration: 256100 loss: 0.0029 lr: 0.02\n",
            "iteration: 256110 loss: 0.0041 lr: 0.02\n",
            "iteration: 256120 loss: 0.0028 lr: 0.02\n",
            "iteration: 256130 loss: 0.0030 lr: 0.02\n",
            "iteration: 256140 loss: 0.0045 lr: 0.02\n",
            "iteration: 256150 loss: 0.0033 lr: 0.02\n",
            "iteration: 256160 loss: 0.0041 lr: 0.02\n",
            "iteration: 256170 loss: 0.0027 lr: 0.02\n",
            "iteration: 256180 loss: 0.0030 lr: 0.02\n",
            "iteration: 256190 loss: 0.0039 lr: 0.02\n",
            "iteration: 256200 loss: 0.0033 lr: 0.02\n",
            "iteration: 256210 loss: 0.0030 lr: 0.02\n",
            "iteration: 256220 loss: 0.0039 lr: 0.02\n",
            "iteration: 256230 loss: 0.0037 lr: 0.02\n",
            "iteration: 256240 loss: 0.0027 lr: 0.02\n",
            "iteration: 256250 loss: 0.0031 lr: 0.02\n",
            "iteration: 256260 loss: 0.0033 lr: 0.02\n",
            "iteration: 256270 loss: 0.0034 lr: 0.02\n",
            "iteration: 256280 loss: 0.0032 lr: 0.02\n",
            "iteration: 256290 loss: 0.0028 lr: 0.02\n",
            "iteration: 256300 loss: 0.0038 lr: 0.02\n",
            "iteration: 256310 loss: 0.0041 lr: 0.02\n",
            "iteration: 256320 loss: 0.0026 lr: 0.02\n",
            "iteration: 256330 loss: 0.0029 lr: 0.02\n",
            "iteration: 256340 loss: 0.0027 lr: 0.02\n",
            "iteration: 256350 loss: 0.0029 lr: 0.02\n",
            "iteration: 256360 loss: 0.0037 lr: 0.02\n",
            "iteration: 256370 loss: 0.0031 lr: 0.02\n",
            "iteration: 256380 loss: 0.0036 lr: 0.02\n",
            "iteration: 256390 loss: 0.0036 lr: 0.02\n",
            "iteration: 256400 loss: 0.0029 lr: 0.02\n",
            "iteration: 256410 loss: 0.0044 lr: 0.02\n",
            "iteration: 256420 loss: 0.0037 lr: 0.02\n",
            "iteration: 256430 loss: 0.0036 lr: 0.02\n",
            "iteration: 256440 loss: 0.0032 lr: 0.02\n",
            "iteration: 256450 loss: 0.0030 lr: 0.02\n",
            "iteration: 256460 loss: 0.0035 lr: 0.02\n",
            "iteration: 256470 loss: 0.0029 lr: 0.02\n",
            "iteration: 256480 loss: 0.0032 lr: 0.02\n",
            "iteration: 256490 loss: 0.0031 lr: 0.02\n",
            "iteration: 256500 loss: 0.0042 lr: 0.02\n",
            "iteration: 256510 loss: 0.0036 lr: 0.02\n",
            "iteration: 256520 loss: 0.0035 lr: 0.02\n",
            "iteration: 256530 loss: 0.0030 lr: 0.02\n",
            "iteration: 256540 loss: 0.0031 lr: 0.02\n",
            "iteration: 256550 loss: 0.0026 lr: 0.02\n",
            "iteration: 256560 loss: 0.0031 lr: 0.02\n",
            "iteration: 256570 loss: 0.0038 lr: 0.02\n",
            "iteration: 256580 loss: 0.0040 lr: 0.02\n",
            "iteration: 256590 loss: 0.0035 lr: 0.02\n",
            "iteration: 256600 loss: 0.0032 lr: 0.02\n",
            "iteration: 256610 loss: 0.0042 lr: 0.02\n",
            "iteration: 256620 loss: 0.0034 lr: 0.02\n",
            "iteration: 256630 loss: 0.0037 lr: 0.02\n",
            "iteration: 256640 loss: 0.0030 lr: 0.02\n",
            "iteration: 256650 loss: 0.0024 lr: 0.02\n",
            "iteration: 256660 loss: 0.0034 lr: 0.02\n",
            "iteration: 256670 loss: 0.0032 lr: 0.02\n",
            "iteration: 256680 loss: 0.0029 lr: 0.02\n",
            "iteration: 256690 loss: 0.0028 lr: 0.02\n",
            "iteration: 256700 loss: 0.0030 lr: 0.02\n",
            "iteration: 256710 loss: 0.0033 lr: 0.02\n",
            "iteration: 256720 loss: 0.0036 lr: 0.02\n",
            "iteration: 256730 loss: 0.0039 lr: 0.02\n",
            "iteration: 256740 loss: 0.0042 lr: 0.02\n",
            "iteration: 256750 loss: 0.0029 lr: 0.02\n",
            "iteration: 256760 loss: 0.0038 lr: 0.02\n",
            "iteration: 256770 loss: 0.0036 lr: 0.02\n",
            "iteration: 256780 loss: 0.0038 lr: 0.02\n",
            "iteration: 256790 loss: 0.0031 lr: 0.02\n",
            "iteration: 256800 loss: 0.0038 lr: 0.02\n",
            "iteration: 256810 loss: 0.0042 lr: 0.02\n",
            "iteration: 256820 loss: 0.0027 lr: 0.02\n",
            "iteration: 256830 loss: 0.0030 lr: 0.02\n",
            "iteration: 256840 loss: 0.0032 lr: 0.02\n",
            "iteration: 256850 loss: 0.0030 lr: 0.02\n",
            "iteration: 256860 loss: 0.0038 lr: 0.02\n",
            "iteration: 256870 loss: 0.0040 lr: 0.02\n",
            "iteration: 256880 loss: 0.0036 lr: 0.02\n",
            "iteration: 256890 loss: 0.0035 lr: 0.02\n",
            "iteration: 256900 loss: 0.0036 lr: 0.02\n",
            "iteration: 256910 loss: 0.0030 lr: 0.02\n",
            "iteration: 256920 loss: 0.0036 lr: 0.02\n",
            "iteration: 256930 loss: 0.0031 lr: 0.02\n",
            "iteration: 256940 loss: 0.0031 lr: 0.02\n",
            "iteration: 256950 loss: 0.0034 lr: 0.02\n",
            "iteration: 256960 loss: 0.0040 lr: 0.02\n",
            "iteration: 256970 loss: 0.0037 lr: 0.02\n",
            "iteration: 256980 loss: 0.0031 lr: 0.02\n",
            "iteration: 256990 loss: 0.0036 lr: 0.02\n",
            "iteration: 257000 loss: 0.0040 lr: 0.02\n",
            "iteration: 257010 loss: 0.0032 lr: 0.02\n",
            "iteration: 257020 loss: 0.0030 lr: 0.02\n",
            "iteration: 257030 loss: 0.0036 lr: 0.02\n",
            "iteration: 257040 loss: 0.0028 lr: 0.02\n",
            "iteration: 257050 loss: 0.0035 lr: 0.02\n",
            "iteration: 257060 loss: 0.0032 lr: 0.02\n",
            "iteration: 257070 loss: 0.0028 lr: 0.02\n",
            "iteration: 257080 loss: 0.0036 lr: 0.02\n",
            "iteration: 257090 loss: 0.0031 lr: 0.02\n",
            "iteration: 257100 loss: 0.0035 lr: 0.02\n",
            "iteration: 257110 loss: 0.0029 lr: 0.02\n",
            "iteration: 257120 loss: 0.0031 lr: 0.02\n",
            "iteration: 257130 loss: 0.0029 lr: 0.02\n",
            "iteration: 257140 loss: 0.0037 lr: 0.02\n",
            "iteration: 257150 loss: 0.0036 lr: 0.02\n",
            "iteration: 257160 loss: 0.0025 lr: 0.02\n",
            "iteration: 257170 loss: 0.0023 lr: 0.02\n",
            "iteration: 257180 loss: 0.0042 lr: 0.02\n",
            "iteration: 257190 loss: 0.0033 lr: 0.02\n",
            "iteration: 257200 loss: 0.0031 lr: 0.02\n",
            "iteration: 257210 loss: 0.0032 lr: 0.02\n",
            "iteration: 257220 loss: 0.0032 lr: 0.02\n",
            "iteration: 257230 loss: 0.0030 lr: 0.02\n",
            "iteration: 257240 loss: 0.0031 lr: 0.02\n",
            "iteration: 257250 loss: 0.0039 lr: 0.02\n",
            "iteration: 257260 loss: 0.0030 lr: 0.02\n",
            "iteration: 257270 loss: 0.0032 lr: 0.02\n",
            "iteration: 257280 loss: 0.0040 lr: 0.02\n",
            "iteration: 257290 loss: 0.0031 lr: 0.02\n",
            "iteration: 257300 loss: 0.0041 lr: 0.02\n",
            "iteration: 257310 loss: 0.0041 lr: 0.02\n",
            "iteration: 257320 loss: 0.0032 lr: 0.02\n",
            "iteration: 257330 loss: 0.0027 lr: 0.02\n",
            "iteration: 257340 loss: 0.0029 lr: 0.02\n",
            "iteration: 257350 loss: 0.0030 lr: 0.02\n",
            "iteration: 257360 loss: 0.0031 lr: 0.02\n",
            "iteration: 257370 loss: 0.0028 lr: 0.02\n",
            "iteration: 257380 loss: 0.0033 lr: 0.02\n",
            "iteration: 257390 loss: 0.0033 lr: 0.02\n",
            "iteration: 257400 loss: 0.0028 lr: 0.02\n",
            "iteration: 257410 loss: 0.0028 lr: 0.02\n",
            "iteration: 257420 loss: 0.0032 lr: 0.02\n",
            "iteration: 257430 loss: 0.0035 lr: 0.02\n",
            "iteration: 257440 loss: 0.0028 lr: 0.02\n",
            "iteration: 257450 loss: 0.0033 lr: 0.02\n",
            "iteration: 257460 loss: 0.0028 lr: 0.02\n",
            "iteration: 257470 loss: 0.0041 lr: 0.02\n",
            "iteration: 257480 loss: 0.0053 lr: 0.02\n",
            "iteration: 257490 loss: 0.0034 lr: 0.02\n",
            "iteration: 257500 loss: 0.0032 lr: 0.02\n",
            "iteration: 257510 loss: 0.0035 lr: 0.02\n",
            "iteration: 257520 loss: 0.0029 lr: 0.02\n",
            "iteration: 257530 loss: 0.0029 lr: 0.02\n",
            "iteration: 257540 loss: 0.0033 lr: 0.02\n",
            "iteration: 257550 loss: 0.0039 lr: 0.02\n",
            "iteration: 257560 loss: 0.0032 lr: 0.02\n",
            "iteration: 257570 loss: 0.0029 lr: 0.02\n",
            "iteration: 257580 loss: 0.0032 lr: 0.02\n",
            "iteration: 257590 loss: 0.0029 lr: 0.02\n",
            "iteration: 257600 loss: 0.0033 lr: 0.02\n",
            "iteration: 257610 loss: 0.0028 lr: 0.02\n",
            "iteration: 257620 loss: 0.0033 lr: 0.02\n",
            "iteration: 257630 loss: 0.0036 lr: 0.02\n",
            "iteration: 257640 loss: 0.0029 lr: 0.02\n",
            "iteration: 257650 loss: 0.0031 lr: 0.02\n",
            "iteration: 257660 loss: 0.0036 lr: 0.02\n",
            "iteration: 257670 loss: 0.0047 lr: 0.02\n",
            "iteration: 257680 loss: 0.0029 lr: 0.02\n",
            "iteration: 257690 loss: 0.0041 lr: 0.02\n",
            "iteration: 257700 loss: 0.0033 lr: 0.02\n",
            "iteration: 257710 loss: 0.0032 lr: 0.02\n",
            "iteration: 257720 loss: 0.0035 lr: 0.02\n",
            "iteration: 257730 loss: 0.0034 lr: 0.02\n",
            "iteration: 257740 loss: 0.0027 lr: 0.02\n",
            "iteration: 257750 loss: 0.0035 lr: 0.02\n",
            "iteration: 257760 loss: 0.0027 lr: 0.02\n",
            "iteration: 257770 loss: 0.0027 lr: 0.02\n",
            "iteration: 257780 loss: 0.0028 lr: 0.02\n",
            "iteration: 257790 loss: 0.0033 lr: 0.02\n",
            "iteration: 257800 loss: 0.0031 lr: 0.02\n",
            "iteration: 257810 loss: 0.0033 lr: 0.02\n",
            "iteration: 257820 loss: 0.0030 lr: 0.02\n",
            "iteration: 257830 loss: 0.0029 lr: 0.02\n",
            "iteration: 257840 loss: 0.0047 lr: 0.02\n",
            "iteration: 257850 loss: 0.0027 lr: 0.02\n",
            "iteration: 257860 loss: 0.0027 lr: 0.02\n",
            "iteration: 257870 loss: 0.0043 lr: 0.02\n",
            "iteration: 257880 loss: 0.0044 lr: 0.02\n",
            "iteration: 257890 loss: 0.0033 lr: 0.02\n",
            "iteration: 257900 loss: 0.0036 lr: 0.02\n",
            "iteration: 257910 loss: 0.0032 lr: 0.02\n",
            "iteration: 257920 loss: 0.0027 lr: 0.02\n",
            "iteration: 257930 loss: 0.0032 lr: 0.02\n",
            "iteration: 257940 loss: 0.0028 lr: 0.02\n",
            "iteration: 257950 loss: 0.0032 lr: 0.02\n",
            "iteration: 257960 loss: 0.0023 lr: 0.02\n",
            "iteration: 257970 loss: 0.0039 lr: 0.02\n",
            "iteration: 257980 loss: 0.0030 lr: 0.02\n",
            "iteration: 257990 loss: 0.0035 lr: 0.02\n",
            "iteration: 258000 loss: 0.0039 lr: 0.02\n",
            "iteration: 258010 loss: 0.0033 lr: 0.02\n",
            "iteration: 258020 loss: 0.0033 lr: 0.02\n",
            "iteration: 258030 loss: 0.0038 lr: 0.02\n",
            "iteration: 258040 loss: 0.0041 lr: 0.02\n",
            "iteration: 258050 loss: 0.0028 lr: 0.02\n",
            "iteration: 258060 loss: 0.0033 lr: 0.02\n",
            "iteration: 258070 loss: 0.0029 lr: 0.02\n",
            "iteration: 258080 loss: 0.0030 lr: 0.02\n",
            "iteration: 258090 loss: 0.0040 lr: 0.02\n",
            "iteration: 258100 loss: 0.0038 lr: 0.02\n",
            "iteration: 258110 loss: 0.0031 lr: 0.02\n",
            "iteration: 258120 loss: 0.0034 lr: 0.02\n",
            "iteration: 258130 loss: 0.0037 lr: 0.02\n",
            "iteration: 258140 loss: 0.0025 lr: 0.02\n",
            "iteration: 258150 loss: 0.0030 lr: 0.02\n",
            "iteration: 258160 loss: 0.0030 lr: 0.02\n",
            "iteration: 258170 loss: 0.0028 lr: 0.02\n",
            "iteration: 258180 loss: 0.0030 lr: 0.02\n",
            "iteration: 258190 loss: 0.0039 lr: 0.02\n",
            "iteration: 258200 loss: 0.0030 lr: 0.02\n",
            "iteration: 258210 loss: 0.0034 lr: 0.02\n",
            "iteration: 258220 loss: 0.0028 lr: 0.02\n",
            "iteration: 258230 loss: 0.0035 lr: 0.02\n",
            "iteration: 258240 loss: 0.0034 lr: 0.02\n",
            "iteration: 258250 loss: 0.0031 lr: 0.02\n",
            "iteration: 258260 loss: 0.0037 lr: 0.02\n",
            "iteration: 258270 loss: 0.0029 lr: 0.02\n",
            "iteration: 258280 loss: 0.0034 lr: 0.02\n",
            "iteration: 258290 loss: 0.0038 lr: 0.02\n",
            "iteration: 258300 loss: 0.0037 lr: 0.02\n",
            "iteration: 258310 loss: 0.0030 lr: 0.02\n",
            "iteration: 258320 loss: 0.0032 lr: 0.02\n",
            "iteration: 258330 loss: 0.0037 lr: 0.02\n",
            "iteration: 258340 loss: 0.0031 lr: 0.02\n",
            "iteration: 258350 loss: 0.0028 lr: 0.02\n",
            "iteration: 258360 loss: 0.0035 lr: 0.02\n",
            "iteration: 258370 loss: 0.0030 lr: 0.02\n",
            "iteration: 258380 loss: 0.0038 lr: 0.02\n",
            "iteration: 258390 loss: 0.0035 lr: 0.02\n",
            "iteration: 258400 loss: 0.0027 lr: 0.02\n",
            "iteration: 258410 loss: 0.0026 lr: 0.02\n",
            "iteration: 258420 loss: 0.0045 lr: 0.02\n",
            "iteration: 258430 loss: 0.0026 lr: 0.02\n",
            "iteration: 258440 loss: 0.0029 lr: 0.02\n",
            "iteration: 258450 loss: 0.0027 lr: 0.02\n",
            "iteration: 258460 loss: 0.0034 lr: 0.02\n",
            "iteration: 258470 loss: 0.0033 lr: 0.02\n",
            "iteration: 258480 loss: 0.0034 lr: 0.02\n",
            "iteration: 258490 loss: 0.0040 lr: 0.02\n",
            "iteration: 258500 loss: 0.0037 lr: 0.02\n",
            "iteration: 258510 loss: 0.0033 lr: 0.02\n",
            "iteration: 258520 loss: 0.0031 lr: 0.02\n",
            "iteration: 258530 loss: 0.0033 lr: 0.02\n",
            "iteration: 258540 loss: 0.0031 lr: 0.02\n",
            "iteration: 258550 loss: 0.0033 lr: 0.02\n",
            "iteration: 258560 loss: 0.0033 lr: 0.02\n",
            "iteration: 258570 loss: 0.0028 lr: 0.02\n",
            "iteration: 258580 loss: 0.0029 lr: 0.02\n",
            "iteration: 258590 loss: 0.0032 lr: 0.02\n",
            "iteration: 258600 loss: 0.0027 lr: 0.02\n",
            "iteration: 258610 loss: 0.0045 lr: 0.02\n",
            "iteration: 258620 loss: 0.0033 lr: 0.02\n",
            "iteration: 258630 loss: 0.0032 lr: 0.02\n",
            "iteration: 258640 loss: 0.0037 lr: 0.02\n",
            "iteration: 258650 loss: 0.0036 lr: 0.02\n",
            "iteration: 258660 loss: 0.0028 lr: 0.02\n",
            "iteration: 258670 loss: 0.0034 lr: 0.02\n",
            "iteration: 258680 loss: 0.0034 lr: 0.02\n",
            "iteration: 258690 loss: 0.0029 lr: 0.02\n",
            "iteration: 258700 loss: 0.0037 lr: 0.02\n",
            "iteration: 258710 loss: 0.0032 lr: 0.02\n",
            "iteration: 258720 loss: 0.0037 lr: 0.02\n",
            "iteration: 258730 loss: 0.0038 lr: 0.02\n",
            "iteration: 258740 loss: 0.0032 lr: 0.02\n",
            "iteration: 258750 loss: 0.0039 lr: 0.02\n",
            "iteration: 258760 loss: 0.0031 lr: 0.02\n",
            "iteration: 258770 loss: 0.0026 lr: 0.02\n",
            "iteration: 258780 loss: 0.0036 lr: 0.02\n",
            "iteration: 258790 loss: 0.0022 lr: 0.02\n",
            "iteration: 258800 loss: 0.0029 lr: 0.02\n",
            "iteration: 258810 loss: 0.0026 lr: 0.02\n",
            "iteration: 258820 loss: 0.0035 lr: 0.02\n",
            "iteration: 258830 loss: 0.0038 lr: 0.02\n",
            "iteration: 258840 loss: 0.0031 lr: 0.02\n",
            "iteration: 258850 loss: 0.0039 lr: 0.02\n",
            "iteration: 258860 loss: 0.0038 lr: 0.02\n",
            "iteration: 258870 loss: 0.0040 lr: 0.02\n",
            "iteration: 258880 loss: 0.0028 lr: 0.02\n",
            "iteration: 258890 loss: 0.0033 lr: 0.02\n",
            "iteration: 258900 loss: 0.0028 lr: 0.02\n",
            "iteration: 258910 loss: 0.0029 lr: 0.02\n",
            "iteration: 258920 loss: 0.0023 lr: 0.02\n",
            "iteration: 258930 loss: 0.0031 lr: 0.02\n",
            "iteration: 258940 loss: 0.0028 lr: 0.02\n",
            "iteration: 258950 loss: 0.0029 lr: 0.02\n",
            "iteration: 258960 loss: 0.0035 lr: 0.02\n",
            "iteration: 258970 loss: 0.0034 lr: 0.02\n",
            "iteration: 258980 loss: 0.0034 lr: 0.02\n",
            "iteration: 258990 loss: 0.0030 lr: 0.02\n",
            "iteration: 259000 loss: 0.0036 lr: 0.02\n",
            "iteration: 259010 loss: 0.0025 lr: 0.02\n",
            "iteration: 259020 loss: 0.0039 lr: 0.02\n",
            "iteration: 259030 loss: 0.0030 lr: 0.02\n",
            "iteration: 259040 loss: 0.0034 lr: 0.02\n",
            "iteration: 259050 loss: 0.0029 lr: 0.02\n",
            "iteration: 259060 loss: 0.0030 lr: 0.02\n",
            "iteration: 259070 loss: 0.0038 lr: 0.02\n",
            "iteration: 259080 loss: 0.0033 lr: 0.02\n",
            "iteration: 259090 loss: 0.0038 lr: 0.02\n",
            "iteration: 259100 loss: 0.0032 lr: 0.02\n",
            "iteration: 259110 loss: 0.0026 lr: 0.02\n",
            "iteration: 259120 loss: 0.0037 lr: 0.02\n",
            "iteration: 259130 loss: 0.0034 lr: 0.02\n",
            "iteration: 259140 loss: 0.0036 lr: 0.02\n",
            "iteration: 259150 loss: 0.0029 lr: 0.02\n",
            "iteration: 259160 loss: 0.0029 lr: 0.02\n",
            "iteration: 259170 loss: 0.0036 lr: 0.02\n",
            "iteration: 259180 loss: 0.0038 lr: 0.02\n",
            "iteration: 259190 loss: 0.0030 lr: 0.02\n",
            "iteration: 259200 loss: 0.0037 lr: 0.02\n",
            "iteration: 259210 loss: 0.0030 lr: 0.02\n",
            "iteration: 259220 loss: 0.0037 lr: 0.02\n",
            "iteration: 259230 loss: 0.0027 lr: 0.02\n",
            "iteration: 259240 loss: 0.0033 lr: 0.02\n",
            "iteration: 259250 loss: 0.0042 lr: 0.02\n",
            "iteration: 259260 loss: 0.0032 lr: 0.02\n",
            "iteration: 259270 loss: 0.0030 lr: 0.02\n",
            "iteration: 259280 loss: 0.0033 lr: 0.02\n",
            "iteration: 259290 loss: 0.0031 lr: 0.02\n",
            "iteration: 259300 loss: 0.0032 lr: 0.02\n",
            "iteration: 259310 loss: 0.0030 lr: 0.02\n",
            "iteration: 259320 loss: 0.0033 lr: 0.02\n",
            "iteration: 259330 loss: 0.0036 lr: 0.02\n",
            "iteration: 259340 loss: 0.0038 lr: 0.02\n",
            "iteration: 259350 loss: 0.0033 lr: 0.02\n",
            "iteration: 259360 loss: 0.0027 lr: 0.02\n",
            "iteration: 259370 loss: 0.0033 lr: 0.02\n",
            "iteration: 259380 loss: 0.0040 lr: 0.02\n",
            "iteration: 259390 loss: 0.0035 lr: 0.02\n",
            "iteration: 259400 loss: 0.0037 lr: 0.02\n",
            "iteration: 259410 loss: 0.0038 lr: 0.02\n",
            "iteration: 259420 loss: 0.0037 lr: 0.02\n",
            "iteration: 259430 loss: 0.0034 lr: 0.02\n",
            "iteration: 259440 loss: 0.0029 lr: 0.02\n",
            "iteration: 259450 loss: 0.0036 lr: 0.02\n",
            "iteration: 259460 loss: 0.0025 lr: 0.02\n",
            "iteration: 259470 loss: 0.0034 lr: 0.02\n",
            "iteration: 259480 loss: 0.0023 lr: 0.02\n",
            "iteration: 259490 loss: 0.0033 lr: 0.02\n",
            "iteration: 259500 loss: 0.0033 lr: 0.02\n",
            "iteration: 259510 loss: 0.0034 lr: 0.02\n",
            "iteration: 259520 loss: 0.0031 lr: 0.02\n",
            "iteration: 259530 loss: 0.0036 lr: 0.02\n",
            "iteration: 259540 loss: 0.0027 lr: 0.02\n",
            "iteration: 259550 loss: 0.0032 lr: 0.02\n",
            "iteration: 259560 loss: 0.0030 lr: 0.02\n",
            "iteration: 259570 loss: 0.0029 lr: 0.02\n",
            "iteration: 259580 loss: 0.0036 lr: 0.02\n",
            "iteration: 259590 loss: 0.0036 lr: 0.02\n",
            "iteration: 259600 loss: 0.0041 lr: 0.02\n",
            "iteration: 259610 loss: 0.0038 lr: 0.02\n",
            "iteration: 259620 loss: 0.0026 lr: 0.02\n",
            "iteration: 259630 loss: 0.0030 lr: 0.02\n",
            "iteration: 259640 loss: 0.0028 lr: 0.02\n",
            "iteration: 259650 loss: 0.0033 lr: 0.02\n",
            "iteration: 259660 loss: 0.0037 lr: 0.02\n",
            "iteration: 259670 loss: 0.0038 lr: 0.02\n",
            "iteration: 259680 loss: 0.0029 lr: 0.02\n",
            "iteration: 259690 loss: 0.0042 lr: 0.02\n",
            "iteration: 259700 loss: 0.0033 lr: 0.02\n",
            "iteration: 259710 loss: 0.0028 lr: 0.02\n",
            "iteration: 259720 loss: 0.0024 lr: 0.02\n",
            "iteration: 259730 loss: 0.0027 lr: 0.02\n",
            "iteration: 259740 loss: 0.0025 lr: 0.02\n",
            "iteration: 259750 loss: 0.0032 lr: 0.02\n",
            "iteration: 259760 loss: 0.0031 lr: 0.02\n",
            "iteration: 259770 loss: 0.0028 lr: 0.02\n",
            "iteration: 259780 loss: 0.0033 lr: 0.02\n",
            "iteration: 259790 loss: 0.0025 lr: 0.02\n",
            "iteration: 259800 loss: 0.0023 lr: 0.02\n",
            "iteration: 259810 loss: 0.0035 lr: 0.02\n",
            "iteration: 259820 loss: 0.0029 lr: 0.02\n",
            "iteration: 259830 loss: 0.0026 lr: 0.02\n",
            "iteration: 259840 loss: 0.0033 lr: 0.02\n",
            "iteration: 259850 loss: 0.0026 lr: 0.02\n",
            "iteration: 259860 loss: 0.0032 lr: 0.02\n",
            "iteration: 259870 loss: 0.0026 lr: 0.02\n",
            "iteration: 259880 loss: 0.0039 lr: 0.02\n",
            "iteration: 259890 loss: 0.0033 lr: 0.02\n",
            "iteration: 259900 loss: 0.0040 lr: 0.02\n",
            "iteration: 259910 loss: 0.0034 lr: 0.02\n",
            "iteration: 259920 loss: 0.0029 lr: 0.02\n",
            "iteration: 259930 loss: 0.0027 lr: 0.02\n",
            "iteration: 259940 loss: 0.0034 lr: 0.02\n",
            "iteration: 259950 loss: 0.0031 lr: 0.02\n",
            "iteration: 259960 loss: 0.0028 lr: 0.02\n",
            "iteration: 259970 loss: 0.0023 lr: 0.02\n",
            "iteration: 259980 loss: 0.0033 lr: 0.02\n",
            "iteration: 259990 loss: 0.0024 lr: 0.02\n",
            "iteration: 260000 loss: 0.0026 lr: 0.02\n",
            "iteration: 260010 loss: 0.0040 lr: 0.02\n",
            "iteration: 260020 loss: 0.0030 lr: 0.02\n",
            "iteration: 260030 loss: 0.0029 lr: 0.02\n",
            "iteration: 260040 loss: 0.0033 lr: 0.02\n",
            "iteration: 260050 loss: 0.0027 lr: 0.02\n",
            "iteration: 260060 loss: 0.0030 lr: 0.02\n",
            "iteration: 260070 loss: 0.0031 lr: 0.02\n",
            "iteration: 260080 loss: 0.0030 lr: 0.02\n",
            "iteration: 260090 loss: 0.0037 lr: 0.02\n",
            "iteration: 260100 loss: 0.0025 lr: 0.02\n",
            "iteration: 260110 loss: 0.0025 lr: 0.02\n",
            "iteration: 260120 loss: 0.0040 lr: 0.02\n",
            "iteration: 260130 loss: 0.0030 lr: 0.02\n",
            "iteration: 260140 loss: 0.0026 lr: 0.02\n",
            "iteration: 260150 loss: 0.0041 lr: 0.02\n",
            "iteration: 260160 loss: 0.0033 lr: 0.02\n",
            "iteration: 260170 loss: 0.0035 lr: 0.02\n",
            "iteration: 260180 loss: 0.0030 lr: 0.02\n",
            "iteration: 260190 loss: 0.0032 lr: 0.02\n",
            "iteration: 260200 loss: 0.0035 lr: 0.02\n",
            "iteration: 260210 loss: 0.0029 lr: 0.02\n",
            "iteration: 260220 loss: 0.0044 lr: 0.02\n",
            "iteration: 260230 loss: 0.0034 lr: 0.02\n",
            "iteration: 260240 loss: 0.0028 lr: 0.02\n",
            "iteration: 260250 loss: 0.0033 lr: 0.02\n",
            "iteration: 260260 loss: 0.0043 lr: 0.02\n",
            "iteration: 260270 loss: 0.0037 lr: 0.02\n",
            "iteration: 260280 loss: 0.0032 lr: 0.02\n",
            "iteration: 260290 loss: 0.0039 lr: 0.02\n",
            "iteration: 260300 loss: 0.0039 lr: 0.02\n",
            "iteration: 260310 loss: 0.0038 lr: 0.02\n",
            "iteration: 260320 loss: 0.0027 lr: 0.02\n",
            "iteration: 260330 loss: 0.0028 lr: 0.02\n",
            "iteration: 260340 loss: 0.0030 lr: 0.02\n",
            "iteration: 260350 loss: 0.0035 lr: 0.02\n",
            "iteration: 260360 loss: 0.0043 lr: 0.02\n",
            "iteration: 260370 loss: 0.0031 lr: 0.02\n",
            "iteration: 260380 loss: 0.0033 lr: 0.02\n",
            "iteration: 260390 loss: 0.0035 lr: 0.02\n",
            "iteration: 260400 loss: 0.0030 lr: 0.02\n",
            "iteration: 260410 loss: 0.0033 lr: 0.02\n",
            "iteration: 260420 loss: 0.0024 lr: 0.02\n",
            "iteration: 260430 loss: 0.0027 lr: 0.02\n",
            "iteration: 260440 loss: 0.0033 lr: 0.02\n",
            "iteration: 260450 loss: 0.0042 lr: 0.02\n",
            "iteration: 260460 loss: 0.0032 lr: 0.02\n",
            "iteration: 260470 loss: 0.0029 lr: 0.02\n",
            "iteration: 260480 loss: 0.0043 lr: 0.02\n",
            "iteration: 260490 loss: 0.0038 lr: 0.02\n",
            "iteration: 260500 loss: 0.0038 lr: 0.02\n",
            "iteration: 260510 loss: 0.0042 lr: 0.02\n",
            "iteration: 260520 loss: 0.0040 lr: 0.02\n",
            "iteration: 260530 loss: 0.0034 lr: 0.02\n",
            "iteration: 260540 loss: 0.0026 lr: 0.02\n",
            "iteration: 260550 loss: 0.0032 lr: 0.02\n",
            "iteration: 260560 loss: 0.0034 lr: 0.02\n",
            "iteration: 260570 loss: 0.0029 lr: 0.02\n",
            "iteration: 260580 loss: 0.0032 lr: 0.02\n",
            "iteration: 260590 loss: 0.0029 lr: 0.02\n",
            "iteration: 260600 loss: 0.0032 lr: 0.02\n",
            "iteration: 260610 loss: 0.0030 lr: 0.02\n",
            "iteration: 260620 loss: 0.0038 lr: 0.02\n",
            "iteration: 260630 loss: 0.0026 lr: 0.02\n",
            "iteration: 260640 loss: 0.0029 lr: 0.02\n",
            "iteration: 260650 loss: 0.0049 lr: 0.02\n",
            "iteration: 260660 loss: 0.0028 lr: 0.02\n",
            "iteration: 260670 loss: 0.0035 lr: 0.02\n",
            "iteration: 260680 loss: 0.0031 lr: 0.02\n",
            "iteration: 260690 loss: 0.0037 lr: 0.02\n",
            "iteration: 260700 loss: 0.0028 lr: 0.02\n",
            "iteration: 260710 loss: 0.0033 lr: 0.02\n",
            "iteration: 260720 loss: 0.0038 lr: 0.02\n",
            "iteration: 260730 loss: 0.0032 lr: 0.02\n",
            "iteration: 260740 loss: 0.0039 lr: 0.02\n",
            "iteration: 260750 loss: 0.0032 lr: 0.02\n",
            "iteration: 260760 loss: 0.0031 lr: 0.02\n",
            "iteration: 260770 loss: 0.0032 lr: 0.02\n",
            "iteration: 260780 loss: 0.0041 lr: 0.02\n",
            "iteration: 260790 loss: 0.0041 lr: 0.02\n",
            "iteration: 260800 loss: 0.0029 lr: 0.02\n",
            "iteration: 260810 loss: 0.0035 lr: 0.02\n",
            "iteration: 260820 loss: 0.0039 lr: 0.02\n",
            "iteration: 260830 loss: 0.0034 lr: 0.02\n",
            "iteration: 260840 loss: 0.0030 lr: 0.02\n",
            "iteration: 260850 loss: 0.0039 lr: 0.02\n",
            "iteration: 260860 loss: 0.0033 lr: 0.02\n",
            "iteration: 260870 loss: 0.0040 lr: 0.02\n",
            "iteration: 260880 loss: 0.0040 lr: 0.02\n",
            "iteration: 260890 loss: 0.0029 lr: 0.02\n",
            "iteration: 260900 loss: 0.0032 lr: 0.02\n",
            "iteration: 260910 loss: 0.0029 lr: 0.02\n",
            "iteration: 260920 loss: 0.0034 lr: 0.02\n",
            "iteration: 260930 loss: 0.0043 lr: 0.02\n",
            "iteration: 260940 loss: 0.0028 lr: 0.02\n",
            "iteration: 260950 loss: 0.0029 lr: 0.02\n",
            "iteration: 260960 loss: 0.0035 lr: 0.02\n",
            "iteration: 260970 loss: 0.0028 lr: 0.02\n",
            "iteration: 260980 loss: 0.0029 lr: 0.02\n",
            "iteration: 260990 loss: 0.0034 lr: 0.02\n",
            "iteration: 261000 loss: 0.0037 lr: 0.02\n",
            "iteration: 261010 loss: 0.0036 lr: 0.02\n",
            "iteration: 261020 loss: 0.0038 lr: 0.02\n",
            "iteration: 261030 loss: 0.0039 lr: 0.02\n",
            "iteration: 261040 loss: 0.0032 lr: 0.02\n",
            "iteration: 261050 loss: 0.0032 lr: 0.02\n",
            "iteration: 261060 loss: 0.0030 lr: 0.02\n",
            "iteration: 261070 loss: 0.0035 lr: 0.02\n",
            "iteration: 261080 loss: 0.0033 lr: 0.02\n",
            "iteration: 261090 loss: 0.0036 lr: 0.02\n",
            "iteration: 261100 loss: 0.0033 lr: 0.02\n",
            "iteration: 261110 loss: 0.0040 lr: 0.02\n",
            "iteration: 261120 loss: 0.0040 lr: 0.02\n",
            "iteration: 261130 loss: 0.0034 lr: 0.02\n",
            "iteration: 261140 loss: 0.0028 lr: 0.02\n",
            "iteration: 261150 loss: 0.0038 lr: 0.02\n",
            "iteration: 261160 loss: 0.0027 lr: 0.02\n",
            "iteration: 261170 loss: 0.0038 lr: 0.02\n",
            "iteration: 261180 loss: 0.0037 lr: 0.02\n",
            "iteration: 261190 loss: 0.0027 lr: 0.02\n",
            "iteration: 261200 loss: 0.0034 lr: 0.02\n",
            "iteration: 261210 loss: 0.0031 lr: 0.02\n",
            "iteration: 261220 loss: 0.0032 lr: 0.02\n",
            "iteration: 261230 loss: 0.0030 lr: 0.02\n",
            "iteration: 261240 loss: 0.0031 lr: 0.02\n",
            "iteration: 261250 loss: 0.0034 lr: 0.02\n",
            "iteration: 261260 loss: 0.0037 lr: 0.02\n",
            "iteration: 261270 loss: 0.0033 lr: 0.02\n",
            "iteration: 261280 loss: 0.0028 lr: 0.02\n",
            "iteration: 261290 loss: 0.0028 lr: 0.02\n",
            "iteration: 261300 loss: 0.0032 lr: 0.02\n",
            "iteration: 261310 loss: 0.0029 lr: 0.02\n",
            "iteration: 261320 loss: 0.0029 lr: 0.02\n",
            "iteration: 261330 loss: 0.0021 lr: 0.02\n",
            "iteration: 261340 loss: 0.0032 lr: 0.02\n",
            "iteration: 261350 loss: 0.0036 lr: 0.02\n",
            "iteration: 261360 loss: 0.0029 lr: 0.02\n",
            "iteration: 261370 loss: 0.0037 lr: 0.02\n",
            "iteration: 261380 loss: 0.0030 lr: 0.02\n",
            "iteration: 261390 loss: 0.0029 lr: 0.02\n",
            "iteration: 261400 loss: 0.0038 lr: 0.02\n",
            "iteration: 261410 loss: 0.0025 lr: 0.02\n",
            "iteration: 261420 loss: 0.0031 lr: 0.02\n",
            "iteration: 261430 loss: 0.0032 lr: 0.02\n",
            "iteration: 261440 loss: 0.0031 lr: 0.02\n",
            "iteration: 261450 loss: 0.0033 lr: 0.02\n",
            "iteration: 261460 loss: 0.0035 lr: 0.02\n",
            "iteration: 261470 loss: 0.0036 lr: 0.02\n",
            "iteration: 261480 loss: 0.0027 lr: 0.02\n",
            "iteration: 261490 loss: 0.0038 lr: 0.02\n",
            "iteration: 261500 loss: 0.0035 lr: 0.02\n",
            "iteration: 261510 loss: 0.0030 lr: 0.02\n",
            "iteration: 261520 loss: 0.0032 lr: 0.02\n",
            "iteration: 261530 loss: 0.0051 lr: 0.02\n",
            "iteration: 261540 loss: 0.0030 lr: 0.02\n",
            "iteration: 261550 loss: 0.0030 lr: 0.02\n",
            "iteration: 261560 loss: 0.0030 lr: 0.02\n",
            "iteration: 261570 loss: 0.0028 lr: 0.02\n",
            "iteration: 261580 loss: 0.0032 lr: 0.02\n",
            "iteration: 261590 loss: 0.0026 lr: 0.02\n",
            "iteration: 261600 loss: 0.0040 lr: 0.02\n",
            "iteration: 261610 loss: 0.0027 lr: 0.02\n",
            "iteration: 261620 loss: 0.0033 lr: 0.02\n",
            "iteration: 261630 loss: 0.0038 lr: 0.02\n",
            "iteration: 261640 loss: 0.0035 lr: 0.02\n",
            "iteration: 261650 loss: 0.0032 lr: 0.02\n",
            "iteration: 261660 loss: 0.0029 lr: 0.02\n",
            "iteration: 261670 loss: 0.0029 lr: 0.02\n",
            "iteration: 261680 loss: 0.0027 lr: 0.02\n",
            "iteration: 261690 loss: 0.0030 lr: 0.02\n",
            "iteration: 261700 loss: 0.0029 lr: 0.02\n",
            "iteration: 261710 loss: 0.0023 lr: 0.02\n",
            "iteration: 261720 loss: 0.0033 lr: 0.02\n",
            "iteration: 261730 loss: 0.0032 lr: 0.02\n",
            "iteration: 261740 loss: 0.0029 lr: 0.02\n",
            "iteration: 261750 loss: 0.0027 lr: 0.02\n",
            "iteration: 261760 loss: 0.0027 lr: 0.02\n",
            "iteration: 261770 loss: 0.0039 lr: 0.02\n",
            "iteration: 261780 loss: 0.0025 lr: 0.02\n",
            "iteration: 261790 loss: 0.0039 lr: 0.02\n",
            "iteration: 261800 loss: 0.0030 lr: 0.02\n",
            "iteration: 261810 loss: 0.0035 lr: 0.02\n",
            "iteration: 261820 loss: 0.0024 lr: 0.02\n",
            "iteration: 261830 loss: 0.0031 lr: 0.02\n",
            "iteration: 261840 loss: 0.0025 lr: 0.02\n",
            "iteration: 261850 loss: 0.0036 lr: 0.02\n",
            "iteration: 261860 loss: 0.0038 lr: 0.02\n",
            "iteration: 261870 loss: 0.0030 lr: 0.02\n",
            "iteration: 261880 loss: 0.0035 lr: 0.02\n",
            "iteration: 261890 loss: 0.0028 lr: 0.02\n",
            "iteration: 261900 loss: 0.0033 lr: 0.02\n",
            "iteration: 261910 loss: 0.0028 lr: 0.02\n",
            "iteration: 261920 loss: 0.0028 lr: 0.02\n",
            "iteration: 261930 loss: 0.0026 lr: 0.02\n",
            "iteration: 261940 loss: 0.0025 lr: 0.02\n",
            "iteration: 261950 loss: 0.0027 lr: 0.02\n",
            "iteration: 261960 loss: 0.0036 lr: 0.02\n",
            "iteration: 261970 loss: 0.0025 lr: 0.02\n",
            "iteration: 261980 loss: 0.0030 lr: 0.02\n",
            "iteration: 261990 loss: 0.0031 lr: 0.02\n",
            "iteration: 262000 loss: 0.0030 lr: 0.02\n",
            "iteration: 262010 loss: 0.0034 lr: 0.02\n",
            "iteration: 262020 loss: 0.0029 lr: 0.02\n",
            "iteration: 262030 loss: 0.0035 lr: 0.02\n",
            "iteration: 262040 loss: 0.0035 lr: 0.02\n",
            "iteration: 262050 loss: 0.0039 lr: 0.02\n",
            "iteration: 262060 loss: 0.0034 lr: 0.02\n",
            "iteration: 262070 loss: 0.0027 lr: 0.02\n",
            "iteration: 262080 loss: 0.0030 lr: 0.02\n",
            "iteration: 262090 loss: 0.0029 lr: 0.02\n",
            "iteration: 262100 loss: 0.0033 lr: 0.02\n",
            "iteration: 262110 loss: 0.0026 lr: 0.02\n",
            "iteration: 262120 loss: 0.0028 lr: 0.02\n",
            "iteration: 262130 loss: 0.0037 lr: 0.02\n",
            "iteration: 262140 loss: 0.0032 lr: 0.02\n",
            "iteration: 262150 loss: 0.0026 lr: 0.02\n",
            "iteration: 262160 loss: 0.0034 lr: 0.02\n",
            "iteration: 262170 loss: 0.0029 lr: 0.02\n",
            "iteration: 262180 loss: 0.0041 lr: 0.02\n",
            "iteration: 262190 loss: 0.0039 lr: 0.02\n",
            "iteration: 262200 loss: 0.0037 lr: 0.02\n",
            "iteration: 262210 loss: 0.0031 lr: 0.02\n",
            "iteration: 262220 loss: 0.0028 lr: 0.02\n",
            "iteration: 262230 loss: 0.0031 lr: 0.02\n",
            "iteration: 262240 loss: 0.0036 lr: 0.02\n",
            "iteration: 262250 loss: 0.0034 lr: 0.02\n",
            "iteration: 262260 loss: 0.0027 lr: 0.02\n",
            "iteration: 262270 loss: 0.0036 lr: 0.02\n",
            "iteration: 262280 loss: 0.0028 lr: 0.02\n",
            "iteration: 262290 loss: 0.0030 lr: 0.02\n",
            "iteration: 262300 loss: 0.0036 lr: 0.02\n",
            "iteration: 262310 loss: 0.0030 lr: 0.02\n",
            "iteration: 262320 loss: 0.0025 lr: 0.02\n",
            "iteration: 262330 loss: 0.0044 lr: 0.02\n",
            "iteration: 262340 loss: 0.0033 lr: 0.02\n",
            "iteration: 262350 loss: 0.0028 lr: 0.02\n",
            "iteration: 262360 loss: 0.0034 lr: 0.02\n",
            "iteration: 262370 loss: 0.0026 lr: 0.02\n",
            "iteration: 262380 loss: 0.0042 lr: 0.02\n",
            "iteration: 262390 loss: 0.0033 lr: 0.02\n",
            "iteration: 262400 loss: 0.0038 lr: 0.02\n",
            "iteration: 262410 loss: 0.0031 lr: 0.02\n",
            "iteration: 262420 loss: 0.0028 lr: 0.02\n",
            "iteration: 262430 loss: 0.0028 lr: 0.02\n",
            "iteration: 262440 loss: 0.0037 lr: 0.02\n",
            "iteration: 262450 loss: 0.0031 lr: 0.02\n",
            "iteration: 262460 loss: 0.0028 lr: 0.02\n",
            "iteration: 262470 loss: 0.0031 lr: 0.02\n",
            "iteration: 262480 loss: 0.0032 lr: 0.02\n",
            "iteration: 262490 loss: 0.0030 lr: 0.02\n",
            "iteration: 262500 loss: 0.0032 lr: 0.02\n",
            "iteration: 262510 loss: 0.0033 lr: 0.02\n",
            "iteration: 262520 loss: 0.0036 lr: 0.02\n",
            "iteration: 262530 loss: 0.0032 lr: 0.02\n",
            "iteration: 262540 loss: 0.0037 lr: 0.02\n",
            "iteration: 262550 loss: 0.0031 lr: 0.02\n",
            "iteration: 262560 loss: 0.0031 lr: 0.02\n",
            "iteration: 262570 loss: 0.0035 lr: 0.02\n",
            "iteration: 262580 loss: 0.0038 lr: 0.02\n",
            "iteration: 262590 loss: 0.0041 lr: 0.02\n",
            "iteration: 262600 loss: 0.0039 lr: 0.02\n",
            "iteration: 262610 loss: 0.0037 lr: 0.02\n",
            "iteration: 262620 loss: 0.0042 lr: 0.02\n",
            "iteration: 262630 loss: 0.0034 lr: 0.02\n",
            "iteration: 262640 loss: 0.0028 lr: 0.02\n",
            "iteration: 262650 loss: 0.0027 lr: 0.02\n",
            "iteration: 262660 loss: 0.0043 lr: 0.02\n",
            "iteration: 262670 loss: 0.0039 lr: 0.02\n",
            "iteration: 262680 loss: 0.0042 lr: 0.02\n",
            "iteration: 262690 loss: 0.0024 lr: 0.02\n",
            "iteration: 262700 loss: 0.0034 lr: 0.02\n",
            "iteration: 262710 loss: 0.0033 lr: 0.02\n",
            "iteration: 262720 loss: 0.0034 lr: 0.02\n",
            "iteration: 262730 loss: 0.0036 lr: 0.02\n",
            "iteration: 262740 loss: 0.0034 lr: 0.02\n",
            "iteration: 262750 loss: 0.0029 lr: 0.02\n",
            "iteration: 262760 loss: 0.0029 lr: 0.02\n",
            "iteration: 262770 loss: 0.0041 lr: 0.02\n",
            "iteration: 262780 loss: 0.0035 lr: 0.02\n",
            "iteration: 262790 loss: 0.0034 lr: 0.02\n",
            "iteration: 262800 loss: 0.0030 lr: 0.02\n",
            "iteration: 262810 loss: 0.0032 lr: 0.02\n",
            "iteration: 262820 loss: 0.0030 lr: 0.02\n",
            "iteration: 262830 loss: 0.0028 lr: 0.02\n",
            "iteration: 262840 loss: 0.0034 lr: 0.02\n",
            "iteration: 262850 loss: 0.0030 lr: 0.02\n",
            "iteration: 262860 loss: 0.0032 lr: 0.02\n",
            "iteration: 262870 loss: 0.0031 lr: 0.02\n",
            "iteration: 262880 loss: 0.0025 lr: 0.02\n",
            "iteration: 262890 loss: 0.0040 lr: 0.02\n",
            "iteration: 262900 loss: 0.0033 lr: 0.02\n",
            "iteration: 262910 loss: 0.0030 lr: 0.02\n",
            "iteration: 262920 loss: 0.0041 lr: 0.02\n",
            "iteration: 262930 loss: 0.0036 lr: 0.02\n",
            "iteration: 262940 loss: 0.0029 lr: 0.02\n",
            "iteration: 262950 loss: 0.0027 lr: 0.02\n",
            "iteration: 262960 loss: 0.0035 lr: 0.02\n",
            "iteration: 262970 loss: 0.0030 lr: 0.02\n",
            "iteration: 262980 loss: 0.0032 lr: 0.02\n",
            "iteration: 262990 loss: 0.0031 lr: 0.02\n",
            "iteration: 263000 loss: 0.0028 lr: 0.02\n",
            "iteration: 263010 loss: 0.0034 lr: 0.02\n",
            "iteration: 263020 loss: 0.0039 lr: 0.02\n",
            "iteration: 263030 loss: 0.0036 lr: 0.02\n",
            "iteration: 263040 loss: 0.0029 lr: 0.02\n",
            "iteration: 263050 loss: 0.0028 lr: 0.02\n",
            "iteration: 263060 loss: 0.0030 lr: 0.02\n",
            "iteration: 263070 loss: 0.0038 lr: 0.02\n",
            "iteration: 263080 loss: 0.0027 lr: 0.02\n",
            "iteration: 263090 loss: 0.0033 lr: 0.02\n",
            "iteration: 263100 loss: 0.0042 lr: 0.02\n",
            "iteration: 263110 loss: 0.0034 lr: 0.02\n",
            "iteration: 263120 loss: 0.0033 lr: 0.02\n",
            "iteration: 263130 loss: 0.0027 lr: 0.02\n",
            "iteration: 263140 loss: 0.0034 lr: 0.02\n",
            "iteration: 263150 loss: 0.0034 lr: 0.02\n",
            "iteration: 263160 loss: 0.0030 lr: 0.02\n",
            "iteration: 263170 loss: 0.0031 lr: 0.02\n",
            "iteration: 263180 loss: 0.0026 lr: 0.02\n",
            "iteration: 263190 loss: 0.0037 lr: 0.02\n",
            "iteration: 263200 loss: 0.0035 lr: 0.02\n",
            "iteration: 263210 loss: 0.0033 lr: 0.02\n",
            "iteration: 263220 loss: 0.0037 lr: 0.02\n",
            "iteration: 263230 loss: 0.0028 lr: 0.02\n",
            "iteration: 263240 loss: 0.0035 lr: 0.02\n",
            "iteration: 263250 loss: 0.0026 lr: 0.02\n",
            "iteration: 263260 loss: 0.0029 lr: 0.02\n",
            "iteration: 263270 loss: 0.0031 lr: 0.02\n",
            "iteration: 263280 loss: 0.0029 lr: 0.02\n",
            "iteration: 263290 loss: 0.0032 lr: 0.02\n",
            "iteration: 263300 loss: 0.0029 lr: 0.02\n",
            "iteration: 263310 loss: 0.0031 lr: 0.02\n",
            "iteration: 263320 loss: 0.0025 lr: 0.02\n",
            "iteration: 263330 loss: 0.0028 lr: 0.02\n",
            "iteration: 263340 loss: 0.0029 lr: 0.02\n",
            "iteration: 263350 loss: 0.0032 lr: 0.02\n",
            "iteration: 263360 loss: 0.0037 lr: 0.02\n",
            "iteration: 263370 loss: 0.0037 lr: 0.02\n",
            "iteration: 263380 loss: 0.0041 lr: 0.02\n",
            "iteration: 263390 loss: 0.0029 lr: 0.02\n",
            "iteration: 263400 loss: 0.0028 lr: 0.02\n",
            "iteration: 263410 loss: 0.0045 lr: 0.02\n",
            "iteration: 263420 loss: 0.0039 lr: 0.02\n",
            "iteration: 263430 loss: 0.0036 lr: 0.02\n",
            "iteration: 263440 loss: 0.0036 lr: 0.02\n",
            "iteration: 263450 loss: 0.0026 lr: 0.02\n",
            "iteration: 263460 loss: 0.0038 lr: 0.02\n",
            "iteration: 263470 loss: 0.0039 lr: 0.02\n",
            "iteration: 263480 loss: 0.0039 lr: 0.02\n",
            "iteration: 263490 loss: 0.0034 lr: 0.02\n",
            "iteration: 263500 loss: 0.0029 lr: 0.02\n",
            "iteration: 263510 loss: 0.0042 lr: 0.02\n",
            "iteration: 263520 loss: 0.0037 lr: 0.02\n",
            "iteration: 263530 loss: 0.0033 lr: 0.02\n",
            "iteration: 263540 loss: 0.0035 lr: 0.02\n",
            "iteration: 263550 loss: 0.0035 lr: 0.02\n",
            "iteration: 263560 loss: 0.0036 lr: 0.02\n",
            "iteration: 263570 loss: 0.0033 lr: 0.02\n",
            "iteration: 263580 loss: 0.0031 lr: 0.02\n",
            "iteration: 263590 loss: 0.0031 lr: 0.02\n",
            "iteration: 263600 loss: 0.0033 lr: 0.02\n",
            "iteration: 263610 loss: 0.0030 lr: 0.02\n",
            "iteration: 263620 loss: 0.0032 lr: 0.02\n",
            "iteration: 263630 loss: 0.0029 lr: 0.02\n",
            "iteration: 263640 loss: 0.0030 lr: 0.02\n",
            "iteration: 263650 loss: 0.0032 lr: 0.02\n",
            "iteration: 263660 loss: 0.0035 lr: 0.02\n",
            "iteration: 263670 loss: 0.0031 lr: 0.02\n",
            "iteration: 263680 loss: 0.0037 lr: 0.02\n",
            "iteration: 263690 loss: 0.0030 lr: 0.02\n",
            "iteration: 263700 loss: 0.0030 lr: 0.02\n",
            "iteration: 263710 loss: 0.0029 lr: 0.02\n",
            "iteration: 263720 loss: 0.0026 lr: 0.02\n",
            "iteration: 263730 loss: 0.0026 lr: 0.02\n",
            "iteration: 263740 loss: 0.0038 lr: 0.02\n",
            "iteration: 263750 loss: 0.0033 lr: 0.02\n",
            "iteration: 263760 loss: 0.0033 lr: 0.02\n",
            "iteration: 263770 loss: 0.0039 lr: 0.02\n",
            "iteration: 263780 loss: 0.0037 lr: 0.02\n",
            "iteration: 263790 loss: 0.0035 lr: 0.02\n",
            "iteration: 263800 loss: 0.0040 lr: 0.02\n",
            "iteration: 263810 loss: 0.0036 lr: 0.02\n",
            "iteration: 263820 loss: 0.0033 lr: 0.02\n",
            "iteration: 263830 loss: 0.0042 lr: 0.02\n",
            "iteration: 263840 loss: 0.0032 lr: 0.02\n",
            "iteration: 263850 loss: 0.0030 lr: 0.02\n",
            "iteration: 263860 loss: 0.0040 lr: 0.02\n",
            "iteration: 263870 loss: 0.0026 lr: 0.02\n",
            "iteration: 263880 loss: 0.0030 lr: 0.02\n",
            "iteration: 263890 loss: 0.0039 lr: 0.02\n",
            "iteration: 263900 loss: 0.0035 lr: 0.02\n",
            "iteration: 263910 loss: 0.0038 lr: 0.02\n",
            "iteration: 263920 loss: 0.0041 lr: 0.02\n",
            "iteration: 263930 loss: 0.0036 lr: 0.02\n",
            "iteration: 263940 loss: 0.0038 lr: 0.02\n",
            "iteration: 263950 loss: 0.0034 lr: 0.02\n",
            "iteration: 263960 loss: 0.0026 lr: 0.02\n",
            "iteration: 263970 loss: 0.0029 lr: 0.02\n",
            "iteration: 263980 loss: 0.0034 lr: 0.02\n",
            "iteration: 263990 loss: 0.0038 lr: 0.02\n",
            "iteration: 264000 loss: 0.0025 lr: 0.02\n",
            "iteration: 264010 loss: 0.0041 lr: 0.02\n",
            "iteration: 264020 loss: 0.0026 lr: 0.02\n",
            "iteration: 264030 loss: 0.0029 lr: 0.02\n",
            "iteration: 264040 loss: 0.0031 lr: 0.02\n",
            "iteration: 264050 loss: 0.0026 lr: 0.02\n",
            "iteration: 264060 loss: 0.0027 lr: 0.02\n",
            "iteration: 264070 loss: 0.0031 lr: 0.02\n",
            "iteration: 264080 loss: 0.0032 lr: 0.02\n",
            "iteration: 264090 loss: 0.0043 lr: 0.02\n",
            "iteration: 264100 loss: 0.0030 lr: 0.02\n",
            "iteration: 264110 loss: 0.0033 lr: 0.02\n",
            "iteration: 264120 loss: 0.0033 lr: 0.02\n",
            "iteration: 264130 loss: 0.0030 lr: 0.02\n",
            "iteration: 264140 loss: 0.0030 lr: 0.02\n",
            "iteration: 264150 loss: 0.0032 lr: 0.02\n",
            "iteration: 264160 loss: 0.0031 lr: 0.02\n",
            "iteration: 264170 loss: 0.0036 lr: 0.02\n",
            "iteration: 264180 loss: 0.0033 lr: 0.02\n",
            "iteration: 264190 loss: 0.0026 lr: 0.02\n",
            "iteration: 264200 loss: 0.0029 lr: 0.02\n",
            "iteration: 264210 loss: 0.0040 lr: 0.02\n",
            "iteration: 264220 loss: 0.0032 lr: 0.02\n",
            "iteration: 264230 loss: 0.0030 lr: 0.02\n",
            "iteration: 264240 loss: 0.0028 lr: 0.02\n",
            "iteration: 264250 loss: 0.0034 lr: 0.02\n",
            "iteration: 264260 loss: 0.0037 lr: 0.02\n",
            "iteration: 264270 loss: 0.0029 lr: 0.02\n",
            "iteration: 264280 loss: 0.0025 lr: 0.02\n",
            "iteration: 264290 loss: 0.0035 lr: 0.02\n",
            "iteration: 264300 loss: 0.0031 lr: 0.02\n",
            "iteration: 264310 loss: 0.0029 lr: 0.02\n",
            "iteration: 264320 loss: 0.0035 lr: 0.02\n",
            "iteration: 264330 loss: 0.0039 lr: 0.02\n",
            "iteration: 264340 loss: 0.0039 lr: 0.02\n",
            "iteration: 264350 loss: 0.0035 lr: 0.02\n",
            "iteration: 264360 loss: 0.0030 lr: 0.02\n",
            "iteration: 264370 loss: 0.0031 lr: 0.02\n",
            "iteration: 264380 loss: 0.0030 lr: 0.02\n",
            "iteration: 264390 loss: 0.0041 lr: 0.02\n",
            "iteration: 264400 loss: 0.0025 lr: 0.02\n",
            "iteration: 264410 loss: 0.0032 lr: 0.02\n",
            "iteration: 264420 loss: 0.0030 lr: 0.02\n",
            "iteration: 264430 loss: 0.0039 lr: 0.02\n",
            "iteration: 264440 loss: 0.0033 lr: 0.02\n",
            "iteration: 264450 loss: 0.0029 lr: 0.02\n",
            "iteration: 264460 loss: 0.0026 lr: 0.02\n",
            "iteration: 264470 loss: 0.0032 lr: 0.02\n",
            "iteration: 264480 loss: 0.0030 lr: 0.02\n",
            "iteration: 264490 loss: 0.0037 lr: 0.02\n",
            "iteration: 264500 loss: 0.0024 lr: 0.02\n",
            "iteration: 264510 loss: 0.0031 lr: 0.02\n",
            "iteration: 264520 loss: 0.0024 lr: 0.02\n",
            "iteration: 264530 loss: 0.0039 lr: 0.02\n",
            "iteration: 264540 loss: 0.0036 lr: 0.02\n",
            "iteration: 264550 loss: 0.0035 lr: 0.02\n",
            "iteration: 264560 loss: 0.0037 lr: 0.02\n",
            "iteration: 264570 loss: 0.0037 lr: 0.02\n",
            "iteration: 264580 loss: 0.0033 lr: 0.02\n",
            "iteration: 264590 loss: 0.0034 lr: 0.02\n",
            "iteration: 264600 loss: 0.0034 lr: 0.02\n",
            "iteration: 264610 loss: 0.0029 lr: 0.02\n",
            "iteration: 264620 loss: 0.0031 lr: 0.02\n",
            "iteration: 264630 loss: 0.0034 lr: 0.02\n",
            "iteration: 264640 loss: 0.0034 lr: 0.02\n",
            "iteration: 264650 loss: 0.0030 lr: 0.02\n",
            "iteration: 264660 loss: 0.0027 lr: 0.02\n",
            "iteration: 264670 loss: 0.0032 lr: 0.02\n",
            "iteration: 264680 loss: 0.0031 lr: 0.02\n",
            "iteration: 264690 loss: 0.0023 lr: 0.02\n",
            "iteration: 264700 loss: 0.0027 lr: 0.02\n",
            "iteration: 264710 loss: 0.0043 lr: 0.02\n",
            "iteration: 264720 loss: 0.0024 lr: 0.02\n",
            "iteration: 264730 loss: 0.0032 lr: 0.02\n",
            "iteration: 264740 loss: 0.0025 lr: 0.02\n",
            "iteration: 264750 loss: 0.0039 lr: 0.02\n",
            "iteration: 264760 loss: 0.0029 lr: 0.02\n",
            "iteration: 264770 loss: 0.0029 lr: 0.02\n",
            "iteration: 264780 loss: 0.0031 lr: 0.02\n",
            "iteration: 264790 loss: 0.0040 lr: 0.02\n",
            "iteration: 264800 loss: 0.0026 lr: 0.02\n",
            "iteration: 264810 loss: 0.0038 lr: 0.02\n",
            "iteration: 264820 loss: 0.0032 lr: 0.02\n",
            "iteration: 264830 loss: 0.0033 lr: 0.02\n",
            "iteration: 264840 loss: 0.0029 lr: 0.02\n",
            "iteration: 264850 loss: 0.0026 lr: 0.02\n",
            "iteration: 264860 loss: 0.0029 lr: 0.02\n",
            "iteration: 264870 loss: 0.0037 lr: 0.02\n",
            "iteration: 264880 loss: 0.0029 lr: 0.02\n",
            "iteration: 264890 loss: 0.0027 lr: 0.02\n",
            "iteration: 264900 loss: 0.0033 lr: 0.02\n",
            "iteration: 264910 loss: 0.0037 lr: 0.02\n",
            "iteration: 264920 loss: 0.0032 lr: 0.02\n",
            "iteration: 264930 loss: 0.0033 lr: 0.02\n",
            "iteration: 264940 loss: 0.0032 lr: 0.02\n",
            "iteration: 264950 loss: 0.0048 lr: 0.02\n",
            "iteration: 264960 loss: 0.0029 lr: 0.02\n",
            "iteration: 264970 loss: 0.0046 lr: 0.02\n",
            "iteration: 264980 loss: 0.0034 lr: 0.02\n",
            "iteration: 264990 loss: 0.0037 lr: 0.02\n",
            "iteration: 265000 loss: 0.0032 lr: 0.02\n",
            "iteration: 265010 loss: 0.0033 lr: 0.02\n",
            "iteration: 265020 loss: 0.0034 lr: 0.02\n",
            "iteration: 265030 loss: 0.0031 lr: 0.02\n",
            "iteration: 265040 loss: 0.0026 lr: 0.02\n",
            "iteration: 265050 loss: 0.0029 lr: 0.02\n",
            "iteration: 265060 loss: 0.0032 lr: 0.02\n",
            "iteration: 265070 loss: 0.0026 lr: 0.02\n",
            "iteration: 265080 loss: 0.0032 lr: 0.02\n",
            "iteration: 265090 loss: 0.0030 lr: 0.02\n",
            "iteration: 265100 loss: 0.0035 lr: 0.02\n",
            "iteration: 265110 loss: 0.0029 lr: 0.02\n",
            "iteration: 265120 loss: 0.0024 lr: 0.02\n",
            "iteration: 265130 loss: 0.0033 lr: 0.02\n",
            "iteration: 265140 loss: 0.0030 lr: 0.02\n",
            "iteration: 265150 loss: 0.0030 lr: 0.02\n",
            "iteration: 265160 loss: 0.0034 lr: 0.02\n",
            "iteration: 265170 loss: 0.0035 lr: 0.02\n",
            "iteration: 265180 loss: 0.0042 lr: 0.02\n",
            "iteration: 265190 loss: 0.0024 lr: 0.02\n",
            "iteration: 265200 loss: 0.0036 lr: 0.02\n",
            "iteration: 265210 loss: 0.0032 lr: 0.02\n",
            "iteration: 265220 loss: 0.0027 lr: 0.02\n",
            "iteration: 265230 loss: 0.0029 lr: 0.02\n",
            "iteration: 265240 loss: 0.0044 lr: 0.02\n",
            "iteration: 265250 loss: 0.0043 lr: 0.02\n",
            "iteration: 265260 loss: 0.0031 lr: 0.02\n",
            "iteration: 265270 loss: 0.0038 lr: 0.02\n",
            "iteration: 265280 loss: 0.0029 lr: 0.02\n",
            "iteration: 265290 loss: 0.0027 lr: 0.02\n",
            "iteration: 265300 loss: 0.0033 lr: 0.02\n",
            "iteration: 265310 loss: 0.0029 lr: 0.02\n",
            "iteration: 265320 loss: 0.0034 lr: 0.02\n",
            "iteration: 265330 loss: 0.0029 lr: 0.02\n",
            "iteration: 265340 loss: 0.0027 lr: 0.02\n",
            "iteration: 265350 loss: 0.0027 lr: 0.02\n",
            "iteration: 265360 loss: 0.0035 lr: 0.02\n",
            "iteration: 265370 loss: 0.0036 lr: 0.02\n",
            "iteration: 265380 loss: 0.0027 lr: 0.02\n",
            "iteration: 265390 loss: 0.0034 lr: 0.02\n",
            "iteration: 265400 loss: 0.0030 lr: 0.02\n",
            "iteration: 265410 loss: 0.0042 lr: 0.02\n",
            "iteration: 265420 loss: 0.0033 lr: 0.02\n",
            "iteration: 265430 loss: 0.0030 lr: 0.02\n",
            "iteration: 265440 loss: 0.0030 lr: 0.02\n",
            "iteration: 265450 loss: 0.0025 lr: 0.02\n",
            "iteration: 265460 loss: 0.0036 lr: 0.02\n",
            "iteration: 265470 loss: 0.0031 lr: 0.02\n",
            "iteration: 265480 loss: 0.0025 lr: 0.02\n",
            "iteration: 265490 loss: 0.0037 lr: 0.02\n",
            "iteration: 265500 loss: 0.0035 lr: 0.02\n",
            "iteration: 265510 loss: 0.0030 lr: 0.02\n",
            "iteration: 265520 loss: 0.0034 lr: 0.02\n",
            "iteration: 265530 loss: 0.0033 lr: 0.02\n",
            "iteration: 265540 loss: 0.0038 lr: 0.02\n",
            "iteration: 265550 loss: 0.0029 lr: 0.02\n",
            "iteration: 265560 loss: 0.0027 lr: 0.02\n",
            "iteration: 265570 loss: 0.0034 lr: 0.02\n",
            "iteration: 265580 loss: 0.0038 lr: 0.02\n",
            "iteration: 265590 loss: 0.0046 lr: 0.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiDwIVf5-3H_"
      },
      "source": [
        "**When you hit \"STOP\" you will get a KeyInterrupt \"error\"! No worries! :)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZygsb2DoEJc"
      },
      "source": [
        "## Start evaluating:\n",
        "This function evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images)\n",
        "and stores the results as .csv file in a subdirectory under **evaluation-results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv4zlbrnoEJg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "12afe9b6-8eae-4465-a1ca-a1d44290f514"
      },
      "source": [
        "%matplotlib notebook\n",
        "deeplabcut.evaluate_network(path_config_file,plotting=True)\n",
        "\n",
        "# Here you want to see a low pixel error! Of course, it can only be as good as the labeler, \n",
        "#so be sure your labels are good! (And you have trained enough ;)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running  DLC_resnet50_Test1_8bpMay19shuffle1_266000  with # of training iterations: 266000\n",
            "Running evaluation ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "60it [00:21,  2.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis is done and the results are stored (see evaluation-results) for snapshot:  snapshot-266000\n",
            "Results for 266000  training iterations: 95 1 train error: 1.48 pixels. Test error: 8.6  pixels.\n",
            "With pcutoff of 0.6  train error: 1.48 pixels. Test error: 7.19 pixels\n",
            "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
            "Plotting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "/* Put everything inside the global mpl namespace */\n",
              "/* global mpl */\n",
              "window.mpl = {};\n",
              "\n",
              "mpl.get_websocket_type = function () {\n",
              "    if (typeof WebSocket !== 'undefined') {\n",
              "        return WebSocket;\n",
              "    } else if (typeof MozWebSocket !== 'undefined') {\n",
              "        return MozWebSocket;\n",
              "    } else {\n",
              "        alert(\n",
              "            'Your browser does not have WebSocket support. ' +\n",
              "                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
              "                'Firefox 4 and 5 are also supported but you ' +\n",
              "                'have to enable WebSockets in about:config.'\n",
              "        );\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n",
              "    this.id = figure_id;\n",
              "\n",
              "    this.ws = websocket;\n",
              "\n",
              "    this.supports_binary = this.ws.binaryType !== undefined;\n",
              "\n",
              "    if (!this.supports_binary) {\n",
              "        var warnings = document.getElementById('mpl-warnings');\n",
              "        if (warnings) {\n",
              "            warnings.style.display = 'block';\n",
              "            warnings.textContent =\n",
              "                'This browser does not support binary websocket messages. ' +\n",
              "                'Performance may be slow.';\n",
              "        }\n",
              "    }\n",
              "\n",
              "    this.imageObj = new Image();\n",
              "\n",
              "    this.context = undefined;\n",
              "    this.message = undefined;\n",
              "    this.canvas = undefined;\n",
              "    this.rubberband_canvas = undefined;\n",
              "    this.rubberband_context = undefined;\n",
              "    this.format_dropdown = undefined;\n",
              "\n",
              "    this.image_mode = 'full';\n",
              "\n",
              "    this.root = document.createElement('div');\n",
              "    this.root.setAttribute('style', 'display: inline-block');\n",
              "    this._root_extra_style(this.root);\n",
              "\n",
              "    parent_element.appendChild(this.root);\n",
              "\n",
              "    this._init_header(this);\n",
              "    this._init_canvas(this);\n",
              "    this._init_toolbar(this);\n",
              "\n",
              "    var fig = this;\n",
              "\n",
              "    this.waiting = false;\n",
              "\n",
              "    this.ws.onopen = function () {\n",
              "        fig.send_message('supports_binary', { value: fig.supports_binary });\n",
              "        fig.send_message('send_image_mode', {});\n",
              "        if (fig.ratio !== 1) {\n",
              "            fig.send_message('set_device_pixel_ratio', {\n",
              "                device_pixel_ratio: fig.ratio,\n",
              "            });\n",
              "        }\n",
              "        fig.send_message('refresh', {});\n",
              "    };\n",
              "\n",
              "    this.imageObj.onload = function () {\n",
              "        if (fig.image_mode === 'full') {\n",
              "            // Full images could contain transparency (where diff images\n",
              "            // almost always do), so we need to clear the canvas so that\n",
              "            // there is no ghosting.\n",
              "            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
              "        }\n",
              "        fig.context.drawImage(fig.imageObj, 0, 0);\n",
              "    };\n",
              "\n",
              "    this.imageObj.onunload = function () {\n",
              "        fig.ws.close();\n",
              "    };\n",
              "\n",
              "    this.ws.onmessage = this._make_on_message_function(this);\n",
              "\n",
              "    this.ondownload = ondownload;\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._init_header = function () {\n",
              "    var titlebar = document.createElement('div');\n",
              "    titlebar.classList =\n",
              "        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n",
              "    var titletext = document.createElement('div');\n",
              "    titletext.classList = 'ui-dialog-title';\n",
              "    titletext.setAttribute(\n",
              "        'style',\n",
              "        'width: 100%; text-align: center; padding: 3px;'\n",
              "    );\n",
              "    titlebar.appendChild(titletext);\n",
              "    this.root.appendChild(titlebar);\n",
              "    this.header = titletext;\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n",
              "\n",
              "mpl.figure.prototype._init_canvas = function () {\n",
              "    var fig = this;\n",
              "\n",
              "    var canvas_div = (this.canvas_div = document.createElement('div'));\n",
              "    canvas_div.setAttribute(\n",
              "        'style',\n",
              "        'border: 1px solid #ddd;' +\n",
              "            'box-sizing: content-box;' +\n",
              "            'clear: both;' +\n",
              "            'min-height: 1px;' +\n",
              "            'min-width: 1px;' +\n",
              "            'outline: 0;' +\n",
              "            'overflow: hidden;' +\n",
              "            'position: relative;' +\n",
              "            'resize: both;'\n",
              "    );\n",
              "\n",
              "    function on_keyboard_event_closure(name) {\n",
              "        return function (event) {\n",
              "            return fig.key_event(event, name);\n",
              "        };\n",
              "    }\n",
              "\n",
              "    canvas_div.addEventListener(\n",
              "        'keydown',\n",
              "        on_keyboard_event_closure('key_press')\n",
              "    );\n",
              "    canvas_div.addEventListener(\n",
              "        'keyup',\n",
              "        on_keyboard_event_closure('key_release')\n",
              "    );\n",
              "\n",
              "    this._canvas_extra_style(canvas_div);\n",
              "    this.root.appendChild(canvas_div);\n",
              "\n",
              "    var canvas = (this.canvas = document.createElement('canvas'));\n",
              "    canvas.classList.add('mpl-canvas');\n",
              "    canvas.setAttribute('style', 'box-sizing: content-box;');\n",
              "\n",
              "    this.context = canvas.getContext('2d');\n",
              "\n",
              "    var backingStore =\n",
              "        this.context.backingStorePixelRatio ||\n",
              "        this.context.webkitBackingStorePixelRatio ||\n",
              "        this.context.mozBackingStorePixelRatio ||\n",
              "        this.context.msBackingStorePixelRatio ||\n",
              "        this.context.oBackingStorePixelRatio ||\n",
              "        this.context.backingStorePixelRatio ||\n",
              "        1;\n",
              "\n",
              "    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
              "\n",
              "    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n",
              "        'canvas'\n",
              "    ));\n",
              "    rubberband_canvas.setAttribute(\n",
              "        'style',\n",
              "        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'\n",
              "    );\n",
              "\n",
              "    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n",
              "    if (this.ResizeObserver === undefined) {\n",
              "        if (window.ResizeObserver !== undefined) {\n",
              "            this.ResizeObserver = window.ResizeObserver;\n",
              "        } else {\n",
              "            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n",
              "            this.ResizeObserver = obs.ResizeObserver;\n",
              "        }\n",
              "    }\n",
              "\n",
              "    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n",
              "        var nentries = entries.length;\n",
              "        for (var i = 0; i < nentries; i++) {\n",
              "            var entry = entries[i];\n",
              "            var width, height;\n",
              "            if (entry.contentBoxSize) {\n",
              "                if (entry.contentBoxSize instanceof Array) {\n",
              "                    // Chrome 84 implements new version of spec.\n",
              "                    width = entry.contentBoxSize[0].inlineSize;\n",
              "                    height = entry.contentBoxSize[0].blockSize;\n",
              "                } else {\n",
              "                    // Firefox implements old version of spec.\n",
              "                    width = entry.contentBoxSize.inlineSize;\n",
              "                    height = entry.contentBoxSize.blockSize;\n",
              "                }\n",
              "            } else {\n",
              "                // Chrome <84 implements even older version of spec.\n",
              "                width = entry.contentRect.width;\n",
              "                height = entry.contentRect.height;\n",
              "            }\n",
              "\n",
              "            // Keep the size of the canvas and rubber band canvas in sync with\n",
              "            // the canvas container.\n",
              "            if (entry.devicePixelContentBoxSize) {\n",
              "                // Chrome 84 implements new version of spec.\n",
              "                canvas.setAttribute(\n",
              "                    'width',\n",
              "                    entry.devicePixelContentBoxSize[0].inlineSize\n",
              "                );\n",
              "                canvas.setAttribute(\n",
              "                    'height',\n",
              "                    entry.devicePixelContentBoxSize[0].blockSize\n",
              "                );\n",
              "            } else {\n",
              "                canvas.setAttribute('width', width * fig.ratio);\n",
              "                canvas.setAttribute('height', height * fig.ratio);\n",
              "            }\n",
              "            canvas.setAttribute(\n",
              "                'style',\n",
              "                'width: ' + width + 'px; height: ' + height + 'px;'\n",
              "            );\n",
              "\n",
              "            rubberband_canvas.setAttribute('width', width);\n",
              "            rubberband_canvas.setAttribute('height', height);\n",
              "\n",
              "            // And update the size in Python. We ignore the initial 0/0 size\n",
              "            // that occurs as the element is placed into the DOM, which should\n",
              "            // otherwise not happen due to the minimum size styling.\n",
              "            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n",
              "                fig.request_resize(width, height);\n",
              "            }\n",
              "        }\n",
              "    });\n",
              "    this.resizeObserverInstance.observe(canvas_div);\n",
              "\n",
              "    function on_mouse_event_closure(name) {\n",
              "        return function (event) {\n",
              "            return fig.mouse_event(event, name);\n",
              "        };\n",
              "    }\n",
              "\n",
              "    rubberband_canvas.addEventListener(\n",
              "        'mousedown',\n",
              "        on_mouse_event_closure('button_press')\n",
              "    );\n",
              "    rubberband_canvas.addEventListener(\n",
              "        'mouseup',\n",
              "        on_mouse_event_closure('button_release')\n",
              "    );\n",
              "    rubberband_canvas.addEventListener(\n",
              "        'dblclick',\n",
              "        on_mouse_event_closure('dblclick')\n",
              "    );\n",
              "    // Throttle sequential mouse events to 1 every 20ms.\n",
              "    rubberband_canvas.addEventListener(\n",
              "        'mousemove',\n",
              "        on_mouse_event_closure('motion_notify')\n",
              "    );\n",
              "\n",
              "    rubberband_canvas.addEventListener(\n",
              "        'mouseenter',\n",
              "        on_mouse_event_closure('figure_enter')\n",
              "    );\n",
              "    rubberband_canvas.addEventListener(\n",
              "        'mouseleave',\n",
              "        on_mouse_event_closure('figure_leave')\n",
              "    );\n",
              "\n",
              "    canvas_div.addEventListener('wheel', function (event) {\n",
              "        if (event.deltaY < 0) {\n",
              "            event.step = 1;\n",
              "        } else {\n",
              "            event.step = -1;\n",
              "        }\n",
              "        on_mouse_event_closure('scroll')(event);\n",
              "    });\n",
              "\n",
              "    canvas_div.appendChild(canvas);\n",
              "    canvas_div.appendChild(rubberband_canvas);\n",
              "\n",
              "    this.rubberband_context = rubberband_canvas.getContext('2d');\n",
              "    this.rubberband_context.strokeStyle = '#000000';\n",
              "\n",
              "    this._resize_canvas = function (width, height, forward) {\n",
              "        if (forward) {\n",
              "            canvas_div.style.width = width + 'px';\n",
              "            canvas_div.style.height = height + 'px';\n",
              "        }\n",
              "    };\n",
              "\n",
              "    // Disable right mouse context menu.\n",
              "    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {\n",
              "        event.preventDefault();\n",
              "        return false;\n",
              "    });\n",
              "\n",
              "    function set_focus() {\n",
              "        canvas.focus();\n",
              "        canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    window.setTimeout(set_focus, 100);\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function () {\n",
              "    var fig = this;\n",
              "\n",
              "    var toolbar = document.createElement('div');\n",
              "    toolbar.classList = 'mpl-toolbar';\n",
              "    this.root.appendChild(toolbar);\n",
              "\n",
              "    function on_click_closure(name) {\n",
              "        return function (_event) {\n",
              "            return fig.toolbar_button_onclick(name);\n",
              "        };\n",
              "    }\n",
              "\n",
              "    function on_mouseover_closure(tooltip) {\n",
              "        return function (event) {\n",
              "            if (!event.currentTarget.disabled) {\n",
              "                return fig.toolbar_button_onmouseover(tooltip);\n",
              "            }\n",
              "        };\n",
              "    }\n",
              "\n",
              "    fig.buttons = {};\n",
              "    var buttonGroup = document.createElement('div');\n",
              "    buttonGroup.classList = 'mpl-button-group';\n",
              "    for (var toolbar_ind in mpl.toolbar_items) {\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) {\n",
              "            /* Instead of a spacer, we start a new button group. */\n",
              "            if (buttonGroup.hasChildNodes()) {\n",
              "                toolbar.appendChild(buttonGroup);\n",
              "            }\n",
              "            buttonGroup = document.createElement('div');\n",
              "            buttonGroup.classList = 'mpl-button-group';\n",
              "            continue;\n",
              "        }\n",
              "\n",
              "        var button = (fig.buttons[name] = document.createElement('button'));\n",
              "        button.classList = 'mpl-widget';\n",
              "        button.setAttribute('role', 'button');\n",
              "        button.setAttribute('aria-disabled', 'false');\n",
              "        button.addEventListener('click', on_click_closure(method_name));\n",
              "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
              "\n",
              "        var icon_img = document.createElement('img');\n",
              "        icon_img.src = '_images/' + image + '.png';\n",
              "        icon_img.srcset = '_images/' + image + '_large.png 2x';\n",
              "        icon_img.alt = tooltip;\n",
              "        button.appendChild(icon_img);\n",
              "\n",
              "        buttonGroup.appendChild(button);\n",
              "    }\n",
              "\n",
              "    if (buttonGroup.hasChildNodes()) {\n",
              "        toolbar.appendChild(buttonGroup);\n",
              "    }\n",
              "\n",
              "    var fmt_picker = document.createElement('select');\n",
              "    fmt_picker.classList = 'mpl-widget';\n",
              "    toolbar.appendChild(fmt_picker);\n",
              "    this.format_dropdown = fmt_picker;\n",
              "\n",
              "    for (var ind in mpl.extensions) {\n",
              "        var fmt = mpl.extensions[ind];\n",
              "        var option = document.createElement('option');\n",
              "        option.selected = fmt === mpl.default_extension;\n",
              "        option.innerHTML = fmt;\n",
              "        fmt_picker.appendChild(option);\n",
              "    }\n",
              "\n",
              "    var status_bar = document.createElement('span');\n",
              "    status_bar.classList = 'mpl-message';\n",
              "    toolbar.appendChild(status_bar);\n",
              "    this.message = status_bar;\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n",
              "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
              "    // which will in turn request a refresh of the image.\n",
              "    this.send_message('resize', { width: x_pixels, height: y_pixels });\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.send_message = function (type, properties) {\n",
              "    properties['type'] = type;\n",
              "    properties['figure_id'] = this.id;\n",
              "    this.ws.send(JSON.stringify(properties));\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.send_draw_message = function () {\n",
              "    if (!this.waiting) {\n",
              "        this.waiting = true;\n",
              "        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
              "    var format_dropdown = fig.format_dropdown;\n",
              "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
              "    fig.ondownload(fig, format);\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_resize = function (fig, msg) {\n",
              "    var size = msg['size'];\n",
              "    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n",
              "        fig._resize_canvas(size[0], size[1], msg['forward']);\n",
              "        fig.send_message('refresh', {});\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n",
              "    var x0 = msg['x0'] / fig.ratio;\n",
              "    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n",
              "    var x1 = msg['x1'] / fig.ratio;\n",
              "    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n",
              "    x0 = Math.floor(x0) + 0.5;\n",
              "    y0 = Math.floor(y0) + 0.5;\n",
              "    x1 = Math.floor(x1) + 0.5;\n",
              "    y1 = Math.floor(y1) + 0.5;\n",
              "    var min_x = Math.min(x0, x1);\n",
              "    var min_y = Math.min(y0, y1);\n",
              "    var width = Math.abs(x1 - x0);\n",
              "    var height = Math.abs(y1 - y0);\n",
              "\n",
              "    fig.rubberband_context.clearRect(\n",
              "        0,\n",
              "        0,\n",
              "        fig.canvas.width / fig.ratio,\n",
              "        fig.canvas.height / fig.ratio\n",
              "    );\n",
              "\n",
              "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n",
              "    // Updates the figure title.\n",
              "    fig.header.textContent = msg['label'];\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_cursor = function (fig, msg) {\n",
              "    fig.rubberband_canvas.style.cursor = msg['cursor'];\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_message = function (fig, msg) {\n",
              "    fig.message.textContent = msg['message'];\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_draw = function (fig, _msg) {\n",
              "    // Request the server to send over a new figure.\n",
              "    fig.send_draw_message();\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n",
              "    fig.image_mode = msg['mode'];\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n",
              "    for (var key in msg) {\n",
              "        if (!(key in fig.buttons)) {\n",
              "            continue;\n",
              "        }\n",
              "        fig.buttons[key].disabled = !msg[key];\n",
              "        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n",
              "    if (msg['mode'] === 'PAN') {\n",
              "        fig.buttons['Pan'].classList.add('active');\n",
              "        fig.buttons['Zoom'].classList.remove('active');\n",
              "    } else if (msg['mode'] === 'ZOOM') {\n",
              "        fig.buttons['Pan'].classList.remove('active');\n",
              "        fig.buttons['Zoom'].classList.add('active');\n",
              "    } else {\n",
              "        fig.buttons['Pan'].classList.remove('active');\n",
              "        fig.buttons['Zoom'].classList.remove('active');\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function () {\n",
              "    // Called whenever the canvas gets updated.\n",
              "    this.send_message('ack', {});\n",
              "};\n",
              "\n",
              "// A function to construct a web socket function for onmessage handling.\n",
              "// Called in the figure constructor.\n",
              "mpl.figure.prototype._make_on_message_function = function (fig) {\n",
              "    return function socket_on_message(evt) {\n",
              "        if (evt.data instanceof Blob) {\n",
              "            var img = evt.data;\n",
              "            if (img.type !== 'image/png') {\n",
              "                /* FIXME: We get \"Resource interpreted as Image but\n",
              "                 * transferred with MIME type text/plain:\" errors on\n",
              "                 * Chrome.  But how to set the MIME type?  It doesn't seem\n",
              "                 * to be part of the websocket stream */\n",
              "                img.type = 'image/png';\n",
              "            }\n",
              "\n",
              "            /* Free the memory for the previous frames */\n",
              "            if (fig.imageObj.src) {\n",
              "                (window.URL || window.webkitURL).revokeObjectURL(\n",
              "                    fig.imageObj.src\n",
              "                );\n",
              "            }\n",
              "\n",
              "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
              "                img\n",
              "            );\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        } else if (\n",
              "            typeof evt.data === 'string' &&\n",
              "            evt.data.slice(0, 21) === 'data:image/png;base64'\n",
              "        ) {\n",
              "            fig.imageObj.src = evt.data;\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        var msg = JSON.parse(evt.data);\n",
              "        var msg_type = msg['type'];\n",
              "\n",
              "        // Call the  \"handle_{type}\" callback, which takes\n",
              "        // the figure and JSON message as its only arguments.\n",
              "        try {\n",
              "            var callback = fig['handle_' + msg_type];\n",
              "        } catch (e) {\n",
              "            console.log(\n",
              "                \"No handler for the '\" + msg_type + \"' message type: \",\n",
              "                msg\n",
              "            );\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        if (callback) {\n",
              "            try {\n",
              "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
              "                callback(fig, msg);\n",
              "            } catch (e) {\n",
              "                console.log(\n",
              "                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n",
              "                    e,\n",
              "                    e.stack,\n",
              "                    msg\n",
              "                );\n",
              "            }\n",
              "        }\n",
              "    };\n",
              "};\n",
              "\n",
              "// from https://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
              "mpl.findpos = function (e) {\n",
              "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
              "    var targ;\n",
              "    if (!e) {\n",
              "        e = window.event;\n",
              "    }\n",
              "    if (e.target) {\n",
              "        targ = e.target;\n",
              "    } else if (e.srcElement) {\n",
              "        targ = e.srcElement;\n",
              "    }\n",
              "    if (targ.nodeType === 3) {\n",
              "        // defeat Safari bug\n",
              "        targ = targ.parentNode;\n",
              "    }\n",
              "\n",
              "    // pageX,Y are the mouse positions relative to the document\n",
              "    var boundingRect = targ.getBoundingClientRect();\n",
              "    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);\n",
              "    var y = e.pageY - (boundingRect.top + document.body.scrollTop);\n",
              "\n",
              "    return { x: x, y: y };\n",
              "};\n",
              "\n",
              "/*\n",
              " * return a copy of an object with only non-object keys\n",
              " * we need this to avoid circular references\n",
              " * https://stackoverflow.com/a/24161582/3208463\n",
              " */\n",
              "function simpleKeys(original) {\n",
              "    return Object.keys(original).reduce(function (obj, key) {\n",
              "        if (typeof original[key] !== 'object') {\n",
              "            obj[key] = original[key];\n",
              "        }\n",
              "        return obj;\n",
              "    }, {});\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.mouse_event = function (event, name) {\n",
              "    var canvas_pos = mpl.findpos(event);\n",
              "\n",
              "    if (name === 'button_press') {\n",
              "        this.canvas.focus();\n",
              "        this.canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    var x = canvas_pos.x * this.ratio;\n",
              "    var y = canvas_pos.y * this.ratio;\n",
              "\n",
              "    this.send_message(name, {\n",
              "        x: x,\n",
              "        y: y,\n",
              "        button: event.button,\n",
              "        step: event.step,\n",
              "        guiEvent: simpleKeys(event),\n",
              "    });\n",
              "\n",
              "    /* This prevents the web browser from automatically changing to\n",
              "     * the text insertion cursor when the button is pressed.  We want\n",
              "     * to control all of the cursor setting manually through the\n",
              "     * 'cursor' event from matplotlib */\n",
              "    event.preventDefault();\n",
              "    return false;\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function (_event, _name) {\n",
              "    // Handle any extra behaviour associated with a key event\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.key_event = function (event, name) {\n",
              "    // Prevent repeat events\n",
              "    if (name === 'key_press') {\n",
              "        if (event.key === this._key) {\n",
              "            return;\n",
              "        } else {\n",
              "            this._key = event.key;\n",
              "        }\n",
              "    }\n",
              "    if (name === 'key_release') {\n",
              "        this._key = null;\n",
              "    }\n",
              "\n",
              "    var value = '';\n",
              "    if (event.ctrlKey && event.key !== 'Control') {\n",
              "        value += 'ctrl+';\n",
              "    }\n",
              "    else if (event.altKey && event.key !== 'Alt') {\n",
              "        value += 'alt+';\n",
              "    }\n",
              "    else if (event.shiftKey && event.key !== 'Shift') {\n",
              "        value += 'shift+';\n",
              "    }\n",
              "\n",
              "    value += 'k' + event.key;\n",
              "\n",
              "    this._key_event_extra(event, name);\n",
              "\n",
              "    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n",
              "    return false;\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onclick = function (name) {\n",
              "    if (name === 'download') {\n",
              "        this.handle_save(this, null);\n",
              "    } else {\n",
              "        this.send_message('toolbar_button', { name: name });\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n",
              "    this.message.textContent = tooltip;\n",
              "};\n",
              "\n",
              "///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n",
              "// prettier-ignore\n",
              "var _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\n",
              "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
              "\n",
              "mpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
              "\n",
              "mpl.default_extension = \"png\";/* global mpl */\n",
              "\n",
              "var comm_websocket_adapter = function (comm) {\n",
              "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
              "    // object with the appropriate methods. Currently this is a non binary\n",
              "    // socket, so there is still some room for performance tuning.\n",
              "    var ws = {};\n",
              "\n",
              "    ws.binaryType = comm.kernel.ws.binaryType;\n",
              "    ws.readyState = comm.kernel.ws.readyState;\n",
              "    function updateReadyState(_event) {\n",
              "        if (comm.kernel.ws) {\n",
              "            ws.readyState = comm.kernel.ws.readyState;\n",
              "        } else {\n",
              "            ws.readyState = 3; // Closed state.\n",
              "        }\n",
              "    }\n",
              "    comm.kernel.ws.addEventListener('open', updateReadyState);\n",
              "    comm.kernel.ws.addEventListener('close', updateReadyState);\n",
              "    comm.kernel.ws.addEventListener('error', updateReadyState);\n",
              "\n",
              "    ws.close = function () {\n",
              "        comm.close();\n",
              "    };\n",
              "    ws.send = function (m) {\n",
              "        //console.log('sending', m);\n",
              "        comm.send(m);\n",
              "    };\n",
              "    // Register the callback with on_msg.\n",
              "    comm.on_msg(function (msg) {\n",
              "        //console.log('receiving', msg['content']['data'], msg);\n",
              "        var data = msg['content']['data'];\n",
              "        if (data['blob'] !== undefined) {\n",
              "            data = {\n",
              "                data: new Blob(msg['buffers'], { type: data['blob'] }),\n",
              "            };\n",
              "        }\n",
              "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
              "        ws.onmessage(data);\n",
              "    });\n",
              "    return ws;\n",
              "};\n",
              "\n",
              "mpl.mpl_figure_comm = function (comm, msg) {\n",
              "    // This is the function which gets called when the mpl process\n",
              "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
              "\n",
              "    var id = msg.content.data.id;\n",
              "    // Get hold of the div created by the display call when the Comm\n",
              "    // socket was opened in Python.\n",
              "    var element = document.getElementById(id);\n",
              "    var ws_proxy = comm_websocket_adapter(comm);\n",
              "\n",
              "    function ondownload(figure, _format) {\n",
              "        window.open(figure.canvas.toDataURL());\n",
              "    }\n",
              "\n",
              "    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n",
              "\n",
              "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
              "    // web socket which is closed, not our websocket->open comm proxy.\n",
              "    ws_proxy.onopen();\n",
              "\n",
              "    fig.parent_element = element;\n",
              "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
              "    if (!fig.cell_info) {\n",
              "        console.error('Failed to find cell for figure', id, fig);\n",
              "        return;\n",
              "    }\n",
              "    fig.cell_info[0].output_area.element.on(\n",
              "        'cleared',\n",
              "        { fig: fig },\n",
              "        fig._remove_fig_handler\n",
              "    );\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_close = function (fig, msg) {\n",
              "    var width = fig.canvas.width / fig.ratio;\n",
              "    fig.cell_info[0].output_area.element.off(\n",
              "        'cleared',\n",
              "        fig._remove_fig_handler\n",
              "    );\n",
              "    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n",
              "\n",
              "    // Update the output cell to use the data from the current canvas.\n",
              "    fig.push_to_output();\n",
              "    var dataURL = fig.canvas.toDataURL();\n",
              "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
              "    // the notebook keyboard shortcuts fail.\n",
              "    IPython.keyboard_manager.enable();\n",
              "    fig.parent_element.innerHTML =\n",
              "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
              "    fig.close_ws(fig, msg);\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.close_ws = function (fig, msg) {\n",
              "    fig.send_message('closing', msg);\n",
              "    // fig.ws.close()\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n",
              "    // Turn the data on the canvas into data in the output cell.\n",
              "    var width = this.canvas.width / this.ratio;\n",
              "    var dataURL = this.canvas.toDataURL();\n",
              "    this.cell_info[1]['text/html'] =\n",
              "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function () {\n",
              "    // Tell IPython that the notebook contents must change.\n",
              "    IPython.notebook.set_dirty(true);\n",
              "    this.send_message('ack', {});\n",
              "    var fig = this;\n",
              "    // Wait a second, then push the new image to the DOM so\n",
              "    // that it is saved nicely (might be nice to debounce this).\n",
              "    setTimeout(function () {\n",
              "        fig.push_to_output();\n",
              "    }, 1000);\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function () {\n",
              "    var fig = this;\n",
              "\n",
              "    var toolbar = document.createElement('div');\n",
              "    toolbar.classList = 'btn-toolbar';\n",
              "    this.root.appendChild(toolbar);\n",
              "\n",
              "    function on_click_closure(name) {\n",
              "        return function (_event) {\n",
              "            return fig.toolbar_button_onclick(name);\n",
              "        };\n",
              "    }\n",
              "\n",
              "    function on_mouseover_closure(tooltip) {\n",
              "        return function (event) {\n",
              "            if (!event.currentTarget.disabled) {\n",
              "                return fig.toolbar_button_onmouseover(tooltip);\n",
              "            }\n",
              "        };\n",
              "    }\n",
              "\n",
              "    fig.buttons = {};\n",
              "    var buttonGroup = document.createElement('div');\n",
              "    buttonGroup.classList = 'btn-group';\n",
              "    var button;\n",
              "    for (var toolbar_ind in mpl.toolbar_items) {\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) {\n",
              "            /* Instead of a spacer, we start a new button group. */\n",
              "            if (buttonGroup.hasChildNodes()) {\n",
              "                toolbar.appendChild(buttonGroup);\n",
              "            }\n",
              "            buttonGroup = document.createElement('div');\n",
              "            buttonGroup.classList = 'btn-group';\n",
              "            continue;\n",
              "        }\n",
              "\n",
              "        button = fig.buttons[name] = document.createElement('button');\n",
              "        button.classList = 'btn btn-default';\n",
              "        button.href = '#';\n",
              "        button.title = name;\n",
              "        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n",
              "        button.addEventListener('click', on_click_closure(method_name));\n",
              "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
              "        buttonGroup.appendChild(button);\n",
              "    }\n",
              "\n",
              "    if (buttonGroup.hasChildNodes()) {\n",
              "        toolbar.appendChild(buttonGroup);\n",
              "    }\n",
              "\n",
              "    // Add the status bar.\n",
              "    var status_bar = document.createElement('span');\n",
              "    status_bar.classList = 'mpl-message pull-right';\n",
              "    toolbar.appendChild(status_bar);\n",
              "    this.message = status_bar;\n",
              "\n",
              "    // Add the close button to the window.\n",
              "    var buttongrp = document.createElement('div');\n",
              "    buttongrp.classList = 'btn-group inline pull-right';\n",
              "    button = document.createElement('button');\n",
              "    button.classList = 'btn btn-mini btn-primary';\n",
              "    button.href = '#';\n",
              "    button.title = 'Stop Interaction';\n",
              "    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n",
              "    button.addEventListener('click', function (_evt) {\n",
              "        fig.handle_close(fig, {});\n",
              "    });\n",
              "    button.addEventListener(\n",
              "        'mouseover',\n",
              "        on_mouseover_closure('Stop Interaction')\n",
              "    );\n",
              "    buttongrp.appendChild(button);\n",
              "    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n",
              "    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._remove_fig_handler = function (event) {\n",
              "    var fig = event.data.fig;\n",
              "    if (event.target !== this) {\n",
              "        // Ignore bubbled events from children.\n",
              "        return;\n",
              "    }\n",
              "    fig.close_ws(fig, {});\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function (el) {\n",
              "    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function (el) {\n",
              "    // this is important to make the div 'focusable\n",
              "    el.setAttribute('tabindex', 0);\n",
              "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
              "    // off when our div gets focus\n",
              "\n",
              "    // location in version 3\n",
              "    if (IPython.notebook.keyboard_manager) {\n",
              "        IPython.notebook.keyboard_manager.register_events(el);\n",
              "    } else {\n",
              "        // location in version 2\n",
              "        IPython.keyboard_manager.register_events(el);\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function (event, _name) {\n",
              "    // Check for shift+enter\n",
              "    if (event.shiftKey && event.which === 13) {\n",
              "        this.canvas_div.blur();\n",
              "        // select the cell after this one\n",
              "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
              "        IPython.notebook.select(index + 1);\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
              "    fig.ondownload(fig, null);\n",
              "};\n",
              "\n",
              "mpl.find_output_cell = function (html_output) {\n",
              "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
              "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
              "    // IPython event is triggered only after the cells have been serialised, which for\n",
              "    // our purposes (turning an active figure into a static one), is too late.\n",
              "    var cells = IPython.notebook.get_cells();\n",
              "    var ncells = cells.length;\n",
              "    for (var i = 0; i < ncells; i++) {\n",
              "        var cell = cells[i];\n",
              "        if (cell.cell_type === 'code') {\n",
              "            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n",
              "                var data = cell.output_area.outputs[j];\n",
              "                if (data.data) {\n",
              "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
              "                    data = data.data;\n",
              "                }\n",
              "                if (data['text/html'] === html_output) {\n",
              "                    return [cell, data, j];\n",
              "                }\n",
              "            }\n",
              "        }\n",
              "    }\n",
              "};\n",
              "\n",
              "// Register the function which deals with the matplotlib target/channel.\n",
              "// The kernel may be null if the page has been refreshed.\n",
              "if (IPython.notebook.kernel !== null) {\n",
              "    IPython.notebook.kernel.comm_manager.register_target(\n",
              "        'matplotlib',\n",
              "        mpl.mpl_figure_comm\n",
              "    );\n",
              "}\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div id='33dd1847-5bad-446c-97a5-dc2077096317'></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:06<00:00,  8.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
            "Please check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\n",
            "Use the function 'analyze_video' to make predictions on new videos.\n",
            "Otherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaLBl3TQtrfB"
      },
      "source": [
        "## There is an optional refinement step you can do outside of Colab:\n",
        "- if your pixel errors are not low enough, please check out the protocol guide on how to refine your network!\n",
        "- You will need to adjust the labels **outside of Colab!** We recommend coming back to train and analyze videos... \n",
        "- Please see the repo and protocol instructions on how to refine your data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVFLSKKfoEJk"
      },
      "source": [
        "## Analyzing new videos: \n",
        "Specify snapshot index for **snapshotindex** in the **config.yaml** file. Otherwise, by default the most recent snapshot is used to analyse the video."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_videofile_path = ['/content/drive/My Drive/Genzel Internship/Test/Test1_8bp-PN-2022-05-19/new videos/New 5-8-22/']\n",
        "new_videofile_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5edZY1a6f-V1",
        "outputId": "a3646aed-deb3-4f5b-8a66-96d17de544ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Genzel Internship/Test/Test1_8bp-PN-2022-05-19/new videos/New 5-8-22/']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deeplabcut.analyze_videos?"
      ],
      "metadata": {
        "id": "_u8M6x6ijeAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_LZiS_0oEJl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "a5ae8f7e-8e00-4d3c-fe18-a57abe2576a3"
      },
      "source": [
        "deeplabcut.analyze_videos(path_config_file, new_videofile_path, save_as_csv=True, videotype=VideoType) # if save_as_csv=True not added, then defult is hd5 file."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using snapshot-266000 for model /content/drive/My Drive/Genzel Internship/Test/Test1_8bp-PN-2022-05-19/dlc-models/iteration-0/Test1_8bpMay19-trainset95shuffle1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:684: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  outputs = layer.apply(inputs, training=is_training)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing all the videos in the directory...\n",
            "Starting to analyze %  /content/drive/My Drive/Genzel Internship/Test/Test1_8bp-PN-2022-05-19/new videos/New 5-8-22/Rat8SD6PointGreyVideo2020-05-04T10_34_08downsampledshort.avi\n",
            "Loading  /content/drive/My Drive/Genzel Internship/Test/Test1_8bp-PN-2022-05-19/new videos/New 5-8-22/Rat8SD6PointGreyVideo2020-05-04T10_34_08downsampledshort.avi\n",
            "Duration of video [s]:  303.0 , recorded with  30.0 fps!\n",
            "Overall # of frames:  9090  found with (before cropping) frame dimensions:  325 256\n",
            "Starting to extract posture\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9090/9090 [01:04<00:00, 141.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving results in /content/drive/My Drive/Genzel Internship/Test/Test1_8bp-PN-2022-05-19/new videos/New 5-8-22...\n",
            "Saving csv poses!\n",
            "The videos are analyzed. Now your research can truly start! \n",
            " You can create labeled videos with 'create_labeled_video'\n",
            "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DLC_resnet50_Test1_8bpMay19shuffle1_266000'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GTiuJESoEKH"
      },
      "source": [
        "## Plot the trajectories of the analyzed videos:\n",
        "This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX21zZbXoEKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4cc68e9-0bba-41af-a52b-e2c57869e5c1"
      },
      "source": [
        "deeplabcut.plot_trajectories(path_config_file, new_videofile_path, videotype=VideoType)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing all the videos in the directory...\n",
            "Loading  /content/drive/My Drive/Genzel Internship/Test/Test1_8bp-PN-2022-05-19/new videos/New 5-8-22/Rat8SD6PointGreyVideo2020-05-04T10_34_08downsampledshort.avi and data.\n",
            "Plots created! Please check the directory \"plot-poses\" within the video directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqaCw15v8EmB"
      },
      "source": [
        "Now you can look at the plot-poses file and check the \"plot-likelihood.png\" might want to change the \"p-cutoff\" in the config.yaml file so that you have only high confidnece points plotted in the video. i.e. ~0.8 or 0.9. The current default is 0.4. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCrUvQIvoEKD"
      },
      "source": [
        "## Create labeled video:\n",
        "This function is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deeplabcut.create_labeled_video?"
      ],
      "metadata": {
        "id": "l527OIXdhm9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aDF7Q7KoEKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b05f34-eed6-4430-c244-41167c233ba1"
      },
      "source": [
        "deeplabcut.create_labeled_video(path_config_file, new_videofile_path, videotype=VideoType)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing all the videos in the directory...\n",
            "Starting to process video: /content/drive/My Drive/Genzel Internship/Test/Test1_8bp-PN-2022-05-19/new videos/New 5-8-22/Rat8SD6PointGreyVideo2020-05-04T10_34_08downsampledshort.avi\n",
            "Loading /content/drive/My Drive/Genzel Internship/Test/Test1_8bp-PN-2022-05-19/new videos/New 5-8-22/Rat8SD6PointGreyVideo2020-05-04T10_34_08downsampledshort.avi and data.\n",
            "Duration of video [s]: 303.0, recorded with 30.0 fps!\n",
            "Overall # of frames: 9090 with cropped frame dimensions: 325 256\n",
            "Generating frames and creating video.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9090/9090 [00:12<00:00, 737.05it/s]\n"
          ]
        }
      ]
    }
  ]
}
